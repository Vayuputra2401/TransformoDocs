{
    "id": "a7f4c197-d67a-43fd-aad2-5e796fa2d870",
    "filename": "ICPR_Kolkata.pdf",
    "date": "2024-09-27T20:55:40.257042",
    "data": "{\"structured_data\": {\"entities\": [{\"text\": \"Brain Tumors\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"U-Net Models\", \"label\": \"ORG\"}, {\"text\": \"Pathikreet Chowdhury\", \"label\": \"PERSON\"}, {\"text\": \"Rajiv Gandhi Institute of Petroleum Technology\", \"label\": \"ORG\"}, {\"text\": \"229304\", \"label\": \"DATE\"}, {\"text\": \"India\", \"label\": \"GPE\"}, {\"text\": \"21cs2026,gsrivastava}@rgipt.ac.in\", \"label\": \"CARDINAL\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"ResNet\", \"label\": \"NORP\"}, {\"text\": \"VGG\", \"label\": \"ORG\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"ResNet\", \"label\": \"NORP\"}, {\"text\": \"the Explainable AI\", \"label\": \"LOC\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"99.79%\", \"label\": \"PERCENT\"}, {\"text\": \"0.889\", \"label\": \"CARDINAL\"}, {\"text\": \"Addition-\", \"label\": \"ORG\"}, {\"text\": \"wereportaLIMEexplanationstabilityscoreof0.8169andasparsity\", \"label\": \"ORG\"}, {\"text\": \"0.1190\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"98.70%\", \"label\": \"PERCENT\"}, {\"text\": \"97.63%\", \"label\": \"PERCENT\"}, {\"text\": \"97.64%\", \"label\": \"PERCENT\"}, {\"text\": \"F1 - Score\", \"label\": \"PRODUCT\"}, {\"text\": \"97.63%\", \"label\": \"PERCENT\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"0.923\", \"label\": \"CARDINAL\"}, {\"text\": \"0.203.These\", \"label\": \"CARDINAL\"}, {\"text\": \"1.00\", \"label\": \"CARDINAL\"}, {\"text\": \"Brain Tumor\", \"label\": \"PERSON\"}, {\"text\": \"AI\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"1 IntroductionSegmentation\", \"label\": \"ORG\"}, {\"text\": \"the past few years\", \"label\": \"DATE\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"DATE\"}, {\"text\": \"3\", \"label\": \"DATE\"}, {\"text\": \"4\", \"label\": \"DATE\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"Explainable AI\", \"label\": \"ORG\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"fed\", \"label\": \"ORG\"}, {\"text\": \"AI\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"8\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet\", \"label\": \"NORP\"}, {\"text\": \"9\", \"label\": \"CARDINAL\"}, {\"text\": \"VGG Net\", \"label\": \"ORG\"}, {\"text\": \"10\", \"label\": \"CARDINAL\"}, {\"text\": \"Section 2\", \"label\": \"LAW\"}, {\"text\": \"Section 3\", \"label\": \"LAW\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"2.1\", \"label\": \"CARDINAL\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Enhanced U-Net\", \"label\": \"PERSON\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"U-Net:Title Suppressed Due to Excessive Length\", \"label\": \"ORG\"}, {\"text\": \"Fig\", \"label\": \"PERSON\"}, {\"text\": \"Neural Network for SegmentationResidual Connections: Residual\", \"label\": \"ORG\"}, {\"text\": \"Batch Normalization:\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"Batch\", \"label\": \"PERSON\"}, {\"text\": \"SpatialDropoutisincorporatedwithineachconvolutionalblock\", \"label\": \"ORG\"}, {\"text\": \"2.2\", \"label\": \"CARDINAL\"}, {\"text\": \"Convolutional Neural Network\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"Conv Layer 1\", \"label\": \"PERSON\"}, {\"text\": \"32\", \"label\": \"CARDINAL\"}, {\"text\": \"4x4\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"Pathikreet and GargiFig\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"Neural Network\", \"label\": \"ORG\"}, {\"text\": \"Conv Layer 2\", \"label\": \"PERSON\"}, {\"text\": \"64\", \"label\": \"CARDINAL\"}, {\"text\": \"4x4\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"Conv Layer 3\", \"label\": \"PERSON\"}, {\"text\": \"4x4\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"Conv Layer 4\", \"label\": \"PERSON\"}, {\"text\": \"128\", \"label\": \"CARDINAL\"}, {\"text\": \"4x4\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"first\", \"label\": \"ORDINAL\"}, {\"text\": \"second\", \"label\": \"ORDINAL\"}, {\"text\": \"max\", \"label\": \"PERSON\"}, {\"text\": \"3x3\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"third\", \"label\": \"ORDINAL\"}, {\"text\": \"max\", \"label\": \"PERSON\"}, {\"text\": \"3x3\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"Fully Connected Layers:\", \"label\": \"FAC\"}, {\"text\": \"Fully Connected\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"512\", \"label\": \"CARDINAL\"}, {\"text\": \"ReLU\", \"label\": \"PRODUCT\"}, {\"text\": \"0.5\", \"label\": \"CARDINAL\"}, {\"text\": \"two\", \"label\": \"CARDINAL\"}, {\"text\": \"max\", \"label\": \"PERSON\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"Proposed ModelsApplication on Classification Models\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"Application on Segmentation Models:\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"2.4\", \"label\": \"CARDINAL\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Data Acquisition\", \"label\": \"ORG\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"256x256\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"Adam\", \"label\": \"PERSON\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"Adam\", \"label\": \"PERSON\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"Model Evaluation\", \"label\": \"PRODUCT\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"F1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"3.1\", \"label\": \"CARDINAL\"}, {\"text\": \"7023\", \"label\": \"CARDINAL\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Jun Cheng\\u2019s\", \"label\": \"PERSON\"}, {\"text\": \"Brain Tumor Dataset\", \"label\": \"PERSON\"}, {\"text\": \"11\", \"label\": \"CARDINAL\"}, {\"text\": \"12\", \"label\": \"CARDINAL\"}, {\"text\": \"3064\", \"label\": \"CARDINAL\"}, {\"text\": \"233\", \"label\": \"CARDINAL\"}, {\"text\": \"three\", \"label\": \"CARDINAL\"}, {\"text\": \"glioma\", \"label\": \"PERSON\"}, {\"text\": \"1426\", \"label\": \"DATE\"}, {\"text\": \"930\", \"label\": \"CARDINAL\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"766\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"four\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"SarTaj Dataset\", \"label\": \"PERSON\"}, {\"text\": \"FLAIR\", \"label\": \"ORG\"}, {\"text\": \"The Cancer Imaging Archive\", \"label\": \"ORG\"}, {\"text\": \"TCIA\", \"label\": \"LAW\"}, {\"text\": \"110\", \"label\": \"CARDINAL\"}, {\"text\": \"FLAIR\", \"label\": \"ORG\"}, {\"text\": \"110\", \"label\": \"CARDINAL\"}, {\"text\": \"dataset8 Pathikreet\", \"label\": \"FAC\"}, {\"text\": \"Gargi\", \"label\": \"PERSON\"}, {\"text\": \"zero\", \"label\": \"CARDINAL\"}, {\"text\": \"1s\", \"label\": \"CARDINAL\"}, {\"text\": \"0s\", \"label\": \"CARDINAL\"}, {\"text\": \"3.2\", \"label\": \"CARDINAL\"}, {\"text\": \"Evaluation MetricsSegmentation\", \"label\": \"EVENT\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Dice Coefficient (Dice Similarity Index\", \"label\": \"ORG\"}, {\"text\": \"DSC):The Dice\", \"label\": \"ORG\"}, {\"text\": \"0\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2|A\\u2229B|\", \"label\": \"CARDINAL\"}, {\"text\": \"Ais\", \"label\": \"PERSON\"}, {\"text\": \"Bis\", \"label\": \"ORG\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"IoU\", \"label\": \"ORG\"}, {\"text\": \"Jaccard Index):IoU\", \"label\": \"PERSON\"}, {\"text\": \"0\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"Predictive Value\", \"label\": \"PERSON\"}, {\"text\": \"TP+FP(3\", \"label\": \"GPE\"}, {\"text\": \"TP\", \"label\": \"PERSON\"}, {\"text\": \"FP\", \"label\": \"PERSON\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"True Positive Rate\", \"label\": \"ORG\"}, {\"text\": \"Recall =TPTP+FN(4\", \"label\": \"ORG\"}, {\"text\": \"FN\", \"label\": \"PERSON\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"F1\", \"label\": \"CARDINAL\"}, {\"text\": \"a single metric\", \"label\": \"QUANTITY\"}, {\"text\": \"F1 Score\", \"label\": \"ORG\"}, {\"text\": \"2\\u00d7Precision\", \"label\": \"CARDINAL\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"d(a\", \"label\": \"GPE\"}, {\"text\": \"d(a\", \"label\": \"GPE\"}, {\"text\": \"Euclidean\", \"label\": \"LOC\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Accuracy\", \"label\": \"PERSON\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"ROC\", \"label\": \"GPE\"}, {\"text\": \"AUC\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"0.5\", \"label\": \"CARDINAL\"}, {\"text\": \"F1\", \"label\": \"CARDINAL\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"1\\u22121\", \"label\": \"CARDINAL\"}, {\"text\": \"8)\", \"label\": \"CARDINAL\"}, {\"text\": \"xiandE(x\\u2032\", \"label\": \"PERSON\"}, {\"text\": \"Pathikreet and Gargi\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"1\\u2212Number\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"Fidelity\", \"label\": \"ORG\"}, {\"text\": \"1\\u22121\", \"label\": \"CARDINAL\"}, {\"text\": \"3.3\", \"label\": \"CARDINAL\"}, {\"text\": \"Segmentation Model (Enhanced U-Net\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"Batch Size\", \"label\": \"PERSON\"}, {\"text\": \"16\", \"label\": \"CARDINAL\"}, {\"text\": \"one\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"50\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"0.1\", \"label\": \"CARDINAL\"}, {\"text\": \"0.2\", \"label\": \"CARDINAL\"}, {\"text\": \"0.3\", \"label\": \"CARDINAL\"}, {\"text\": \"0\", \"label\": \"CARDINAL\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"TheAdamoptimizerischosenforitsadaptivelearningrate\", \"label\": \"ORG\"}, {\"text\": \"11\", \"label\": \"CARDINAL\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"Binary Cross Entropy\", \"label\": \"ORG\"}, {\"text\": \"NNX\", \"label\": \"ORG\"}, {\"text\": \"jyijlog(pij\", \"label\": \"PERSON\"}, {\"text\": \"11\", \"label\": \"CARDINAL\"}, {\"text\": \"Nis\", \"label\": \"ORG\"}, {\"text\": \"Classification Model (Custom\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"Batch Size\", \"label\": \"PERSON\"}, {\"text\": \"32\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"100\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"0.5\", \"label\": \"CARDINAL\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"Adam\", \"label\": \"PERSON\"}, {\"text\": \"Adam\", \"label\": \"PERSON\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"between 0 and 1\", \"label\": \"CARDINAL\"}, {\"text\": \"Cross Entropy\", \"label\": \"ORG\"}, {\"text\": \"NNX\", \"label\": \"ORG\"}, {\"text\": \"Nis\", \"label\": \"ORG\"}, {\"text\": \"Softmax\", \"label\": \"NORP\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"1000\", \"label\": \"CARDINAL\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"L1\", \"label\": \"CARDINAL\"}, {\"text\": \"Pathikreet\", \"label\": \"GPE\"}, {\"text\": \"Gargi\", \"label\": \"PERSON\"}, {\"text\": \"5\", \"label\": \"DATE\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"Max\", \"label\": \"PERSON\"}, {\"text\": \"200\", \"label\": \"CARDINAL\"}, {\"text\": \"two\", \"label\": \"CARDINAL\"}, {\"text\": \"0.5\", \"label\": \"CARDINAL\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"Custom CNN\", \"label\": \"PERSON\"}, {\"text\": \"ResNet32\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"98.70%\", \"label\": \"PERCENT\"}, {\"text\": \"96.35%\", \"label\": \"PERCENT\"}, {\"text\": \"96.2%\", \"label\": \"PERCENT\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"97.63%\", \"label\": \"PERCENT\"}, {\"text\": \"ResNet32\", \"label\": \"CARDINAL\"}, {\"text\": \"95.24%\", \"label\": \"PERCENT\"}, {\"text\": \"95.78%\", \"label\": \"PERCENT\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"97.64%\", \"label\": \"PERCENT\"}, {\"text\": \"96.12%\", \"label\": \"PERCENT\"}, {\"text\": \"95.12%\", \"label\": \"PERCENT\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"F1-Score\", \"label\": \"ORG\"}, {\"text\": \"97.47%\", \"label\": \"PERCENT\"}, {\"text\": \"ResNet32\", \"label\": \"CARDINAL\"}, {\"text\": \"96.15%\", \"label\": \"PERCENT\"}, {\"text\": \"95.76%\", \"label\": \"PERCENT\"}, {\"text\": \"Custom\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"0.923\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet32\", \"label\": \"CARDINAL\"}, {\"text\": \"0.846\", \"label\": \"CARDINAL\"}, {\"text\": \"0.687\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"0.208\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet32\", \"label\": \"CARDINAL\"}, {\"text\": \"0.196\", \"label\": \"PRODUCT\"}, {\"text\": \"0.225\", \"label\": \"CARDINAL\"}, {\"text\": \"0.556\", \"label\": \"CARDINAL\"}, {\"text\": \"0.418\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"0.310\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"ResNet32\", \"label\": \"PERSON\"}, {\"text\": \"F1-Score\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"Table 2\", \"label\": \"PRODUCT\"}, {\"text\": \"Custom U-Net\", \"label\": \"ORG\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"ResNet\", \"label\": \"ORG\"}, {\"text\": \"99.8%\", \"label\": \"PERCENT\"}, {\"text\": \"the U-Net\", \"label\": \"ORG\"}, {\"text\": \"ResNet\", \"label\": \"ORG\"}, {\"text\": \"99.11%\", \"label\": \"PERCENT\"}, {\"text\": \"The Custom U-Net\", \"label\": \"GPE\"}, {\"text\": \"0.0132\", \"label\": \"CARDINAL\"}, {\"text\": \"the U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.0425\", \"label\": \"CARDINAL\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.889\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet\", \"label\": \"LOC\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.847\", \"label\": \"CARDINAL\"}, {\"text\": \"The Custom U-Net\", \"label\": \"ORG\"}, {\"text\": \"LIMETitle Suppressed Due\", \"label\": \"PERSON\"}, {\"text\": \"13\", \"label\": \"CARDINAL\"}, {\"text\": \"Custom CNN\", \"label\": \"PERSON\"}, {\"text\": \"ResNet\", \"label\": \"NORP\"}, {\"text\": \"Model ResNet32\", \"label\": \"PRODUCT\"}, {\"text\": \"98.70%\", \"label\": \"PERCENT\"}, {\"text\": \"96.35%\", \"label\": \"PERCENT\"}, {\"text\": \"96.2%\", \"label\": \"PERCENT\"}, {\"text\": \"97.63\", \"label\": \"CARDINAL\"}, {\"text\": \"95.78\", \"label\": \"CARDINAL\"}, {\"text\": \"97.64\", \"label\": \"CARDINAL\"}, {\"text\": \"95.12\", \"label\": \"CARDINAL\"}, {\"text\": \"F1\", \"label\": \"CARDINAL\"}, {\"text\": \"97.47\", \"label\": \"CARDINAL\"}, {\"text\": \"96.15\", \"label\": \"CARDINAL\"}, {\"text\": \"95.76\", \"label\": \"CARDINAL\"}, {\"text\": \"0.923\", \"label\": \"CARDINAL\"}, {\"text\": \"0.687\", \"label\": \"CARDINAL\"}, {\"text\": \"0.208\", \"label\": \"CARDINAL\"}, {\"text\": \"0.225\", \"label\": \"PRODUCT\"}, {\"text\": \"Fidelity Score\", \"label\": \"ORG\"}, {\"text\": \"0.310 0.556 0.418\", \"label\": \"MONEY\"}, {\"text\": \"0.8169\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet\", \"label\": \"LOC\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.7873\", \"label\": \"CARDINAL\"}, {\"text\": \"0.1190\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet\", \"label\": \"LOC\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.1221\", \"label\": \"CARDINAL\"}, {\"text\": \"ResNet\", \"label\": \"LOC\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"0.6036\", \"label\": \"CARDINAL\"}, {\"text\": \"the Custom U-Net\", \"label\": \"FAC\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"the Confusion MatrixTable 2\", \"label\": \"LAW\"}, {\"text\": \"Custom U-net\", \"label\": \"PERSON\"}, {\"text\": \"ResUnetEvaluation Metrics Custom\", \"label\": \"ORG\"}, {\"text\": \"Model U-net\", \"label\": \"PERSON\"}, {\"text\": \"ResNet\", \"label\": \"NORP\"}, {\"text\": \"99.8%\", \"label\": \"PERCENT\"}, {\"text\": \"99.11%\", \"label\": \"PERCENT\"}, {\"text\": \"0.0132 0.0425\", \"label\": \"CARDINAL\"}, {\"text\": \"Union Score\", \"label\": \"ORG\"}, {\"text\": \"0.889\", \"label\": \"CARDINAL\"}, {\"text\": \"0.847\", \"label\": \"MONEY\"}, {\"text\": \"0.8169 0.7873\", \"label\": \"CARDINAL\"}, {\"text\": \"Fidelity Score\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"Plot\", \"label\": \"ORG\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"three\", \"label\": \"CARDINAL\"}, {\"text\": \"7\", \"label\": \"CARDINAL\"}, {\"text\": \"IoU Score Negative\", \"label\": \"ORG\"}, {\"text\": \"IoU\", \"label\": \"ORG\"}, {\"text\": \"1.14\", \"label\": \"CARDINAL\"}, {\"text\": \"Pathikreet and GargiFig\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"3.Confusion Matrix\", \"label\": \"QUANTITY\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"Fig\", \"label\": \"PERSON\"}, {\"text\": \"4.Plot\", \"label\": \"CARDINAL\"}, {\"text\": \"Fig\", \"label\": \"PERSON\"}, {\"text\": \"LIME\", \"label\": \"ORG\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"Conclusion and Future Scope\", \"label\": \"ORG\"}, {\"text\": \"15\", \"label\": \"CARDINAL\"}, {\"text\": \"Fig\", \"label\": \"PERSON\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"three\", \"label\": \"CARDINAL\"}, {\"text\": \"Fig\", \"label\": \"PERSON\"}, {\"text\": \"1\", \"label\": \"CARDINAL\"}, {\"text\": \"U-Net\", \"label\": \"ORG\"}, {\"text\": \"CNN\", \"label\": \"ORG\"}, {\"text\": \"Adriano Pinto\", \"label\": \"PERSON\"}, {\"text\": \"Victor Alves\", \"label\": \"PERSON\"}, {\"text\": \"Carlos A Silva\", \"label\": \"PERSON\"}, {\"text\": \"35(5):1240\\u20131251, 2016\", \"label\": \"DATE\"}, {\"text\": \"2\", \"label\": \"CARDINAL\"}, {\"text\": \"Mahmoud Khaled Abd-Ellah\", \"label\": \"PERSON\"}, {\"text\": \"Ali Ismail Awad\", \"label\": \"PERSON\"}, {\"text\": \"Ashraf AM Khalaf\", \"label\": \"ORG\"}, {\"text\": \"Hesham FA Hamed\", \"label\": \"PERSON\"}, {\"text\": \"61:300\\u2013318, 2019\", \"label\": \"DATE\"}, {\"text\": \"3\", \"label\": \"CARDINAL\"}, {\"text\": \"Esraa Galal Mahmoud\", \"label\": \"GPE\"}, {\"text\": \"Nadder Hamdy\", \"label\": \"PERSON\"}, {\"text\": \"2010\", \"label\": \"DATE\"}, {\"text\": \"Computer Engineering &amp; Systems\", \"label\": \"ORG\"}, {\"text\": \"368\\u2013373\", \"label\": \"CARDINAL\"}, {\"text\": \"2010\", \"label\": \"DATE\"}, {\"text\": \"4\", \"label\": \"CARDINAL\"}, {\"text\": \"Applied Sciences\", \"label\": \"ORG\"}, {\"text\": \"10(6):1999, 2020\", \"label\": \"DATE\"}, {\"text\": \"5\", \"label\": \"CARDINAL\"}, {\"text\": \"Gargi Srivastava\", \"label\": \"PERSON\"}, {\"text\": \"Vibhav\", \"label\": \"GPE\"}, {\"text\": \"International Journal of Ad-\", \"label\": \"ORG\"}, {\"text\": \"Networking and Applications - IJANA\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"Advancements in Smart Computing and Information Security\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2022\", \"label\": \"DATE\"}, {\"text\": \"Rajkot\", \"label\": \"GPE\"}, {\"text\": \"India\", \"label\": \"GPE\"}, {\"text\": \"November 24-26, 2022\", \"label\": \"DATE\"}, {\"text\": \"11\\u201316\", \"label\": \"CARDINAL\"}, {\"text\": \"2022\", \"label\": \"DATE\"}, {\"text\": \"6\", \"label\": \"CARDINAL\"}, {\"text\": \"Barredo Arrieta\", \"label\": \"PERSON\"}, {\"text\": \"Natalia D\\u00edaz-Rodr\\u00edguez\", \"label\": \"ORG\"}, {\"text\": \"Javier Del Ser\", \"label\": \"PERSON\"}, {\"text\": \"AdrienBennetot\", \"label\": \"PERSON\"}, {\"text\": \"Siham Tabik\", \"label\": \"PERSON\"}, {\"text\": \"Alberto Barbado\", \"label\": \"GPE\"}, {\"text\": \"Salvador Garc\\u00eda\", \"label\": \"PERSON\"}, {\"text\": \"Sergio\", \"label\": \"PERSON\"}, {\"text\": \"Daniel Molina\", \"label\": \"PERSON\"}, {\"text\": \"Richard Benjamins\", \"label\": \"PERSON\"}, {\"text\": \"Concepts\", \"label\": \"GPE\"}, {\"text\": \"58:82\\u2013115, 2020\", \"label\": \"DATE\"}, {\"text\": \"Velden\", \"label\": \"GPE\"}, {\"text\": \"Kenneth GA\", \"label\": \"PERSON\"}, {\"text\": \"Gilhuijs\", \"label\": \"PERSON\"}, {\"text\": \"Max A\", \"label\": \"PERSON\"}, {\"text\": \"Viergever\", \"label\": \"GPE\"}, {\"text\": \"Medical Image Analysis\", \"label\": \"ORG\"}, {\"text\": \"79:102470\", \"label\": \"CARDINAL\"}, {\"text\": \"2022\", \"label\": \"DATE\"}, {\"text\": \"8\", \"label\": \"CARDINAL\"}, {\"text\": \"Sabrina Kirrane\", \"label\": \"PERSON\"}, {\"text\": \"2020\", \"label\": \"DATE\"}, {\"text\": \"9\", \"label\": \"CARDINAL\"}, {\"text\": \"Diogo Almeida\", \"label\": \"PERSON\"}, {\"text\": \"Kevin Lyman\", \"label\": \"PERSON\"}, {\"text\": \"General-\", \"label\": \"CARDINAL\"}, {\"text\": \"2016\", \"label\": \"DATE\"}, {\"text\": \"10\", \"label\": \"CARDINAL\"}, {\"text\": \"Yuting Ye\", \"label\": \"PERSON\"}, {\"text\": \"Robert Wang\", \"label\": \"PERSON\"}, {\"text\": \"Chiao Liu\", \"label\": \"PERSON\"}, {\"text\": \"Kaushik\", \"label\": \"GPE\"}, {\"text\": \"Roy\", \"label\": \"PERSON\"}, {\"text\": \"Frontiers\", \"label\": \"ORG\"}, {\"text\": \"13:95, 2019\", \"label\": \"DATE\"}, {\"text\": \"11\", \"label\": \"CARDINAL\"}, {\"text\": \"Jun Cheng\", \"label\": \"PERSON\"}, {\"text\": \"Wei Huang\", \"label\": \"PERSON\"}, {\"text\": \"Shuangliang Cao\", \"label\": \"PERSON\"}, {\"text\": \"Ru Yang\", \"label\": \"PERSON\"}, {\"text\": \"Wei Yang\", \"label\": \"PERSON\"}, {\"text\": \"ZhaoqiangYun\", \"label\": \"PERSON\"}, {\"text\": \"Zhijian Wang\", \"label\": \"PERSON\"}, {\"text\": \"Qianjin Feng\", \"label\": \"PERSON\"}, {\"text\": \"10(10):e0140381,\", \"label\": \"DATE\"}, {\"text\": \"2015\", \"label\": \"DATE\"}, {\"text\": \"12\", \"label\": \"CARDINAL\"}, {\"text\": \"Jun Cheng\", \"label\": \"PERSON\"}, {\"text\": \"Wei Yang\", \"label\": \"PERSON\"}, {\"text\": \"Meiyan Huang\", \"label\": \"PERSON\"}, {\"text\": \"Wei Huang\", \"label\": \"PERSON\"}, {\"text\": \"Jun Jiang\", \"label\": \"PERSON\"}, {\"text\": \"Yujia Zhou\", \"label\": \"PERSON\"}, {\"text\": \"Ru Yang\", \"label\": \"PERSON\"}, {\"text\": \"Jie Zhao\", \"label\": \"PERSON\"}, {\"text\": \"Yanqiu Feng\", \"label\": \"PERSON\"}, {\"text\": \"Qianjin Feng\", \"label\": \"PERSON\"}, {\"text\": \"al. Retrieval\", \"label\": \"ORG\"}, {\"text\": \"11(6):e0157112, 2016\", \"label\": \"DATE\"}], \"sentences\": [\"Enhanced Classification and Segmentation ofBrain Tumors in MRI Images Using CustomCNN and U-Net Models with XAIPathikreet Chowdhury and Gargi Srivastava[0000\\u22120001\\u22126770\\u2212561X]Rajiv Gandhi Institute of Petroleum Technology, Jais Uttar Pradesh 229304, India{21cs2026,gsrivastava}@rgipt.ac.inhttp://www.rgipt.ac.inAbstract.\", \"This study aims to classify brain tumors in MRI images intofour categories: glioma, meningioma, absent, and pituitary tumors, aswell as segment low-grade gliomas.\", \"We evaluate our proposed models onfour publicly available datasets to ensure robustness and generalizability.\", \"For classification tasks, we compare the performance of our custom CNNmodel against established models like ResNet and VGG.\", \"For segmenta-tion tasks, we compare our custom U-Net model with the original U-Netand ResNet-based encoders.\", \"To validate the effectiveness of our models,we employ the Explainable AI (XAI) method LIME, providing insightsinto why our custom architectures outperform others.\", \"Our custom U-Netmodel achieves a validation accuracy of 99.79% and an Intersection overUnion (IoU) score of 0.889 for low-grade glioma segmentation.\", \"Addition-ally,wereportaLIMEexplanationstabilityscoreof0.8169andasparsityscore of 0.1190.\", \"The proposed custom CNN model achieves a validationaccuracy of 98.70% , weighted avg precision of 97.63% , recall of 97.64%and weighted F1 - Score of 97.63%.\", \"The model achieves a LIME stabilityscore of 0.923 and a sparsity score of 0.203.These results highlight thepotential of our custom models to enhance accuracy and interpretabilityin brain tumor classification and segmentation tasks, offering significantimprovements over existing methodologies.\", \"The custom U-net model isalso an excellent negative classifier achieving a perfect 1.00 IoU score forclassifying MRI scans which do not have any tumor.\", \"Keywords: Brain Tumor \\u00b7Explainable AI \\u00b7LIME.\", \"1 IntroductionSegmentation and classification of brain tumors in MRI images have been exten-sively researched over the past few years.\", \"Researchers have been leveraging deepneural networks to achieve significant improvements in this area\", \"[1, 2, 3, 4, 5].\", \"However, the advent of Explainable AI (XAI)\", \"[6] has introduced new pos-sibilities, allowing us to scrutinize network structures and understand the un-derlying mechanisms behind their final results.\", \"This perspective is crucial fordiscerning why certain models perform better than others and how existing neu-ral network architectures can be modified for improved efficiency and accuracy.2 Pathikreet and GargiConsequently, this can lead to smaller networks that save training time and pro-vide faster results when deployed.\", \"Previously, deep learning models operated largely as black boxes.\", \"Input imageswere fed into the network, and based on the output, additional images that ledto misclassifications were incorporated into the training set to enhance learning.\", \"Developers focused on layer adjustment, feedback incorporation, loss prevention,and hyperparameter tuning, often relying on trial and error to achieve better re-sults.\", \"With XAI, the opaque nature of neural networks has transformed intoa transparent process, providing clear insights into the inner workings of thenetwork.\", \"This transparency empowers developers to exert more control over thenetwork, significantly reducing development time and reliance on trial and error\", \"[7].\", \"Moreover, traditional deep learning models were constrained to predicting pre-defined classes without the ability to indicate uncertainty.\", \"XAI now enablesus to identify when the network reaches an \\\"I don\\u2019t know\\\" stage, allowing forthe development of custom models tailored to specific needs rather than merelyadapting existing network architectures for different domains through transferlearning.\", \"Explainable AI also enhances the accountability, fairness, and transparency ofdeep learning models.\", \"This is particularly important in medical diagnostics,where the consequences of misclassification and segmentation can be severe.\", \"In this paper, we develop a custom segmentation and classification model, pro-viding interpretation with the LIME model\", \"[8].\", \"We compare our results withbenchmark neural network models such as ResNet [9] and VGG Net [10].\", \"The paper is organized as follows: we detail our methodology in Section 2, fol-lowed by the experimentation details in Section 3 and presentation of our resultsand their discussion in Section4.\", \"The conclusions and implications our findingsare presented in the Section 5.2 Methodology2.1 Proposed Neural Network for Segmentation\", \"The proposed segmentation network is a custom U-Net, specifically designed toenhance the segmentation of brain MRI images.\", \"The architecture retains thecanonical encoder-decoder structure of the original U-Net, with several enhance-ments aimed at capturing more complex features and improving segmentationperformance.\", \"Figure 1 displays the network architecture.\", \"Proposed Enhanced U-Net Our proposed Enhanced U-Net model intro-duces several critical modifications to the original U-Net architecture, aimedat augmenting its performance for brain tumor segmentation tasks.\", \"These en-hancements are strategically designed to improve feature extraction, model ro-bustness, and overall segmentation accuracy.\", \"Here are the key differences andenhancements compared to the original U-Net:Title Suppressed Due to Excessive Length 3Fig.\", \"1.Proposed Neural Network for SegmentationResidual Connections: Residual connections are integrated within each convo-lutional block.\", \"This strategy helps alleviate the vanishing gradient problem andfacilitates the training of deeper networks by allowing gradients to flow moreeffectively through the network.\", \"The residual path is implemented by addingthe input to the output of the convolutional layers within the block.\", \"Batch Normalization:\", \"Batch normalization layers are employed after each con-volutional layer.\", \"This technique stabilizes and accelerates the training process bynormalizing the activations, thus reducing internal covariate shift.\", \"It also allowsfor higher learning rates and reduces sensitivity to initialization.\", \"Spatial Dropout:\", \"SpatialDropoutisincorporatedwithineachconvolutionalblocktoimprovegeneralizationandpreventoverfitting.\", \"Thisformofdropoutrandomlydrops entire feature maps rather than individual elements, which is particularlyeffective for spatial data, ensuring that the model does not become overly relianton specific feature maps.\", \"Byincorporatingtheseenhancements,ourproposedEnhancedU-Netmodelaimsto significantly improve segmentation performance in terms of accuracy, robust-ness, and generalization capabilities.\", \"2.2 Proposed Neural Network for ClassificationThe proposed classification network is a custom Convolutional Neural Network(CNN) specifically designed for classifying brain MRI images into benign andmalignant tumors.\", \"It incorporates advanced convolutional layers with batch nor-malization, ReLU activations, and strategic pooling operations to enhance fea-tureextractionandclassificationaccuracy.\", \"Figure2displaystheProposedNeuralNetwork for Classification.\", \"Convolutional Layers:\", \"The network comprises four convolutional layers, eachcontributing to hierarchical feature learning:\\u2013Conv Layer 1: Applies 32 filters of size 4x4 with a stride of 1, utilizing batchnormalization and ReLU activation.4 Pathikreet and GargiFig.\", \"2.Proposed Neural Network for Classification\\u2013Conv Layer 2: Employs 64 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.\", \"\\u2013Conv Layer 3: Utilizes 128 filters of size 4x4 with a stride of 1, integratedwith batch normalization and ReLU activation.\", \"\\u2013Conv Layer 4: Applies 128 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.\", \"Pooling Layers:\", \"Pooling operations are strategically placed to reduce spatialdimensions and enhance translational invariance:\\u2013Pooling Layer 1: After the first and second convolutional layers, a max pool-ing operation with a kernel size of 3x3 and a stride of 3 is applied.\", \"\\u2013Pooling Layer 2: Following the third convolutional layer, a max pooling op-eration with a kernel size of 3x3 and a stride of 2 is employed.\", \"Fully Connected Layers: After feature extraction through convolution and pool-ing, the network incorporates fully connected layers for final classification:\\u2013Fully Connected (FC) Layer 1: Composed of 512 units with ReLU activation,facilitating complex feature integration.\\u2013Dropout Regularization: Implemented with a dropout rate of 0.5 before thefinal classification layer to prevent overfitting by randomly deactivating neu-rons during training.\", \"\\u2013Final Classification FC Layer 2: The last layer outputs predictions corre-spondingtothenumberofclasses,leveragingintegratedfeaturesforaccurateclassification.\", \"Enhanced Performance: The strategic use of two different strides in the maxpooling layers significantly enhances test accuracy, allowing the network to ef-fectively capture and integrate hierarchical features from varying spatial scales.\", \"This design choice optimizes the network\\u2019s ability to discern between benign andmalignant brain tumor images with increased precision and reliability in medicalimaging applications.\", \"Title Suppressed Due to Excessive Length 52.3 Applying LIME on Proposed ModelsApplication on Classification Models: For the classification task, LIME isapplied to understand the reasons behind the classification of brain MRI imagesas benign or malignant or no tumor.\", \"The steps include:1.\", \"Selecting Instances: A subset of correctly classified and misclassified imagesfrom the test set is selected for explanation.\", \"2.\", \"Generating Explanations: LIME generates explanations for each selected in-stance by highlighting the regions of the image that most influenced themodel\\u2019s decision.\", \"3.\", \"Visualizing Explanations: The explanations are visualized as heatmaps over-laid on the original images, showing which parts of the image contributedmost to the classification.\", \"Application on Segmentation Models: For the segmentation task, LIMEhelps to explain why certain regions of the MRI images were segmented as tumorareas.\", \"The steps include:1.\", \"Selecting Instances: A set of segmented images, including both successfuland failed segmentations, is chosen for explanation.\", \"2. Generating Perturbations: Perturbations are applied to the images, and themodel\\u2019s segmentation predictions for these perturbed samples are obtained.\", \"3.\", \"Fitting an Interpretable Model: A simpler model is fitted to approximate thesegmentation decisions of the original model.\", \"4. Visualizing Explanations: The important features (image segments) that in-fluenced the segmentation decision are visualized, helping to understand themodel\\u2019s behavior.\", \"2.4 Algorithm for the Proposed WorkThe following steps outline the algorithm we employed in our research for pre-processing data, training custom models for classification and segmentation, ap-plying LIME for explainability, and evaluating the models using specific metrics.\", \"Step 1: Data Preparation:1.\", \"Data Acquisition:\", \"We collected a publicly available dataset of brain MRIimages.\", \"The dataset includes labels for classification (glioma , pituitary ,meningioma ,no tumor) and ground truth masks for segmentation.\", \"2.\", \"Data Preprocessing:We normalized the images to a standard scale (e.g., [0,1]).\", \"The images were resized to a fixed size (e.g., 256x256 pixels) to ensureuniform input dimensions.\", \"Used a custom data loading function to load ,preprocess MRI Images and load them in a Dataframe.\", \"We augmented thedataset using techniques such as cropping out the brain sections only toenhance model generalization and accuracy and also adding data augmenta-tions such as horizontal flips and rotations.6\", \"Pathikreet and GargiStep 2: Model Training:1.\", \"Classification Model: We defined a custom CNN architecture for classifica-tion.\", \"The model parameters were initialized, and the model was compiledwith an appropriate optimizer (Adam) and loss function (cross-entropy loss).\", \"The dataset was split into training, validation, and test sets.\", \"We trained themodel on the training set and validated it on the validation set.\", \"The trainingprocesswasmonitored,andearlystoppingwasappliedtopreventoverfitting.\", \"2. Segmentation Model: We defined a custom U-Net architecture for segmen-tation.\", \"The model parameters were initialized, and the model was com-piled with an appropriate optimizer (Adam) and loss function (binary cross-entropy with Dice coefficient).\", \"The model was trained on the training setand validated on the validation set.\", \"The training process was monitored,and early stopping was applied to prevent overfitting.\", \"Step 3: Model Evaluation1.\", \"Classification Evaluation:\", \"We evaluated the trained classification model onthe test set using metrics such as accuracy, precision, recall, and F1 score.\", \"2. Segmentation Evaluation:\", \"We evaluated the trained segmentation model onthe test set using metrics such as Dice coefficient, Intersection over Union(IoU), and pixel-wise accuracy.\", \"Step 4: Applying LIME for Explainability1.\", \"Instance Selection: We selected a subset of correctly classified and misclas-sified images from the test set for classification explainability.\", \"We selected aset of successful and failed segmentations from the test set for segmentationexplainability.\", \"2. Generate Explanations: We applied LIME to generate explanations for theselected instances.\", \"For classification, we created perturbed samples and fitan interpretable model to approximate the classifier\\u2019s behavior locally.\", \"Forsegmentation, we created perturbed samples and fit an interpretable modelto approximate the segmenter\\u2019s behavior locally.\", \"3.\", \"VisualizationandInterpretation:Wevisualizedtheexplanationsasheatmapsto highlight the regions of the images that contributed most to the model\\u2019spredictions.\", \"The visualizations were interpreted to understand the model\\u2019sdecision-making process.\", \"Step 5: Quantitative Evaluation of Explanations1.\", \"Explanation Stability: We measured the consistency of the explanationswhen the input image was slightly perturbed.\", \"2. Explanation Sparsity:\", \"We evaluated the proportion of the image highlightedin the explanation to assess conciseness.\", \"3. Explanation Fidelity: We assessed how well the interpretable model approx-imated the original model\\u2019s predictions.\", \"Title Suppressed Due to Excessive Length 73 Experimental Details3.1 Dataset UsedFor Classification Tasks:\", \"For the classification tasks, we employed a com-bined dataset comprising 7023 images of human brain MRI images.\", \"These im-ages are categorized into four distinct classes: glioma, meningioma, no tumor,andpituitary.\", \"Thisdatasetamalgamatesimagesfrommultiplesources,providinga diverse and comprehensive collection for robust classification model trainingand evaluation.\", \"Sources of Data:1.\", \"Jun Cheng\\u2019s Brain Tumor Dataset [11, 12]: This dataset contains 3064 T1-weighted contrast-enhanced images from 233 patients, distributed acrossthree tumor types: meningioma (708 slices), glioma (1426 slices), and pitu-itary tumor (930 slices).\", \"The dataset is split into four subsets, each archivedin a .zip file containing approximately 766 slices.\", \"The data is organized inMATLAB(.mat)format,witheachfilestoringtheimagedataandassociatedannotations.\", \"2. Br35H Dataset: The Br35H dataset contains a diverse collection of brainMRI images used for various brain tumor classification tasks.\", \"The datasetincludes images categorized into the four classes mentioned above, furtherenhancing the diversity and robustness of our classification model.\", \"3. SarTaj Dataset: This dataset includes additional brain MRI images used tocomplement the classification model\\u2019s training dataset, ensuring a robustlearning process.\", \"The combined dataset from these sources provided a rich variety of images,enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumors.\", \"The images in the combined dataset were preprocessed to ensure uniformity inresolution and intensity normalization, followed by augmentation techniques tofurther increase the dataset size and variability, such as rotation, flipping, andcontrast adjustment.\", \"For Segmentation Tasks:Brain MRI Segmentation Dataset: This dataset includes brain MR images alongwith manual FLAIR abnormality segmentation masks.\", \"The images were sourcedfrom The Cancer Imaging Archive (TCIA) and correspond to 110 patients in-cludedinTheCancerGenomeAtlas(TCGA)lower-gradegliomacollection,eachhavingatleastonefluid-attenuatedinversionrecovery(FLAIR)sequenceandas-sociated genomic clusterdata.\", \"FLAIR sequences, known for their high sensitivityto lesions and abnormalities within the brain tissue.\", \"Manual segmentation maskswere created by expert radiologists, delineating the abnormal regions with highprecision.\", \"Covers 110 patients, providing a comprehensive and diverse dataset8 Pathikreet and Gargifor training and evaluating segmentation models.\", \"Each image has been standardized to a uniform resolution, ensuring consistencyacross the dataset.\", \"Images were preprocessed to correct for intensity inhomo-geneities and were normalized to have zero mean and unit variance.\", \"Data aug-mentation techniques, including random rotations, scaling, and elastic deforma-tions, were applied to increase the effective size of the training set and to improvethe robustness of the segmentation model.\", \"The segmentation masks provide abinary representation of the tumor regions, with 1s indicating the presence of atumor and 0s representing healthy tissue.\", \"These masks are crucial for trainingsupervised learning models for segmentation tasks.3.2 Evaluation MetricsSegmentation : For evaluating the performance of our segmentation models,we employed several standard metrics to ensure a comprehensive assessment:1.\", \"Dice Coefficient (Dice Similarity Index, DSC):The Dice coefficient measuresthe overlap between the predicted segmentation and the ground truth.\", \"Itranges from 0 to 1, with 1 indicating perfect overlap.\", \"D(A, B) =2|A\\u2229B||A|+|B|(1)where Ais the set of pixels in the predicted segmentation and Bis the setof pixels in the ground truth segmentation.\", \"2. Intersection over Union (IoU, Jaccard Index):IoU measures the ratio of theintersection to the union of the predicted and ground truth segmentations.\", \"It ranges from 0 to 1, with 1 indicating perfect segmentation.\", \"IoU=Area of overlapArea of union=A\\u2229BA\\u222aB(2)3.\", \"Precision (Positive Predictive Value):\", \"Precision indicates the proportion oftrue positive pixels among all pixels that were predicted as positive.\", \"Precision =TPTP+FP(3)where TP is the number of true positive pixels and FP is the number of falsepositive pixels.\", \"4.\", \"Recall (Sensitivity, True Positive Rate):\", \"Recall measures the proportion oftrue positive pixels among all actual positive pixels.\", \"Recall =TPTP+FN(4)where FN is the number of false negative pixels.\", \"Title Suppressed Due to Excessive Length 95.\", \"F1 Score: The F1 score is the harmonic mean of precision and recall, pro-viding a single metric that balances both.\", \"F1 Score =2\\u00d7Precision \\u00d7RecallPrecision +Recall(5)\", \"6.\", \"Hausdorff Distance:\", \"This metric measures the maximum distance betweenthe predicted segmentation boundary and the ground truth boundary, pro-viding insight into the spatial accuracy of the segmentation.\", \"dH(X, Y) =maxsupx\\u2208Xd(x, Y),supy\\u2208Yd(X, y)(6)where sup represents the supremum operator and d(a, B) =infb\\u2208Bd(a, b).\", \"infis the infimum operator d(a, B)quantifies the distance from a point a\\u2208Xto the subset B\\u2286X.d(a, b)is the Euclidean distance between points aandb.Classification : For the classification tasks, the following metrics were utilizedto evaluate the model performance:1.\", \"Accuracy: Accuracy is the ratio of correctly predicted instances to the totalinstances.\", \"It provides a straightforward measure of overall performance.\", \"2. Confusion Matrix:\", \"The confusion matrix provides a detailed breakdown ofthe classification performance, displaying the counts of true positives, truenegatives, false positives, and false negatives for each class.\", \"3.\", \"Receiver Operating Characteristic (ROC) Curve and Area Under the Curve(AUC): The ROC curve plots the true positive rate against the false pos-itive rate at various threshold settings.\", \"The AUC provides a single scalarvalue summarizing the model\\u2019s performance across all thresholds.\", \"An AUCof 1 indicates perfect classification, while an AUC of 0.5 suggests no betterperformance than random guessing.\", \"Accuracy =TP+TNTP+TN+FP+FN(7)Precision, Recall and F1-scores are also used to evaluate the results.\", \"LIME (Local Interpretable Model-Agnostic Explanations) :\", \"For the in-terpretability of our models, particularly in understanding their decision-makingprocesses, we employed LIME to generate explanations.\", \"The following metricswere used to evaluate the quality of the explanations provided by LIME:\", \"1. Explanation Stability: Explanation stability measures the consistency of ex-planations when slight perturbations are made to the input data.\", \"High sta-bility indicates that small changes in the input do not significantly alter theexplanation.\", \"Stability = 1\\u22121nXi=1n|E(xi)\\u2212E(x\\u2032i)| (8)where E(xi)is the explanation for instance xiandE(x\\u2032i)is the explanationfor the perturbed instance and nis the number of samples.10 Pathikreet and Gargi2. Explanation Sparsity: Explanation sparsity evaluates the proportion of fea-tures used in the explanation compared to the total number of features.\", \"Sparse explanations are preferred as they are easier to interpret.\", \"Sparsity = 1\\u2212Number of features in explanationTotal number of features(9)3.\", \"ExplanationFidelity:Explanationfidelitymeasureshowwelltheexplanationapproximates the original model\\u2019s behavior.\", \"High fidelity indicates that thesurrogate model used for generating explanations closely mimics the originalmodel.\", \"Fidelity = 1\\u22121nnXi=1(f(xi)\\u2212g(xi))2(10)where f(xi)is the prediction of the original model for instance xiandg(xi)is the prediction of the surrogate model.\", \"3.3 Setting Up Parameter ValuesFor both the segmentation and classification tasks, careful selection and tuningof hyperparameters were crucial to optimizing the performance of our neuralnetwork models.\", \"The parameter values for various components of our proposedwork are detailed below:Segmentation Model (Enhanced U-Net)1.\", \"Learning Rate: 1e-4.\", \"The learning rate determines the step size at each itera-tion while moving toward a minimum of the loss function.\", \"A smaller learningrate ensures a stable convergence but might require more epochs.\", \"2. Batch Size: 16.\", \"The batch size indicates the number of training samplesused in one forward/backward pass.\", \"A moderate batch size balances memoryefficiency and gradient stability.\", \"3.\", \"Epochs: 50\", \"The number of epochs defines how many times the learning al-gorithm will work through the entire training dataset.\", \"More epochs can leadto better convergence but may also increase the risk of overfitting.\", \"4.\", \"Dropout Rate:\\u2013Initial layers: 0.1\\u2013Middle layers: 0.2\\u2013Final layer: 0.3Dropoutisusedtopreventoverfittingbyrandomlysettingafractionofinputunits to 0 at each update during training time.\", \"Different rates are used fordifferent layers to balance regularization and learning.\", \"5.\", \"OptimizerAdam.\", \"TheAdamoptimizerischosenforitsadaptivelearningratecapabilities and efficient handling of sparse gradients, making it suitable fortraining deep neural networks.\", \"Title Suppressed Due to Excessive Length 116.\", \"Loss Function:\", \"Binary cross entropy is used for measuring the performanceof a classification model whose output is a probability value between 0 and1.\", \"It is well-suited for segmentation tasks where the output is a binary mask.\", \"Binary Cross Entropy =\\u22121NNXiMXjyijlog(pij) (11)where Nis the number of samples and Mis the number of classes.\", \"Classification Model (Custom CNN)1.\", \"Learning Rate: 1e-3.\", \"A higher learning rate compared to the segmentationmodel to ensure faster convergence while maintaining stability.\", \"2. Batch Size: 32.\", \"A larger batch size to improve gradient estimation accuracyand training speed.\", \"3.\", \"Epochs: 100.\", \"A higher number of epochs to ensure sufficient training forconvergence.\", \"4.\", \"Dropout Rate: 0.5.\", \"A higher dropout rate to strongly regularize the modeland prevent overfitting, given the smaller dataset size.\", \"5.\", \"Optimizer Adam.\", \"Adam optimizer is chosen for its efficient gradient compu-tation and adaptive learning rates, facilitating robust training.\", \"6.\", \"Loss Function: Cross Entropy Loss.\", \"Cross entropy loss is used for multi-classclassification tasks, where it evaluates the performance of a classificationmodel whose output is a probability value between 0 and 1 for each class.\", \"Cross Entropy =\\u22121NNXj=1[tjlog(pj)\", \"+ (1 \\u2212tj)log(1\\u2212pj)](12)\", \"Nis the number of data points, tjis the truth value and pjis the Softmaxprobability for taking the truth value.\", \"LIME Parameters1. Number of Samples: 1000.\", \"The number of perturbed samples generated toexplain each prediction.\", \"More samples can improve explanation fidelity butincrease computational cost.\", \"2. KernelWidth:0.25.Thewidthofthekernelusedforweightingtheperturbedsamples.\", \"A smaller width focuses the explanation on samples closer to theoriginal instance.\", \"3.\", \"Feature Selection Method: Forward Selection.\", \"The method used for selectingthe most important features in the perturbed samples.\", \"Forward selectioniteratively adds features to improve explanation quality.\", \"4.\", \"Regularization: L1 Regularization.\", \"L1 regularization encourages sparsity inthe explanation, making it more interpretable by focusing on the most in-fluential features.12 Pathikreet and Gargi5.\", \"Segmenter:\", \"Quickshift\\u2013Kernel Size: 4.\", \"Controls the spatial scale of the segmentation.\", \"A largerkernel size results in larger segments.\\u2013Max Distance: 200.\", \"Limits the distance in the color space between twopixels to be merged.\", \"\\u2013Ratio: 0.5.\", \"Balances the color proximity and spatial proximity.\", \"A higherratio gives more importance to color similarity.\", \"Quickshift is a mode-seeking segmentation algorithm that clusters pixelsbased on color similarity and spatial proximity.\", \"It is used to generate super-pixels, which are smaller segments of the image that preserve local informa-tion.\", \"4 Results and DiscussionTable 1 presents the evaluation results of our proposed Custom CNN modelcompared to ResNet32 and VGG16 models.\", \"The Custom CNN model achieved the highest accuracy (98.70%) comparedto ResNet32 (96.35%) and VGG16 (96.2%), indicating superior overall perfor-mance in classifying brain tumors.\", \"The Custom CNN model also demonstratedthe highest precision (97.63%), compared to ResNet32 (95.24%) and VGG16(95.78%), indicating a lower rate of false positives.\", \"Recall:\", \"The Custom CNNmodel achieved a recall of 97.64%, slightly higher than ResNet32 (96.12%) andVGG16 (95.12%), indicating a higher rate of true positives.\", \"The Custom CNNmodel showed the highest F1-Score (97.47%), compared to ResNet32 (96.15%)and VGG16 (95.76%), balancing precision and recall effectively.\", \"The CustomCNN model had the highest LIME Explanation Stability Score (0.923), com-pared to ResNet32 (0.846) and VGG16 (0.687), indicating more stable explana-tions across similar inputs.\", \"The Custom CNN model had a LIME ExplanationSparsity Score of 0.208, compared to ResNet32 (0.196) and VGG16 (0.225), withlower sparsity indicating more concise explanations.\", \"ResNet32 had the highestLIME Explanation Fidelity Score (0.556), followed by VGG16 (0.418), and theCustom CNN model (0.310).\", \"This measures how well the explanation model ap-proximates the original model.\", \"These results demonstrate that the Custom CNN model generally outperformsboth ResNet32 and VGG16 in terms of classification accuracy, precision, recall,and F1-Score, while providing highly stable and reasonably concise explanationsas measured by LIME.\", \"Table 2 presents the evaluation results of our proposed Custom U-Net modelcompared with a U-Net model that uses ResNet as its encoder.\", \"The Custom U-Net model achieved a higher validation accuracy (99.8%) compared to the U-Netwith ResNet as its encoder (99.11%).\", \"The Custom U-Net model demonstrated alower validation loss (0.0132) than the U-Net with ResNet (0.0425), indicatingbetter performance in minimizing error.\", \"The IoU score, which measures the over-lapbetweenthepredictedandgroundtruthsegments,washigherfortheCustomU-Net model (0.889) compared to the ResNet-based U-Net (0.847), suggest-ing more accurate segmentation.\", \"The Custom U-Net model had a higher LIMETitle Suppressed Due to Excessive Length 13Table 1.\", \"Evaluation Results of proposed Custom CNN model compared to ResNetand VGG16Evaluation Metrics Custom CNN Model ResNet32 VGG16Accuracy 98.70% 96.35% 96.2%Precision 97.63 95.24 95.78Recall 97.64 96.12 95.12F1-Score 97.47 96.15 95.76LIME Explaination Stability Score 0.923 0.846 0.687LIME Explaination Sparsity Score 0.208 0.196 0.225LIME Explaination Fidelity Score 0.310 0.556 0.418Explanation Stability Score (0.8169) versus the ResNet-based U-Net (0.7873),indicating more stable explanations across similar inputs.\", \"Both models showedsimilarLIMEExplanationSparsityScores,withtheCustomU-Netmodelhavinga slightly lower score (0.1190) compared to the ResNet-based U-Net (0.1221).\", \"Lower sparsity indicates more concise explanations.\", \"The ResNet-based U-Netmodel exhibited a higher LIME Explanation Fidelity Score (0.6036) comparedto the Custom U-Net model (0.5447), which measures how well the explanationmodel approximates the original model.\", \"Figure 3 displays the Confusion MatrixTable 2.\", \"Evaluation Results for the proposed Custom U-net model compared withResUnetEvaluation Metrics Custom U-net Model U-net with ResNet as EncoderValidation Accuracy 99.8% 99.11%Validation Loss 0.0132 0.0425Intersection over Union Score (IoU) 0.889 0.847LIME Explaination Stability Score 0.8169 0.7873LIME Explaination Sparsity Score 0.1190 0.1221LIME Explaination Fidelity Score 0.5447 0.6036of custom CNN.\", \"Figure 4 displays the Plot of ground truth scans , masks andpredictedmasksandlabelsalongwithIoUScore.\", \"Figure5displaystheMRIScanon which LIME visualizations are generated.\", \"Figure 6 displays the LIME Visual-izations of Custom CNN with feature heatmap , positive and negative influencesand top three feature visualization.\", \"Figure 7 displays the plot of ground truthscans , masks and predicted masks and labels along with IoU Score NegativeClassifier acheiving perfect IoU of 1.14 Pathikreet and GargiFig.\", \"3.Confusion Matrix of custom CNNFig.\", \"4.Plot of ground truth scans , masks and predicted masks and labels along withIoU ScoreFig.\", \"5.MRI Scan on which LIME visualizations are generated5 Conclusion and Future Scope\", \"In this study, we developed and evaluated advanced neural network architecturesforbraintumorclassificationandsegmentationusingMRIimages.\", \"OurenhancedTitle Suppressed Due to Excessive Length 15Fig.\", \"6.LIME Visualizations of Custom CNN with feature heatmap , positive andnegative influences and top three feature visualizationFig.\", \"7.Plot of ground truth scans , masks and predicted masks and labels along withIoU Score Negative Classifier acheiving perfect IoU of 1U-Net model demonstrated improved segmentation performance, while our cus-tom CNN showed significant accuracy in classification tasks.\", \"The integration ofExplainable AI (XAI) techniques, particularly LIME, provided valuable insightsinto the model\\u2019s decision-making processes, enhancing interpretability and trust-worthiness.\", \"Combining multiple neural network architectures, such as integrating attentionmechanisms or transformers, could improve both classification and segmenta-tion performance.\", \"Leveraging larger and more diverse datasets can improve thegeneralizability of our models across different populations and imaging modal-ities.\", \"Developing models capable of real-time processing can facilitate clinicalapplications, enabling quicker diagnosis and treatment planning.\", \"Incorporatingmore advanced XAI techniques can provide deeper insights into model behavior,improving interpretability and clinical acceptance.\", \"Utilizing pre-trained modelson broader datasets and fine-tuning them on specific medical imaging tasks canenhance model performance and reduce training time.\", \"Implementing these mod-els in clinical workflows and electronic health records (EHR) systems can aid inautomated diagnosis and decision support.\", \"Bibliography\", \"[1] S\\u00e9rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva.\", \"Braintumor segmentation using convolutional neural networks in mri images.\", \"IEEE transactions on medical imaging , 35(5):1240\\u20131251, 2016.\", \"[2] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, andHesham FA Hamed.\", \"A review on brain tumor diagnosis from mri images:Practical implications, key achievements, and lessons learned.\", \"Magneticresonance imaging , 61:300\\u2013318, 2019.\", \"[3] Ehab F Badran, Esraa Galal Mahmoud, and Nadder Hamdy.\", \"An algorithmfor detecting brain tumors in mri images.\", \"In The 2010 International Con-ference on Computer Engineering &amp; Systems , pages 368\\u2013373.\", \"IEEE, 2010.\", \"[4] Milica M Bad\\u017ea and Marko \\u010c Barjaktarovi\\u0107.\", \"Classification of brain tumorsfrom mri images using a convolutional neural network.\", \"Applied Sciences ,10(6):1999, 2020.\", \"[5] Prashant, Gargi Srivastava, and Vibhav Prakash Singh.\", \"Ensemble of deeplearning approaches for detection of brain.\", \"In International Journal of Ad-vanced Networking and Applications - IJANA:\", \"1st International Conferenceon Advancements in Smart Computing and Information Security, ASCIS2022, Rajkot, India, November 24-26, 2022 , pages 11\\u201316.\", \"Eswar Publica-tions, 2022.\", \"[6] Alejandro Barredo Arrieta, Natalia D\\u00edaz-Rodr\\u00edguez, Javier Del Ser, AdrienBennetot, Siham Tabik, Alberto Barbado, Salvador Garc\\u00eda, Sergio Gil-L\\u00f3pez, Daniel Molina, Richard Benjamins, et al.\", \"Explainable artificial in-telligence (xai): Concepts, taxonomies, opportunities and challenges towardresponsible ai.\", \"Information fusion , 58:82\\u2013115, 2020.\", \"[7] Bas HM Van der Velden, Hugo J Kuijf, Kenneth GA Gilhuijs, and Max AViergever.\", \"Explainable artificial intelligence (xai) in deep learning-basedmedical image analysis.\", \"Medical Image Analysis , 79:102470, 2022.\", \"[8] J\\u00fcrgen Dieber and Sabrina Kirrane.\", \"Why model why?\", \"assessing thestrengths and limitations of lime.\", \"arXiv preprint arXiv:2012.00093 , 2020.\", \"[9] Sasha Targ, Diogo Almeida, and Kevin Lyman.\", \"Resnet in resnet: General-izing residual architectures.\", \"arXiv preprint arXiv:1603.08029 , 2016.\", \"[10] Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.Going deeper in spiking neural networks: Vgg and residual architectures.\", \"Frontiers in neuroscience , 13:95, 2019.\", \"[11] Jun Cheng, Wei Huang, Shuangliang Cao, Ru Yang, Wei Yang, ZhaoqiangYun, Zhijian Wang, and Qianjin Feng.\", \"Enhanced performance of braintumor classification via tumor region augmentation and partition.\", \"PloSone, 10(10):e0140381, 2015.\", \"[12] Jun Cheng, Wei Yang, Meiyan Huang, Wei Huang, Jun Jiang, Yujia Zhou,Ru Yang, Jie Zhao, Yanqiu Feng, Qianjin Feng, et al. Retrieval of braintumors by adaptive spatial pooling and fisher vector representation.\", \"PloSone, 11(6):e0157112, 2016.\"], \"keywords\": [\"shift\", \"accountability\", \"Conclusion\", \"classification\", \"Evaluation\", \"balance\", \"methodology\", \"ment\", \"approach\", \"accuracy\", \"ization\", \"nature\", \"presentation\", \"prediction\", \"learning\", \"net\", \"Precision\", \"Conference\", \"Score\", \"proportion\", \"space\", \"Institute\", \"intro-\", \"Connections\", \"Segmentation\", \"Visualizations\", \"advent\", \"region\", \"Mahmoud\", \"point\", \"age\", \"Journal\", \"fairness\", \"VGG\", \"Yang\", \"Performance\", \"Almeida\", \"absent\", \"Normalization\", \"evaluation\", \"cludedinTheCancerGenomeAtlas(TCGA)lower\", \"1\", \"Accuracy\", \"method\", \"predictedmasksandlabelsalongwithIoUScore\", \"Computing\", \"pixel\", \"keyword\", \"terpretability\", \"Netmodelaims\", \"Methodology\", \"choice\", \"ron\", \"Details\", \"patient\", \"FP\", \"robustness\", \"function\", \"operator\", \"architecture\", \"processing\", \"stability\", \"Sensitivity\", \"label\", \"arXiv\", \"guessing\", \"interpretability\", \"b\", \"Gargi\", \"variability\", \"influence\", \"Function\", \"annotation\", \"result\", \"forbraintumorclassificationandsegmentationusingmriimage\", \"Images\", \"trial\", \"uniformity\", \"sequence\", \"auc\", \"explainability\", \"process\", \"Network\", \"normalization\", \"Hamdy\", \"meningioma\", \"tation\", \"technique\", \"sup\", \"interpretation\", \"conciseness\", \"time\", \"dataset\", \"Enhanced\", \"Jiang\", \"iou\", \"Srivastava\", \"difference\", \"byincorporatingtheseenhancement\", \"Zhao\", \"change\", \"Alves\", \"application\", \"matrix\", \"wevisualizedtheexplanationsasheatmap\", \"Barjaktarovi\\u0107\", \"perturbation\", \"brain\", \"Explanations\", \"+\", \"Yun\", \"Method\", \"collection\", \"Preparation\", \"Prashant\", \"Figure5displaystheMRIScan\", \"cost\", \"union\", \"that\", \"risk\", \"Mis\", \"segmentation\", \"consistency\", \"subset\", \"implication\", \"distance\", \"training\", \"Classification\", \"entropy\", \"Layer\", \"truth\", \"activation\", \"Scope\", \"metric\", \"step\", \"Ellah\", \"use\", \"Engineering\", \"Dataframe\", \"misclassification\", \"Scan\", \"Gilhuijs\", \"development\", \"performance\", \"sult\", \"ing\", \"classifier\", \"explanationfidelity\", \"integration\", \"flip\", \"11\\u201316\", \"-\", \"taxonomy\", \"Tumors\", \"pitu-\", \"curve\", \"stride\", \"table\", \"Recall\", \"tion\", \"we\", \"Section\", \"7.plot\", \"decision\", \"overfitting\", \"Wang\", \"workflow\", \"arxiv:2012.00093\", \"tuning\", \"i)|\", \"Feng\", \"Khalaf\", \"speed\", \"index):iou\", \"diversity\", \"lapbetweenthepredictedandgroundtruthsegment\", \"size\", \"sample\", \"system\", \"custom\", \"setting\", \"prevention\", \"stance\", \"overlap\", \"Work\", \"support\", \"telligence\", \"Badran\", \"slice\", \"instance\", \"assessment\", \"potential\", \"diagnostic\", \"score\", \"ai\", \"perspective\", \"Table\", \"encoder\", \"Cao\", \"segment\", \"handling\", \"Ad-\", \"IoU\", \"class\", \"conclusion\", \"lime\", \"Security\", \"optimizer\", \"ally\", \"fusion\", \"representation\", \"variance\", \"spondingtothenumberofclasse\", \"|a|+|b|(1\", \"neuroscience\", \"minimum\", \"insight\", \"Algorithm\", \"opportunity\", \"tureextractionandclassificationaccuracy\", \"fidelity\", \"Fidelity\", \"filter\", \"ap-\", \"need\", \"el\", \"al\", \"IEEE\", \"LIME\", \"y\", \"=\", \"CNN\", \"type\", \"intelligence\", \"explanationfidelitymeasureshowwelltheexplanation\", \"November\", \"Dieber\", \"segmenter\", \"modification\", \"resolution\", \"malization\", \"detection\", \"each\", \"Kirrane\", \"input\", \"detail\", \"uncertainty\", \"model\", \"Sparsity\", \"enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumor\", \"visualizationandinterpretation\", \"working\", \"rotation\", \"value\", \"Application\", \"measure\", \"intersection\", \"part\", \"breakdown\", \"connection\", \"LIMETitle\", \"clusterdata\", \"algorithm\", \"rate\", \"review\", \"adjustment\", \"scan\", \"Archive\", \"tissue\", \"hyperparameter\", \"similarLIMEExplanationSparsityScores\", \"lesson\", \"validation\", \"stage\", \"gradient\", \"tumor\", \"Explainability\", \"Stability\", \"flipping\", \"structure\", \"scale\", \"I\", \"transformer\", \"plot\", \"TP+FP(3\", \"ference\", \"hancement\", \"sparsity\", \"ture\", \"parameter\", \"transaction\", \"data\", \"update\", \"FN\", \"Sciences\", \"pool-\", \"relu\", \"Ye\", \"d(a\", \"bility\", \"B\", \"challenge\", \"AUC\", \"presence\", \"Fig\", \"work\", \"Chowdhury\", \"XAI\", \"Size\", \"MATLAB(.mat)format\", \"width\", \"jyijlog(pij\", \"OptimizerAdam\", \"convergence\", \"Singh\", \"error\", \"initialization\", \"invariance\", \"similarity\", \"Dataset\", \"probability\", \"extraction\", \"set\", \"category\", \"Loss\", \"Cheng\", \"control\", \"quality\", \"acceptance\", \"recall\", \"task\", \"Model\", \"lesion\", \"Netmodelhaving\", \"term\", \"preprocess\", \"number\", \"Huang\", \"dimension\", \"component\", \"Index\", \"Section4\", \"display\", \"Plot\", \"importance\", \"analysis\", \"%\", \"mance\", \"positive\", \"bustness\", \"loss\", \"Retrieval\", \"boundary\", \"element\", \"layer\", \"consequence\", \"itie\", \"Distance\", \"study\", \"256x256\", \"toimprovegeneralizationandpreventoverfitting\", \"enhancement\", \"partition\", \"efficiency\", \"Quickshift\", \"coefficient\", \"source\", \"Abstract\", \"Velden\", \"ascis\", \"this\", \"operation\", \"behavior\", \"population\", \"both\", \"planning\", \"test\", \"section\", \"figure\", \"Hamed\", \"selection\", \"D(A\", \"generalization\", \"Union\", \"supx\\u2208Xd(x\", \"xai\", \"network\", \"Networking\", \"cluster\", \"Zhou\", \"precision\", \"Curve\", \"paper\", \"limit\", \"Dropout\", \"radiologist\", \"pro-\", \"transparency\", \"AI\", \"negative\", \"Coefficient\", \"count\", \"proximity\", \"effectiveness\", \"Models\", \"limitation\", \"Classifier\", \"ratio\", \"ability\", \"developer\", \"resnet\", \"General-\", \"feature\", \"convolution\", \"capability\", \"unit\", \"Systems\", \"other\", \"pooling\", \"Kuijf\", \"glioma\", \"geneitie\", \"frontier\", \"file\", \"ResNet32\", \"Intersection\", \"com-\", \"box\", \"reliance\", \"Awad\", \"Selection\", \"area\", \"imaging\", \"finding\", \"Matrix\", \"xiandg(xi\", \"mask\", \"variety\", \"Area\", \"map\", \"TP+FN(4\", \"incorporation\", \"Liu\", \"Parameters\", \"Results\", \"Pathikreet\", \"domain\", \"eration\", \"Adam\", \"Acquisition\", \"it\", \"IJANA\", \"Tumor\", \"achievement\", \"Data\", \"Advancements\", \"b)is\", \"t1-\", \"gorithm\", \"they\", \"threshold\", \"visualization\", \"ResNet\", \"sensitivity\", \"datum\", \"regularization\", \"epoch\", \"gradegliomacollection\", \"reason\", \"worthiness\", \"image\", \"mechanism\", \"Pereira\", \"Entropy\", \"explanation\", \"Dice\", \"scaling\", \"pass\", \"diagnosis\", \"abnormality\", \"improvement\", \"thisdatasetamalgamatesimagesfrommultiplesource\", \"which\", \"augmentation\", \"U\", \"Rate\", \"planation\", \"Tasks\", \"generalizability\", \"Roy\", \"Y),supy\\u2208Yd(X\", \"Targ\", \"path\", \"mean\", \"Pinto\", \"bibliography\", \"output\", \"Lyman\", \"discussion\", \"researcher\", \"tp\", \"sibilitie\", \"reliability\", \"features(9\", \"problem\", \"Net\", \"Viergever\", \"Value\", \"VGG16\", \"Sengupta\", \"research\", \"Length\", \"Silva\", \"year\", \"TCIA\", \"strength\", \"arxiv:1603.08029\", \"nis\", \"Analysis\", \"Regularization\", \"Technology\", \"strategy\", \"block\"], \"word_count\": 6577, \"sentence_count\": 326}, \"analytics\": {\"word_count\": 6577, \"sentence_count\": 326, \"average_sentence_length\": 20.17, \"entity_count\": 524, \"keyword_count\": 523, \"most_common_entities\": [[\"CARDINAL\", 217], [\"ORG\", 109], [\"PERSON\", 88], [\"PERCENT\", 24], [\"DATE\", 23]], \"most_common_words\": [[\"model\", 84], [\"segmentation\", 51], [\"classification\", 44], [\"custom\", 39], [\"images\", 35], [\"net\", 31], [\"lime\", 31], [\"score\", 30], [\"u\", 29], [\"brain\", 28]]}, \"json_output\": \"{\\n  \\\"structured_data\\\": {\\n    \\\"entities\\\": [\\n      {\\n        \\\"text\\\": \\\"Brain Tumors\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net Models\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Pathikreet Chowdhury\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Rajiv Gandhi Institute of Petroleum Technology\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"229304\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"India\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"21cs2026,gsrivastava}@rgipt.ac.in\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"VGG\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the Explainable AI\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"99.79%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.889\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Addition-\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"wereportaLIMEexplanationstabilityscoreof0.8169andasparsity\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.1190\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"98.70%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.63%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.64%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1 - Score\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.63%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.923\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.203.These\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1.00\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Brain Tumor\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"AI\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1 IntroductionSegmentation\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the past few years\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Explainable AI\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"fed\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"AI\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"8\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"9\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"VGG Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"10\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Section 2\\\",\\n        \\\"label\\\": \\\"LAW\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Section 3\\\",\\n        \\\"label\\\": \\\"LAW\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2.1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Enhanced U-Net\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net:Title Suppressed Due to Excessive Length\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fig\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Neural Network for SegmentationResidual Connections: Residual\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Batch Normalization:\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Batch\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"SpatialDropoutisincorporatedwithineachconvolutionalblock\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2.2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Convolutional Neural Network\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Conv Layer 1\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4x4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Pathikreet and GargiFig\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Neural Network\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Conv Layer 2\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"64\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4x4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Conv Layer 3\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4x4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Conv Layer 4\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"128\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4x4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"first\\\",\\n        \\\"label\\\": \\\"ORDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"second\\\",\\n        \\\"label\\\": \\\"ORDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"max\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3x3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"third\\\",\\n        \\\"label\\\": \\\"ORDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"max\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3x3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fully Connected Layers:\\\",\\n        \\\"label\\\": \\\"FAC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fully Connected\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"512\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ReLU\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"two\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"max\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Proposed ModelsApplication on Classification Models\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Application on Segmentation Models:\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2.4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Data Acquisition\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"256x256\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Adam\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Adam\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Model Evaluation\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3.1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"7023\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jun Cheng\\u2019s\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Brain Tumor Dataset\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"12\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3064\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"233\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"three\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"glioma\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1426\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"930\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"766\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"four\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"SarTaj Dataset\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"FLAIR\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"The Cancer Imaging Archive\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"TCIA\\\",\\n        \\\"label\\\": \\\"LAW\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"110\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"FLAIR\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"110\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"dataset8 Pathikreet\\\",\\n        \\\"label\\\": \\\"FAC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Gargi\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"zero\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1s\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0s\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3.2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Evaluation MetricsSegmentation\\\",\\n        \\\"label\\\": \\\"EVENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Dice Coefficient (Dice Similarity Index\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"DSC):The Dice\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2|A\\u2229B|\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Ais\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Bis\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"IoU\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jaccard Index):IoU\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Predictive Value\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"TP+FP(3\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"TP\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"FP\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"True Positive Rate\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Recall =TPTP+FN(4\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"FN\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"a single metric\\\",\\n        \\\"label\\\": \\\"QUANTITY\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1 Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\u00d7Precision\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"d(a\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"d(a\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Euclidean\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Accuracy\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ROC\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"AUC\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\u22121\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"8)\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"xiandE(x\\u2032\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Pathikreet and Gargi\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\u2212Number\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fidelity\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\u22121\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3.3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Segmentation Model (Enhanced U-Net\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Batch Size\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"16\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"one\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"50\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"TheAdamoptimizerischosenforitsadaptivelearningrate\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Binary Cross Entropy\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"NNX\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"jyijlog(pij\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Nis\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Classification Model (Custom\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Batch Size\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"100\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Adam\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Adam\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"between 0 and 1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Cross Entropy\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"NNX\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Nis\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Softmax\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1000\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"L1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Pathikreet\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Gargi\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Max\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"200\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"two\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Custom CNN\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"98.70%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.35%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.2%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.63%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.24%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.78%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.64%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.12%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.12%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1-Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.47%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.15%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.76%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Custom\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.923\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.846\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.687\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.208\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.196\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.225\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.556\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.418\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.310\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet32\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1-Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Table 2\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Custom U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"99.8%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"99.11%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"The Custom U-Net\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.0132\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.0425\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.889\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.847\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"The Custom U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIMETitle Suppressed Due\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"13\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Custom CNN\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Model ResNet32\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"98.70%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.35%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.2%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.63\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.78\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.64\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.12\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"F1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"97.47\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"96.15\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"95.76\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.923\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.687\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.208\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.225\\\",\\n        \\\"label\\\": \\\"PRODUCT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fidelity Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.310 0.556 0.418\\\",\\n        \\\"label\\\": \\\"MONEY\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.8169\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.7873\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.1190\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.1221\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"LOC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.6036\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the Custom U-Net\\\",\\n        \\\"label\\\": \\\"FAC\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"the Confusion MatrixTable 2\\\",\\n        \\\"label\\\": \\\"LAW\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Custom U-net\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResUnetEvaluation Metrics Custom\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Model U-net\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ResNet\\\",\\n        \\\"label\\\": \\\"NORP\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"99.8%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"99.11%\\\",\\n        \\\"label\\\": \\\"PERCENT\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.0132 0.0425\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Union Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.889\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.847\\\",\\n        \\\"label\\\": \\\"MONEY\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"0.8169 0.7873\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fidelity Score\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Plot\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"three\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"7\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"IoU Score Negative\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"IoU\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1.14\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Pathikreet and GargiFig\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3.Confusion Matrix\\\",\\n        \\\"label\\\": \\\"QUANTITY\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fig\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4.Plot\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fig\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"LIME\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Conclusion and Future Scope\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"15\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fig\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"three\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Fig\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"1\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"U-Net\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"CNN\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Adriano Pinto\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Victor Alves\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Carlos A Silva\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"35(5):1240\\u20131251, 2016\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Mahmoud Khaled Abd-Ellah\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Ali Ismail Awad\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Ashraf AM Khalaf\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Hesham FA Hamed\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"61:300\\u2013318, 2019\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"3\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Esraa Galal Mahmoud\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Nadder Hamdy\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2010\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Computer Engineering &amp; Systems\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"368\\u2013373\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2010\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"4\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Applied Sciences\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"10(6):1999, 2020\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"5\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Gargi Srivastava\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Vibhav\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"International Journal of Ad-\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Networking and Applications - IJANA\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Advancements in Smart Computing and Information Security\\\",\\n        \\\"label\\\": \\\"WORK_OF_ART\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2022\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Rajkot\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"India\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"November 24-26, 2022\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11\\u201316\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2022\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"6\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Barredo Arrieta\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Natalia D\\u00edaz-Rodr\\u00edguez\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Javier Del Ser\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"AdrienBennetot\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Siham Tabik\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Alberto Barbado\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Salvador Garc\\u00eda\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Sergio\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Daniel Molina\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Richard Benjamins\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Concepts\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"58:82\\u2013115, 2020\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Velden\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Kenneth GA\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Gilhuijs\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Max A\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Viergever\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Medical Image Analysis\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"79:102470\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2022\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"8\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Sabrina Kirrane\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2020\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"9\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Diogo Almeida\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Kevin Lyman\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"General-\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2016\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"10\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Yuting Ye\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Robert Wang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Chiao Liu\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Kaushik\\\",\\n        \\\"label\\\": \\\"GPE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Roy\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Frontiers\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"13:95, 2019\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jun Cheng\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Wei Huang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Shuangliang Cao\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Ru Yang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Wei Yang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"ZhaoqiangYun\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Zhijian Wang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Qianjin Feng\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"10(10):e0140381,\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"2015\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"12\\\",\\n        \\\"label\\\": \\\"CARDINAL\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jun Cheng\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Wei Yang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Meiyan Huang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Wei Huang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jun Jiang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Yujia Zhou\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Ru Yang\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Jie Zhao\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Yanqiu Feng\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"Qianjin Feng\\\",\\n        \\\"label\\\": \\\"PERSON\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"al. Retrieval\\\",\\n        \\\"label\\\": \\\"ORG\\\"\\n      },\\n      {\\n        \\\"text\\\": \\\"11(6):e0157112, 2016\\\",\\n        \\\"label\\\": \\\"DATE\\\"\\n      }\\n    ],\\n    \\\"sentences\\\": [\\n      \\\"Enhanced Classification and Segmentation ofBrain Tumors in MRI Images Using CustomCNN and U-Net Models with XAIPathikreet Chowdhury and Gargi Srivastava[0000\\u22120001\\u22126770\\u2212561X]Rajiv Gandhi Institute of Petroleum Technology, Jais Uttar Pradesh 229304, India{21cs2026,gsrivastava}@rgipt.ac.inhttp://www.rgipt.ac.inAbstract.\\\",\\n      \\\"This study aims to classify brain tumors in MRI images intofour categories: glioma, meningioma, absent, and pituitary tumors, aswell as segment low-grade gliomas.\\\",\\n      \\\"We evaluate our proposed models onfour publicly available datasets to ensure robustness and generalizability.\\\",\\n      \\\"For classification tasks, we compare the performance of our custom CNNmodel against established models like ResNet and VGG.\\\",\\n      \\\"For segmenta-tion tasks, we compare our custom U-Net model with the original U-Netand ResNet-based encoders.\\\",\\n      \\\"To validate the effectiveness of our models,we employ the Explainable AI (XAI) method LIME, providing insightsinto why our custom architectures outperform others.\\\",\\n      \\\"Our custom U-Netmodel achieves a validation accuracy of 99.79% and an Intersection overUnion (IoU) score of 0.889 for low-grade glioma segmentation.\\\",\\n      \\\"Addition-ally,wereportaLIMEexplanationstabilityscoreof0.8169andasparsityscore of 0.1190.\\\",\\n      \\\"The proposed custom CNN model achieves a validationaccuracy of 98.70% , weighted avg precision of 97.63% , recall of 97.64%and weighted F1 - Score of 97.63%.\\\",\\n      \\\"The model achieves a LIME stabilityscore of 0.923 and a sparsity score of 0.203.These results highlight thepotential of our custom models to enhance accuracy and interpretabilityin brain tumor classification and segmentation tasks, offering significantimprovements over existing methodologies.\\\",\\n      \\\"The custom U-net model isalso an excellent negative classifier achieving a perfect 1.00 IoU score forclassifying MRI scans which do not have any tumor.\\\",\\n      \\\"Keywords: Brain Tumor \\u00b7Explainable AI \\u00b7LIME.\\\",\\n      \\\"1 IntroductionSegmentation and classification of brain tumors in MRI images have been exten-sively researched over the past few years.\\\",\\n      \\\"Researchers have been leveraging deepneural networks to achieve significant improvements in this area\\\",\\n      \\\"[1, 2, 3, 4, 5].\\\",\\n      \\\"However, the advent of Explainable AI (XAI)\\\",\\n      \\\"[6] has introduced new pos-sibilities, allowing us to scrutinize network structures and understand the un-derlying mechanisms behind their final results.\\\",\\n      \\\"This perspective is crucial fordiscerning why certain models perform better than others and how existing neu-ral network architectures can be modified for improved efficiency and accuracy.2 Pathikreet and GargiConsequently, this can lead to smaller networks that save training time and pro-vide faster results when deployed.\\\",\\n      \\\"Previously, deep learning models operated largely as black boxes.\\\",\\n      \\\"Input imageswere fed into the network, and based on the output, additional images that ledto misclassifications were incorporated into the training set to enhance learning.\\\",\\n      \\\"Developers focused on layer adjustment, feedback incorporation, loss prevention,and hyperparameter tuning, often relying on trial and error to achieve better re-sults.\\\",\\n      \\\"With XAI, the opaque nature of neural networks has transformed intoa transparent process, providing clear insights into the inner workings of thenetwork.\\\",\\n      \\\"This transparency empowers developers to exert more control over thenetwork, significantly reducing development time and reliance on trial and error\\\",\\n      \\\"[7].\\\",\\n      \\\"Moreover, traditional deep learning models were constrained to predicting pre-defined classes without the ability to indicate uncertainty.\\\",\\n      \\\"XAI now enablesus to identify when the network reaches an \\\\\\\"I don\\u2019t know\\\\\\\" stage, allowing forthe development of custom models tailored to specific needs rather than merelyadapting existing network architectures for different domains through transferlearning.\\\",\\n      \\\"Explainable AI also enhances the accountability, fairness, and transparency ofdeep learning models.\\\",\\n      \\\"This is particularly important in medical diagnostics,where the consequences of misclassification and segmentation can be severe.\\\",\\n      \\\"In this paper, we develop a custom segmentation and classification model, pro-viding interpretation with the LIME model\\\",\\n      \\\"[8].\\\",\\n      \\\"We compare our results withbenchmark neural network models such as ResNet [9] and VGG Net [10].\\\",\\n      \\\"The paper is organized as follows: we detail our methodology in Section 2, fol-lowed by the experimentation details in Section 3 and presentation of our resultsand their discussion in Section4.\\\",\\n      \\\"The conclusions and implications our findingsare presented in the Section 5.2 Methodology2.1 Proposed Neural Network for Segmentation\\\",\\n      \\\"The proposed segmentation network is a custom U-Net, specifically designed toenhance the segmentation of brain MRI images.\\\",\\n      \\\"The architecture retains thecanonical encoder-decoder structure of the original U-Net, with several enhance-ments aimed at capturing more complex features and improving segmentationperformance.\\\",\\n      \\\"Figure 1 displays the network architecture.\\\",\\n      \\\"Proposed Enhanced U-Net Our proposed Enhanced U-Net model intro-duces several critical modifications to the original U-Net architecture, aimedat augmenting its performance for brain tumor segmentation tasks.\\\",\\n      \\\"These en-hancements are strategically designed to improve feature extraction, model ro-bustness, and overall segmentation accuracy.\\\",\\n      \\\"Here are the key differences andenhancements compared to the original U-Net:Title Suppressed Due to Excessive Length 3Fig.\\\",\\n      \\\"1.Proposed Neural Network for SegmentationResidual Connections: Residual connections are integrated within each convo-lutional block.\\\",\\n      \\\"This strategy helps alleviate the vanishing gradient problem andfacilitates the training of deeper networks by allowing gradients to flow moreeffectively through the network.\\\",\\n      \\\"The residual path is implemented by addingthe input to the output of the convolutional layers within the block.\\\",\\n      \\\"Batch Normalization:\\\",\\n      \\\"Batch normalization layers are employed after each con-volutional layer.\\\",\\n      \\\"This technique stabilizes and accelerates the training process bynormalizing the activations, thus reducing internal covariate shift.\\\",\\n      \\\"It also allowsfor higher learning rates and reduces sensitivity to initialization.\\\",\\n      \\\"Spatial Dropout:\\\",\\n      \\\"SpatialDropoutisincorporatedwithineachconvolutionalblocktoimprovegeneralizationandpreventoverfitting.\\\",\\n      \\\"Thisformofdropoutrandomlydrops entire feature maps rather than individual elements, which is particularlyeffective for spatial data, ensuring that the model does not become overly relianton specific feature maps.\\\",\\n      \\\"Byincorporatingtheseenhancements,ourproposedEnhancedU-Netmodelaimsto significantly improve segmentation performance in terms of accuracy, robust-ness, and generalization capabilities.\\\",\\n      \\\"2.2 Proposed Neural Network for ClassificationThe proposed classification network is a custom Convolutional Neural Network(CNN) specifically designed for classifying brain MRI images into benign andmalignant tumors.\\\",\\n      \\\"It incorporates advanced convolutional layers with batch nor-malization, ReLU activations, and strategic pooling operations to enhance fea-tureextractionandclassificationaccuracy.\\\",\\n      \\\"Figure2displaystheProposedNeuralNetwork for Classification.\\\",\\n      \\\"Convolutional Layers:\\\",\\n      \\\"The network comprises four convolutional layers, eachcontributing to hierarchical feature learning:\\u2013Conv Layer 1: Applies 32 filters of size 4x4 with a stride of 1, utilizing batchnormalization and ReLU activation.4 Pathikreet and GargiFig.\\\",\\n      \\\"2.Proposed Neural Network for Classification\\u2013Conv Layer 2: Employs 64 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.\\\",\\n      \\\"\\u2013Conv Layer 3: Utilizes 128 filters of size 4x4 with a stride of 1, integratedwith batch normalization and ReLU activation.\\\",\\n      \\\"\\u2013Conv Layer 4: Applies 128 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.\\\",\\n      \\\"Pooling Layers:\\\",\\n      \\\"Pooling operations are strategically placed to reduce spatialdimensions and enhance translational invariance:\\u2013Pooling Layer 1: After the first and second convolutional layers, a max pool-ing operation with a kernel size of 3x3 and a stride of 3 is applied.\\\",\\n      \\\"\\u2013Pooling Layer 2: Following the third convolutional layer, a max pooling op-eration with a kernel size of 3x3 and a stride of 2 is employed.\\\",\\n      \\\"Fully Connected Layers: After feature extraction through convolution and pool-ing, the network incorporates fully connected layers for final classification:\\u2013Fully Connected (FC) Layer 1: Composed of 512 units with ReLU activation,facilitating complex feature integration.\\u2013Dropout Regularization: Implemented with a dropout rate of 0.5 before thefinal classification layer to prevent overfitting by randomly deactivating neu-rons during training.\\\",\\n      \\\"\\u2013Final Classification FC Layer 2: The last layer outputs predictions corre-spondingtothenumberofclasses,leveragingintegratedfeaturesforaccurateclassification.\\\",\\n      \\\"Enhanced Performance: The strategic use of two different strides in the maxpooling layers significantly enhances test accuracy, allowing the network to ef-fectively capture and integrate hierarchical features from varying spatial scales.\\\",\\n      \\\"This design choice optimizes the network\\u2019s ability to discern between benign andmalignant brain tumor images with increased precision and reliability in medicalimaging applications.\\\",\\n      \\\"Title Suppressed Due to Excessive Length 52.3 Applying LIME on Proposed ModelsApplication on Classification Models: For the classification task, LIME isapplied to understand the reasons behind the classification of brain MRI imagesas benign or malignant or no tumor.\\\",\\n      \\\"The steps include:1.\\\",\\n      \\\"Selecting Instances: A subset of correctly classified and misclassified imagesfrom the test set is selected for explanation.\\\",\\n      \\\"2.\\\",\\n      \\\"Generating Explanations: LIME generates explanations for each selected in-stance by highlighting the regions of the image that most influenced themodel\\u2019s decision.\\\",\\n      \\\"3.\\\",\\n      \\\"Visualizing Explanations: The explanations are visualized as heatmaps over-laid on the original images, showing which parts of the image contributedmost to the classification.\\\",\\n      \\\"Application on Segmentation Models: For the segmentation task, LIMEhelps to explain why certain regions of the MRI images were segmented as tumorareas.\\\",\\n      \\\"The steps include:1.\\\",\\n      \\\"Selecting Instances: A set of segmented images, including both successfuland failed segmentations, is chosen for explanation.\\\",\\n      \\\"2. Generating Perturbations: Perturbations are applied to the images, and themodel\\u2019s segmentation predictions for these perturbed samples are obtained.\\\",\\n      \\\"3.\\\",\\n      \\\"Fitting an Interpretable Model: A simpler model is fitted to approximate thesegmentation decisions of the original model.\\\",\\n      \\\"4. Visualizing Explanations: The important features (image segments) that in-fluenced the segmentation decision are visualized, helping to understand themodel\\u2019s behavior.\\\",\\n      \\\"2.4 Algorithm for the Proposed WorkThe following steps outline the algorithm we employed in our research for pre-processing data, training custom models for classification and segmentation, ap-plying LIME for explainability, and evaluating the models using specific metrics.\\\",\\n      \\\"Step 1: Data Preparation:1.\\\",\\n      \\\"Data Acquisition:\\\",\\n      \\\"We collected a publicly available dataset of brain MRIimages.\\\",\\n      \\\"The dataset includes labels for classification (glioma , pituitary ,meningioma ,no tumor) and ground truth masks for segmentation.\\\",\\n      \\\"2.\\\",\\n      \\\"Data Preprocessing:We normalized the images to a standard scale (e.g., [0,1]).\\\",\\n      \\\"The images were resized to a fixed size (e.g., 256x256 pixels) to ensureuniform input dimensions.\\\",\\n      \\\"Used a custom data loading function to load ,preprocess MRI Images and load them in a Dataframe.\\\",\\n      \\\"We augmented thedataset using techniques such as cropping out the brain sections only toenhance model generalization and accuracy and also adding data augmenta-tions such as horizontal flips and rotations.6\\\",\\n      \\\"Pathikreet and GargiStep 2: Model Training:1.\\\",\\n      \\\"Classification Model: We defined a custom CNN architecture for classifica-tion.\\\",\\n      \\\"The model parameters were initialized, and the model was compiledwith an appropriate optimizer (Adam) and loss function (cross-entropy loss).\\\",\\n      \\\"The dataset was split into training, validation, and test sets.\\\",\\n      \\\"We trained themodel on the training set and validated it on the validation set.\\\",\\n      \\\"The trainingprocesswasmonitored,andearlystoppingwasappliedtopreventoverfitting.\\\",\\n      \\\"2. Segmentation Model: We defined a custom U-Net architecture for segmen-tation.\\\",\\n      \\\"The model parameters were initialized, and the model was com-piled with an appropriate optimizer (Adam) and loss function (binary cross-entropy with Dice coefficient).\\\",\\n      \\\"The model was trained on the training setand validated on the validation set.\\\",\\n      \\\"The training process was monitored,and early stopping was applied to prevent overfitting.\\\",\\n      \\\"Step 3: Model Evaluation1.\\\",\\n      \\\"Classification Evaluation:\\\",\\n      \\\"We evaluated the trained classification model onthe test set using metrics such as accuracy, precision, recall, and F1 score.\\\",\\n      \\\"2. Segmentation Evaluation:\\\",\\n      \\\"We evaluated the trained segmentation model onthe test set using metrics such as Dice coefficient, Intersection over Union(IoU), and pixel-wise accuracy.\\\",\\n      \\\"Step 4: Applying LIME for Explainability1.\\\",\\n      \\\"Instance Selection: We selected a subset of correctly classified and misclas-sified images from the test set for classification explainability.\\\",\\n      \\\"We selected aset of successful and failed segmentations from the test set for segmentationexplainability.\\\",\\n      \\\"2. Generate Explanations: We applied LIME to generate explanations for theselected instances.\\\",\\n      \\\"For classification, we created perturbed samples and fitan interpretable model to approximate the classifier\\u2019s behavior locally.\\\",\\n      \\\"Forsegmentation, we created perturbed samples and fit an interpretable modelto approximate the segmenter\\u2019s behavior locally.\\\",\\n      \\\"3.\\\",\\n      \\\"VisualizationandInterpretation:Wevisualizedtheexplanationsasheatmapsto highlight the regions of the images that contributed most to the model\\u2019spredictions.\\\",\\n      \\\"The visualizations were interpreted to understand the model\\u2019sdecision-making process.\\\",\\n      \\\"Step 5: Quantitative Evaluation of Explanations1.\\\",\\n      \\\"Explanation Stability: We measured the consistency of the explanationswhen the input image was slightly perturbed.\\\",\\n      \\\"2. Explanation Sparsity:\\\",\\n      \\\"We evaluated the proportion of the image highlightedin the explanation to assess conciseness.\\\",\\n      \\\"3. Explanation Fidelity: We assessed how well the interpretable model approx-imated the original model\\u2019s predictions.\\\",\\n      \\\"Title Suppressed Due to Excessive Length 73 Experimental Details3.1 Dataset UsedFor Classification Tasks:\\\",\\n      \\\"For the classification tasks, we employed a com-bined dataset comprising 7023 images of human brain MRI images.\\\",\\n      \\\"These im-ages are categorized into four distinct classes: glioma, meningioma, no tumor,andpituitary.\\\",\\n      \\\"Thisdatasetamalgamatesimagesfrommultiplesources,providinga diverse and comprehensive collection for robust classification model trainingand evaluation.\\\",\\n      \\\"Sources of Data:1.\\\",\\n      \\\"Jun Cheng\\u2019s Brain Tumor Dataset [11, 12]: This dataset contains 3064 T1-weighted contrast-enhanced images from 233 patients, distributed acrossthree tumor types: meningioma (708 slices), glioma (1426 slices), and pitu-itary tumor (930 slices).\\\",\\n      \\\"The dataset is split into four subsets, each archivedin a .zip file containing approximately 766 slices.\\\",\\n      \\\"The data is organized inMATLAB(.mat)format,witheachfilestoringtheimagedataandassociatedannotations.\\\",\\n      \\\"2. Br35H Dataset: The Br35H dataset contains a diverse collection of brainMRI images used for various brain tumor classification tasks.\\\",\\n      \\\"The datasetincludes images categorized into the four classes mentioned above, furtherenhancing the diversity and robustness of our classification model.\\\",\\n      \\\"3. SarTaj Dataset: This dataset includes additional brain MRI images used tocomplement the classification model\\u2019s training dataset, ensuring a robustlearning process.\\\",\\n      \\\"The combined dataset from these sources provided a rich variety of images,enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumors.\\\",\\n      \\\"The images in the combined dataset were preprocessed to ensure uniformity inresolution and intensity normalization, followed by augmentation techniques tofurther increase the dataset size and variability, such as rotation, flipping, andcontrast adjustment.\\\",\\n      \\\"For Segmentation Tasks:Brain MRI Segmentation Dataset: This dataset includes brain MR images alongwith manual FLAIR abnormality segmentation masks.\\\",\\n      \\\"The images were sourcedfrom The Cancer Imaging Archive (TCIA) and correspond to 110 patients in-cludedinTheCancerGenomeAtlas(TCGA)lower-gradegliomacollection,eachhavingatleastonefluid-attenuatedinversionrecovery(FLAIR)sequenceandas-sociated genomic clusterdata.\\\",\\n      \\\"FLAIR sequences, known for their high sensitivityto lesions and abnormalities within the brain tissue.\\\",\\n      \\\"Manual segmentation maskswere created by expert radiologists, delineating the abnormal regions with highprecision.\\\",\\n      \\\"Covers 110 patients, providing a comprehensive and diverse dataset8 Pathikreet and Gargifor training and evaluating segmentation models.\\\",\\n      \\\"Each image has been standardized to a uniform resolution, ensuring consistencyacross the dataset.\\\",\\n      \\\"Images were preprocessed to correct for intensity inhomo-geneities and were normalized to have zero mean and unit variance.\\\",\\n      \\\"Data aug-mentation techniques, including random rotations, scaling, and elastic deforma-tions, were applied to increase the effective size of the training set and to improvethe robustness of the segmentation model.\\\",\\n      \\\"The segmentation masks provide abinary representation of the tumor regions, with 1s indicating the presence of atumor and 0s representing healthy tissue.\\\",\\n      \\\"These masks are crucial for trainingsupervised learning models for segmentation tasks.3.2 Evaluation MetricsSegmentation : For evaluating the performance of our segmentation models,we employed several standard metrics to ensure a comprehensive assessment:1.\\\",\\n      \\\"Dice Coefficient (Dice Similarity Index, DSC):The Dice coefficient measuresthe overlap between the predicted segmentation and the ground truth.\\\",\\n      \\\"Itranges from 0 to 1, with 1 indicating perfect overlap.\\\",\\n      \\\"D(A, B) =2|A\\u2229B||A|+|B|(1)where Ais the set of pixels in the predicted segmentation and Bis the setof pixels in the ground truth segmentation.\\\",\\n      \\\"2. Intersection over Union (IoU, Jaccard Index):IoU measures the ratio of theintersection to the union of the predicted and ground truth segmentations.\\\",\\n      \\\"It ranges from 0 to 1, with 1 indicating perfect segmentation.\\\",\\n      \\\"IoU=Area of overlapArea of union=A\\u2229BA\\u222aB(2)3.\\\",\\n      \\\"Precision (Positive Predictive Value):\\\",\\n      \\\"Precision indicates the proportion oftrue positive pixels among all pixels that were predicted as positive.\\\",\\n      \\\"Precision =TPTP+FP(3)where TP is the number of true positive pixels and FP is the number of falsepositive pixels.\\\",\\n      \\\"4.\\\",\\n      \\\"Recall (Sensitivity, True Positive Rate):\\\",\\n      \\\"Recall measures the proportion oftrue positive pixels among all actual positive pixels.\\\",\\n      \\\"Recall =TPTP+FN(4)where FN is the number of false negative pixels.\\\",\\n      \\\"Title Suppressed Due to Excessive Length 95.\\\",\\n      \\\"F1 Score: The F1 score is the harmonic mean of precision and recall, pro-viding a single metric that balances both.\\\",\\n      \\\"F1 Score =2\\u00d7Precision \\u00d7RecallPrecision +Recall(5)\\\",\\n      \\\"6.\\\",\\n      \\\"Hausdorff Distance:\\\",\\n      \\\"This metric measures the maximum distance betweenthe predicted segmentation boundary and the ground truth boundary, pro-viding insight into the spatial accuracy of the segmentation.\\\",\\n      \\\"dH(X, Y) =maxsupx\\u2208Xd(x, Y),supy\\u2208Yd(X, y)(6)where sup represents the supremum operator and d(a, B) =infb\\u2208Bd(a, b).\\\",\\n      \\\"infis the infimum operator d(a, B)quantifies the distance from a point a\\u2208Xto the subset B\\u2286X.d(a, b)is the Euclidean distance between points aandb.Classification : For the classification tasks, the following metrics were utilizedto evaluate the model performance:1.\\\",\\n      \\\"Accuracy: Accuracy is the ratio of correctly predicted instances to the totalinstances.\\\",\\n      \\\"It provides a straightforward measure of overall performance.\\\",\\n      \\\"2. Confusion Matrix:\\\",\\n      \\\"The confusion matrix provides a detailed breakdown ofthe classification performance, displaying the counts of true positives, truenegatives, false positives, and false negatives for each class.\\\",\\n      \\\"3.\\\",\\n      \\\"Receiver Operating Characteristic (ROC) Curve and Area Under the Curve(AUC): The ROC curve plots the true positive rate against the false pos-itive rate at various threshold settings.\\\",\\n      \\\"The AUC provides a single scalarvalue summarizing the model\\u2019s performance across all thresholds.\\\",\\n      \\\"An AUCof 1 indicates perfect classification, while an AUC of 0.5 suggests no betterperformance than random guessing.\\\",\\n      \\\"Accuracy =TP+TNTP+TN+FP+FN(7)Precision, Recall and F1-scores are also used to evaluate the results.\\\",\\n      \\\"LIME (Local Interpretable Model-Agnostic Explanations) :\\\",\\n      \\\"For the in-terpretability of our models, particularly in understanding their decision-makingprocesses, we employed LIME to generate explanations.\\\",\\n      \\\"The following metricswere used to evaluate the quality of the explanations provided by LIME:\\\",\\n      \\\"1. Explanation Stability: Explanation stability measures the consistency of ex-planations when slight perturbations are made to the input data.\\\",\\n      \\\"High sta-bility indicates that small changes in the input do not significantly alter theexplanation.\\\",\\n      \\\"Stability = 1\\u22121nXi=1n|E(xi)\\u2212E(x\\u2032i)| (8)where E(xi)is the explanation for instance xiandE(x\\u2032i)is the explanationfor the perturbed instance and nis the number of samples.10 Pathikreet and Gargi2. Explanation Sparsity: Explanation sparsity evaluates the proportion of fea-tures used in the explanation compared to the total number of features.\\\",\\n      \\\"Sparse explanations are preferred as they are easier to interpret.\\\",\\n      \\\"Sparsity = 1\\u2212Number of features in explanationTotal number of features(9)3.\\\",\\n      \\\"ExplanationFidelity:Explanationfidelitymeasureshowwelltheexplanationapproximates the original model\\u2019s behavior.\\\",\\n      \\\"High fidelity indicates that thesurrogate model used for generating explanations closely mimics the originalmodel.\\\",\\n      \\\"Fidelity = 1\\u22121nnXi=1(f(xi)\\u2212g(xi))2(10)where f(xi)is the prediction of the original model for instance xiandg(xi)is the prediction of the surrogate model.\\\",\\n      \\\"3.3 Setting Up Parameter ValuesFor both the segmentation and classification tasks, careful selection and tuningof hyperparameters were crucial to optimizing the performance of our neuralnetwork models.\\\",\\n      \\\"The parameter values for various components of our proposedwork are detailed below:Segmentation Model (Enhanced U-Net)1.\\\",\\n      \\\"Learning Rate: 1e-4.\\\",\\n      \\\"The learning rate determines the step size at each itera-tion while moving toward a minimum of the loss function.\\\",\\n      \\\"A smaller learningrate ensures a stable convergence but might require more epochs.\\\",\\n      \\\"2. Batch Size: 16.\\\",\\n      \\\"The batch size indicates the number of training samplesused in one forward/backward pass.\\\",\\n      \\\"A moderate batch size balances memoryefficiency and gradient stability.\\\",\\n      \\\"3.\\\",\\n      \\\"Epochs: 50\\\",\\n      \\\"The number of epochs defines how many times the learning al-gorithm will work through the entire training dataset.\\\",\\n      \\\"More epochs can leadto better convergence but may also increase the risk of overfitting.\\\",\\n      \\\"4.\\\",\\n      \\\"Dropout Rate:\\u2013Initial layers: 0.1\\u2013Middle layers: 0.2\\u2013Final layer: 0.3Dropoutisusedtopreventoverfittingbyrandomlysettingafractionofinputunits to 0 at each update during training time.\\\",\\n      \\\"Different rates are used fordifferent layers to balance regularization and learning.\\\",\\n      \\\"5.\\\",\\n      \\\"OptimizerAdam.\\\",\\n      \\\"TheAdamoptimizerischosenforitsadaptivelearningratecapabilities and efficient handling of sparse gradients, making it suitable fortraining deep neural networks.\\\",\\n      \\\"Title Suppressed Due to Excessive Length 116.\\\",\\n      \\\"Loss Function:\\\",\\n      \\\"Binary cross entropy is used for measuring the performanceof a classification model whose output is a probability value between 0 and1.\\\",\\n      \\\"It is well-suited for segmentation tasks where the output is a binary mask.\\\",\\n      \\\"Binary Cross Entropy =\\u22121NNXiMXjyijlog(pij) (11)where Nis the number of samples and Mis the number of classes.\\\",\\n      \\\"Classification Model (Custom CNN)1.\\\",\\n      \\\"Learning Rate: 1e-3.\\\",\\n      \\\"A higher learning rate compared to the segmentationmodel to ensure faster convergence while maintaining stability.\\\",\\n      \\\"2. Batch Size: 32.\\\",\\n      \\\"A larger batch size to improve gradient estimation accuracyand training speed.\\\",\\n      \\\"3.\\\",\\n      \\\"Epochs: 100.\\\",\\n      \\\"A higher number of epochs to ensure sufficient training forconvergence.\\\",\\n      \\\"4.\\\",\\n      \\\"Dropout Rate: 0.5.\\\",\\n      \\\"A higher dropout rate to strongly regularize the modeland prevent overfitting, given the smaller dataset size.\\\",\\n      \\\"5.\\\",\\n      \\\"Optimizer Adam.\\\",\\n      \\\"Adam optimizer is chosen for its efficient gradient compu-tation and adaptive learning rates, facilitating robust training.\\\",\\n      \\\"6.\\\",\\n      \\\"Loss Function: Cross Entropy Loss.\\\",\\n      \\\"Cross entropy loss is used for multi-classclassification tasks, where it evaluates the performance of a classificationmodel whose output is a probability value between 0 and 1 for each class.\\\",\\n      \\\"Cross Entropy =\\u22121NNXj=1[tjlog(pj)\\\",\\n      \\\"+ (1 \\u2212tj)log(1\\u2212pj)](12)\\\",\\n      \\\"Nis the number of data points, tjis the truth value and pjis the Softmaxprobability for taking the truth value.\\\",\\n      \\\"LIME Parameters1. Number of Samples: 1000.\\\",\\n      \\\"The number of perturbed samples generated toexplain each prediction.\\\",\\n      \\\"More samples can improve explanation fidelity butincrease computational cost.\\\",\\n      \\\"2. KernelWidth:0.25.Thewidthofthekernelusedforweightingtheperturbedsamples.\\\",\\n      \\\"A smaller width focuses the explanation on samples closer to theoriginal instance.\\\",\\n      \\\"3.\\\",\\n      \\\"Feature Selection Method: Forward Selection.\\\",\\n      \\\"The method used for selectingthe most important features in the perturbed samples.\\\",\\n      \\\"Forward selectioniteratively adds features to improve explanation quality.\\\",\\n      \\\"4.\\\",\\n      \\\"Regularization: L1 Regularization.\\\",\\n      \\\"L1 regularization encourages sparsity inthe explanation, making it more interpretable by focusing on the most in-fluential features.12 Pathikreet and Gargi5.\\\",\\n      \\\"Segmenter:\\\",\\n      \\\"Quickshift\\u2013Kernel Size: 4.\\\",\\n      \\\"Controls the spatial scale of the segmentation.\\\",\\n      \\\"A largerkernel size results in larger segments.\\u2013Max Distance: 200.\\\",\\n      \\\"Limits the distance in the color space between twopixels to be merged.\\\",\\n      \\\"\\u2013Ratio: 0.5.\\\",\\n      \\\"Balances the color proximity and spatial proximity.\\\",\\n      \\\"A higherratio gives more importance to color similarity.\\\",\\n      \\\"Quickshift is a mode-seeking segmentation algorithm that clusters pixelsbased on color similarity and spatial proximity.\\\",\\n      \\\"It is used to generate super-pixels, which are smaller segments of the image that preserve local informa-tion.\\\",\\n      \\\"4 Results and DiscussionTable 1 presents the evaluation results of our proposed Custom CNN modelcompared to ResNet32 and VGG16 models.\\\",\\n      \\\"The Custom CNN model achieved the highest accuracy (98.70%) comparedto ResNet32 (96.35%) and VGG16 (96.2%), indicating superior overall perfor-mance in classifying brain tumors.\\\",\\n      \\\"The Custom CNN model also demonstratedthe highest precision (97.63%), compared to ResNet32 (95.24%) and VGG16(95.78%), indicating a lower rate of false positives.\\\",\\n      \\\"Recall:\\\",\\n      \\\"The Custom CNNmodel achieved a recall of 97.64%, slightly higher than ResNet32 (96.12%) andVGG16 (95.12%), indicating a higher rate of true positives.\\\",\\n      \\\"The Custom CNNmodel showed the highest F1-Score (97.47%), compared to ResNet32 (96.15%)and VGG16 (95.76%), balancing precision and recall effectively.\\\",\\n      \\\"The CustomCNN model had the highest LIME Explanation Stability Score (0.923), com-pared to ResNet32 (0.846) and VGG16 (0.687), indicating more stable explana-tions across similar inputs.\\\",\\n      \\\"The Custom CNN model had a LIME ExplanationSparsity Score of 0.208, compared to ResNet32 (0.196) and VGG16 (0.225), withlower sparsity indicating more concise explanations.\\\",\\n      \\\"ResNet32 had the highestLIME Explanation Fidelity Score (0.556), followed by VGG16 (0.418), and theCustom CNN model (0.310).\\\",\\n      \\\"This measures how well the explanation model ap-proximates the original model.\\\",\\n      \\\"These results demonstrate that the Custom CNN model generally outperformsboth ResNet32 and VGG16 in terms of classification accuracy, precision, recall,and F1-Score, while providing highly stable and reasonably concise explanationsas measured by LIME.\\\",\\n      \\\"Table 2 presents the evaluation results of our proposed Custom U-Net modelcompared with a U-Net model that uses ResNet as its encoder.\\\",\\n      \\\"The Custom U-Net model achieved a higher validation accuracy (99.8%) compared to the U-Netwith ResNet as its encoder (99.11%).\\\",\\n      \\\"The Custom U-Net model demonstrated alower validation loss (0.0132) than the U-Net with ResNet (0.0425), indicatingbetter performance in minimizing error.\\\",\\n      \\\"The IoU score, which measures the over-lapbetweenthepredictedandgroundtruthsegments,washigherfortheCustomU-Net model (0.889) compared to the ResNet-based U-Net (0.847), suggest-ing more accurate segmentation.\\\",\\n      \\\"The Custom U-Net model had a higher LIMETitle Suppressed Due to Excessive Length 13Table 1.\\\",\\n      \\\"Evaluation Results of proposed Custom CNN model compared to ResNetand VGG16Evaluation Metrics Custom CNN Model ResNet32 VGG16Accuracy 98.70% 96.35% 96.2%Precision 97.63 95.24 95.78Recall 97.64 96.12 95.12F1-Score 97.47 96.15 95.76LIME Explaination Stability Score 0.923 0.846 0.687LIME Explaination Sparsity Score 0.208 0.196 0.225LIME Explaination Fidelity Score 0.310 0.556 0.418Explanation Stability Score (0.8169) versus the ResNet-based U-Net (0.7873),indicating more stable explanations across similar inputs.\\\",\\n      \\\"Both models showedsimilarLIMEExplanationSparsityScores,withtheCustomU-Netmodelhavinga slightly lower score (0.1190) compared to the ResNet-based U-Net (0.1221).\\\",\\n      \\\"Lower sparsity indicates more concise explanations.\\\",\\n      \\\"The ResNet-based U-Netmodel exhibited a higher LIME Explanation Fidelity Score (0.6036) comparedto the Custom U-Net model (0.5447), which measures how well the explanationmodel approximates the original model.\\\",\\n      \\\"Figure 3 displays the Confusion MatrixTable 2.\\\",\\n      \\\"Evaluation Results for the proposed Custom U-net model compared withResUnetEvaluation Metrics Custom U-net Model U-net with ResNet as EncoderValidation Accuracy 99.8% 99.11%Validation Loss 0.0132 0.0425Intersection over Union Score (IoU) 0.889 0.847LIME Explaination Stability Score 0.8169 0.7873LIME Explaination Sparsity Score 0.1190 0.1221LIME Explaination Fidelity Score 0.5447 0.6036of custom CNN.\\\",\\n      \\\"Figure 4 displays the Plot of ground truth scans , masks andpredictedmasksandlabelsalongwithIoUScore.\\\",\\n      \\\"Figure5displaystheMRIScanon which LIME visualizations are generated.\\\",\\n      \\\"Figure 6 displays the LIME Visual-izations of Custom CNN with feature heatmap , positive and negative influencesand top three feature visualization.\\\",\\n      \\\"Figure 7 displays the plot of ground truthscans , masks and predicted masks and labels along with IoU Score NegativeClassifier acheiving perfect IoU of 1.14 Pathikreet and GargiFig.\\\",\\n      \\\"3.Confusion Matrix of custom CNNFig.\\\",\\n      \\\"4.Plot of ground truth scans , masks and predicted masks and labels along withIoU ScoreFig.\\\",\\n      \\\"5.MRI Scan on which LIME visualizations are generated5 Conclusion and Future Scope\\\",\\n      \\\"In this study, we developed and evaluated advanced neural network architecturesforbraintumorclassificationandsegmentationusingMRIimages.\\\",\\n      \\\"OurenhancedTitle Suppressed Due to Excessive Length 15Fig.\\\",\\n      \\\"6.LIME Visualizations of Custom CNN with feature heatmap , positive andnegative influences and top three feature visualizationFig.\\\",\\n      \\\"7.Plot of ground truth scans , masks and predicted masks and labels along withIoU Score Negative Classifier acheiving perfect IoU of 1U-Net model demonstrated improved segmentation performance, while our cus-tom CNN showed significant accuracy in classification tasks.\\\",\\n      \\\"The integration ofExplainable AI (XAI) techniques, particularly LIME, provided valuable insightsinto the model\\u2019s decision-making processes, enhancing interpretability and trust-worthiness.\\\",\\n      \\\"Combining multiple neural network architectures, such as integrating attentionmechanisms or transformers, could improve both classification and segmenta-tion performance.\\\",\\n      \\\"Leveraging larger and more diverse datasets can improve thegeneralizability of our models across different populations and imaging modal-ities.\\\",\\n      \\\"Developing models capable of real-time processing can facilitate clinicalapplications, enabling quicker diagnosis and treatment planning.\\\",\\n      \\\"Incorporatingmore advanced XAI techniques can provide deeper insights into model behavior,improving interpretability and clinical acceptance.\\\",\\n      \\\"Utilizing pre-trained modelson broader datasets and fine-tuning them on specific medical imaging tasks canenhance model performance and reduce training time.\\\",\\n      \\\"Implementing these mod-els in clinical workflows and electronic health records (EHR) systems can aid inautomated diagnosis and decision support.\\\",\\n      \\\"Bibliography\\\",\\n      \\\"[1] S\\u00e9rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva.\\\",\\n      \\\"Braintumor segmentation using convolutional neural networks in mri images.\\\",\\n      \\\"IEEE transactions on medical imaging , 35(5):1240\\u20131251, 2016.\\\",\\n      \\\"[2] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, andHesham FA Hamed.\\\",\\n      \\\"A review on brain tumor diagnosis from mri images:Practical implications, key achievements, and lessons learned.\\\",\\n      \\\"Magneticresonance imaging , 61:300\\u2013318, 2019.\\\",\\n      \\\"[3] Ehab F Badran, Esraa Galal Mahmoud, and Nadder Hamdy.\\\",\\n      \\\"An algorithmfor detecting brain tumors in mri images.\\\",\\n      \\\"In The 2010 International Con-ference on Computer Engineering &amp; Systems , pages 368\\u2013373.\\\",\\n      \\\"IEEE, 2010.\\\",\\n      \\\"[4] Milica M Bad\\u017ea and Marko \\u010c Barjaktarovi\\u0107.\\\",\\n      \\\"Classification of brain tumorsfrom mri images using a convolutional neural network.\\\",\\n      \\\"Applied Sciences ,10(6):1999, 2020.\\\",\\n      \\\"[5] Prashant, Gargi Srivastava, and Vibhav Prakash Singh.\\\",\\n      \\\"Ensemble of deeplearning approaches for detection of brain.\\\",\\n      \\\"In International Journal of Ad-vanced Networking and Applications - IJANA:\\\",\\n      \\\"1st International Conferenceon Advancements in Smart Computing and Information Security, ASCIS2022, Rajkot, India, November 24-26, 2022 , pages 11\\u201316.\\\",\\n      \\\"Eswar Publica-tions, 2022.\\\",\\n      \\\"[6] Alejandro Barredo Arrieta, Natalia D\\u00edaz-Rodr\\u00edguez, Javier Del Ser, AdrienBennetot, Siham Tabik, Alberto Barbado, Salvador Garc\\u00eda, Sergio Gil-L\\u00f3pez, Daniel Molina, Richard Benjamins, et al.\\\",\\n      \\\"Explainable artificial in-telligence (xai): Concepts, taxonomies, opportunities and challenges towardresponsible ai.\\\",\\n      \\\"Information fusion , 58:82\\u2013115, 2020.\\\",\\n      \\\"[7] Bas HM Van der Velden, Hugo J Kuijf, Kenneth GA Gilhuijs, and Max AViergever.\\\",\\n      \\\"Explainable artificial intelligence (xai) in deep learning-basedmedical image analysis.\\\",\\n      \\\"Medical Image Analysis , 79:102470, 2022.\\\",\\n      \\\"[8] J\\u00fcrgen Dieber and Sabrina Kirrane.\\\",\\n      \\\"Why model why?\\\",\\n      \\\"assessing thestrengths and limitations of lime.\\\",\\n      \\\"arXiv preprint arXiv:2012.00093 , 2020.\\\",\\n      \\\"[9] Sasha Targ, Diogo Almeida, and Kevin Lyman.\\\",\\n      \\\"Resnet in resnet: General-izing residual architectures.\\\",\\n      \\\"arXiv preprint arXiv:1603.08029 , 2016.\\\",\\n      \\\"[10] Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.Going deeper in spiking neural networks: Vgg and residual architectures.\\\",\\n      \\\"Frontiers in neuroscience , 13:95, 2019.\\\",\\n      \\\"[11] Jun Cheng, Wei Huang, Shuangliang Cao, Ru Yang, Wei Yang, ZhaoqiangYun, Zhijian Wang, and Qianjin Feng.\\\",\\n      \\\"Enhanced performance of braintumor classification via tumor region augmentation and partition.\\\",\\n      \\\"PloSone, 10(10):e0140381, 2015.\\\",\\n      \\\"[12] Jun Cheng, Wei Yang, Meiyan Huang, Wei Huang, Jun Jiang, Yujia Zhou,Ru Yang, Jie Zhao, Yanqiu Feng, Qianjin Feng, et al. Retrieval of braintumors by adaptive spatial pooling and fisher vector representation.\\\",\\n      \\\"PloSone, 11(6):e0157112, 2016.\\\"\\n    ],\\n    \\\"keywords\\\": [\\n      \\\"shift\\\",\\n      \\\"accountability\\\",\\n      \\\"Conclusion\\\",\\n      \\\"classification\\\",\\n      \\\"Evaluation\\\",\\n      \\\"balance\\\",\\n      \\\"methodology\\\",\\n      \\\"ment\\\",\\n      \\\"approach\\\",\\n      \\\"accuracy\\\",\\n      \\\"ization\\\",\\n      \\\"nature\\\",\\n      \\\"presentation\\\",\\n      \\\"prediction\\\",\\n      \\\"learning\\\",\\n      \\\"net\\\",\\n      \\\"Precision\\\",\\n      \\\"Conference\\\",\\n      \\\"Score\\\",\\n      \\\"proportion\\\",\\n      \\\"space\\\",\\n      \\\"Institute\\\",\\n      \\\"intro-\\\",\\n      \\\"Connections\\\",\\n      \\\"Segmentation\\\",\\n      \\\"Visualizations\\\",\\n      \\\"advent\\\",\\n      \\\"region\\\",\\n      \\\"Mahmoud\\\",\\n      \\\"point\\\",\\n      \\\"age\\\",\\n      \\\"Journal\\\",\\n      \\\"fairness\\\",\\n      \\\"VGG\\\",\\n      \\\"Yang\\\",\\n      \\\"Performance\\\",\\n      \\\"Almeida\\\",\\n      \\\"absent\\\",\\n      \\\"Normalization\\\",\\n      \\\"evaluation\\\",\\n      \\\"cludedinTheCancerGenomeAtlas(TCGA)lower\\\",\\n      \\\"1\\\",\\n      \\\"Accuracy\\\",\\n      \\\"method\\\",\\n      \\\"predictedmasksandlabelsalongwithIoUScore\\\",\\n      \\\"Computing\\\",\\n      \\\"pixel\\\",\\n      \\\"keyword\\\",\\n      \\\"terpretability\\\",\\n      \\\"Netmodelaims\\\",\\n      \\\"Methodology\\\",\\n      \\\"choice\\\",\\n      \\\"ron\\\",\\n      \\\"Details\\\",\\n      \\\"patient\\\",\\n      \\\"FP\\\",\\n      \\\"robustness\\\",\\n      \\\"function\\\",\\n      \\\"operator\\\",\\n      \\\"architecture\\\",\\n      \\\"processing\\\",\\n      \\\"stability\\\",\\n      \\\"Sensitivity\\\",\\n      \\\"label\\\",\\n      \\\"arXiv\\\",\\n      \\\"guessing\\\",\\n      \\\"interpretability\\\",\\n      \\\"b\\\",\\n      \\\"Gargi\\\",\\n      \\\"variability\\\",\\n      \\\"influence\\\",\\n      \\\"Function\\\",\\n      \\\"annotation\\\",\\n      \\\"result\\\",\\n      \\\"forbraintumorclassificationandsegmentationusingmriimage\\\",\\n      \\\"Images\\\",\\n      \\\"trial\\\",\\n      \\\"uniformity\\\",\\n      \\\"sequence\\\",\\n      \\\"auc\\\",\\n      \\\"explainability\\\",\\n      \\\"process\\\",\\n      \\\"Network\\\",\\n      \\\"normalization\\\",\\n      \\\"Hamdy\\\",\\n      \\\"meningioma\\\",\\n      \\\"tation\\\",\\n      \\\"technique\\\",\\n      \\\"sup\\\",\\n      \\\"interpretation\\\",\\n      \\\"conciseness\\\",\\n      \\\"time\\\",\\n      \\\"dataset\\\",\\n      \\\"Enhanced\\\",\\n      \\\"Jiang\\\",\\n      \\\"iou\\\",\\n      \\\"Srivastava\\\",\\n      \\\"difference\\\",\\n      \\\"byincorporatingtheseenhancement\\\",\\n      \\\"Zhao\\\",\\n      \\\"change\\\",\\n      \\\"Alves\\\",\\n      \\\"application\\\",\\n      \\\"matrix\\\",\\n      \\\"wevisualizedtheexplanationsasheatmap\\\",\\n      \\\"Barjaktarovi\\u0107\\\",\\n      \\\"perturbation\\\",\\n      \\\"brain\\\",\\n      \\\"Explanations\\\",\\n      \\\"+\\\",\\n      \\\"Yun\\\",\\n      \\\"Method\\\",\\n      \\\"collection\\\",\\n      \\\"Preparation\\\",\\n      \\\"Prashant\\\",\\n      \\\"Figure5displaystheMRIScan\\\",\\n      \\\"cost\\\",\\n      \\\"union\\\",\\n      \\\"that\\\",\\n      \\\"risk\\\",\\n      \\\"Mis\\\",\\n      \\\"segmentation\\\",\\n      \\\"consistency\\\",\\n      \\\"subset\\\",\\n      \\\"implication\\\",\\n      \\\"distance\\\",\\n      \\\"training\\\",\\n      \\\"Classification\\\",\\n      \\\"entropy\\\",\\n      \\\"Layer\\\",\\n      \\\"truth\\\",\\n      \\\"activation\\\",\\n      \\\"Scope\\\",\\n      \\\"metric\\\",\\n      \\\"step\\\",\\n      \\\"Ellah\\\",\\n      \\\"use\\\",\\n      \\\"Engineering\\\",\\n      \\\"Dataframe\\\",\\n      \\\"misclassification\\\",\\n      \\\"Scan\\\",\\n      \\\"Gilhuijs\\\",\\n      \\\"development\\\",\\n      \\\"performance\\\",\\n      \\\"sult\\\",\\n      \\\"ing\\\",\\n      \\\"classifier\\\",\\n      \\\"explanationfidelity\\\",\\n      \\\"integration\\\",\\n      \\\"flip\\\",\\n      \\\"11\\u201316\\\",\\n      \\\"-\\\",\\n      \\\"taxonomy\\\",\\n      \\\"Tumors\\\",\\n      \\\"pitu-\\\",\\n      \\\"curve\\\",\\n      \\\"stride\\\",\\n      \\\"table\\\",\\n      \\\"Recall\\\",\\n      \\\"tion\\\",\\n      \\\"we\\\",\\n      \\\"Section\\\",\\n      \\\"7.plot\\\",\\n      \\\"decision\\\",\\n      \\\"overfitting\\\",\\n      \\\"Wang\\\",\\n      \\\"workflow\\\",\\n      \\\"arxiv:2012.00093\\\",\\n      \\\"tuning\\\",\\n      \\\"i)|\\\",\\n      \\\"Feng\\\",\\n      \\\"Khalaf\\\",\\n      \\\"speed\\\",\\n      \\\"index):iou\\\",\\n      \\\"diversity\\\",\\n      \\\"lapbetweenthepredictedandgroundtruthsegment\\\",\\n      \\\"size\\\",\\n      \\\"sample\\\",\\n      \\\"system\\\",\\n      \\\"custom\\\",\\n      \\\"setting\\\",\\n      \\\"prevention\\\",\\n      \\\"stance\\\",\\n      \\\"overlap\\\",\\n      \\\"Work\\\",\\n      \\\"support\\\",\\n      \\\"telligence\\\",\\n      \\\"Badran\\\",\\n      \\\"slice\\\",\\n      \\\"instance\\\",\\n      \\\"assessment\\\",\\n      \\\"potential\\\",\\n      \\\"diagnostic\\\",\\n      \\\"score\\\",\\n      \\\"ai\\\",\\n      \\\"perspective\\\",\\n      \\\"Table\\\",\\n      \\\"encoder\\\",\\n      \\\"Cao\\\",\\n      \\\"segment\\\",\\n      \\\"handling\\\",\\n      \\\"Ad-\\\",\\n      \\\"IoU\\\",\\n      \\\"class\\\",\\n      \\\"conclusion\\\",\\n      \\\"lime\\\",\\n      \\\"Security\\\",\\n      \\\"optimizer\\\",\\n      \\\"ally\\\",\\n      \\\"fusion\\\",\\n      \\\"representation\\\",\\n      \\\"variance\\\",\\n      \\\"spondingtothenumberofclasse\\\",\\n      \\\"|a|+|b|(1\\\",\\n      \\\"neuroscience\\\",\\n      \\\"minimum\\\",\\n      \\\"insight\\\",\\n      \\\"Algorithm\\\",\\n      \\\"opportunity\\\",\\n      \\\"tureextractionandclassificationaccuracy\\\",\\n      \\\"fidelity\\\",\\n      \\\"Fidelity\\\",\\n      \\\"filter\\\",\\n      \\\"ap-\\\",\\n      \\\"need\\\",\\n      \\\"el\\\",\\n      \\\"al\\\",\\n      \\\"IEEE\\\",\\n      \\\"LIME\\\",\\n      \\\"y\\\",\\n      \\\"=\\\",\\n      \\\"CNN\\\",\\n      \\\"type\\\",\\n      \\\"intelligence\\\",\\n      \\\"explanationfidelitymeasureshowwelltheexplanation\\\",\\n      \\\"November\\\",\\n      \\\"Dieber\\\",\\n      \\\"segmenter\\\",\\n      \\\"modification\\\",\\n      \\\"resolution\\\",\\n      \\\"malization\\\",\\n      \\\"detection\\\",\\n      \\\"each\\\",\\n      \\\"Kirrane\\\",\\n      \\\"input\\\",\\n      \\\"detail\\\",\\n      \\\"uncertainty\\\",\\n      \\\"model\\\",\\n      \\\"Sparsity\\\",\\n      \\\"enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumor\\\",\\n      \\\"visualizationandinterpretation\\\",\\n      \\\"working\\\",\\n      \\\"rotation\\\",\\n      \\\"value\\\",\\n      \\\"Application\\\",\\n      \\\"measure\\\",\\n      \\\"intersection\\\",\\n      \\\"part\\\",\\n      \\\"breakdown\\\",\\n      \\\"connection\\\",\\n      \\\"LIMETitle\\\",\\n      \\\"clusterdata\\\",\\n      \\\"algorithm\\\",\\n      \\\"rate\\\",\\n      \\\"review\\\",\\n      \\\"adjustment\\\",\\n      \\\"scan\\\",\\n      \\\"Archive\\\",\\n      \\\"tissue\\\",\\n      \\\"hyperparameter\\\",\\n      \\\"similarLIMEExplanationSparsityScores\\\",\\n      \\\"lesson\\\",\\n      \\\"validation\\\",\\n      \\\"stage\\\",\\n      \\\"gradient\\\",\\n      \\\"tumor\\\",\\n      \\\"Explainability\\\",\\n      \\\"Stability\\\",\\n      \\\"flipping\\\",\\n      \\\"structure\\\",\\n      \\\"scale\\\",\\n      \\\"I\\\",\\n      \\\"transformer\\\",\\n      \\\"plot\\\",\\n      \\\"TP+FP(3\\\",\\n      \\\"ference\\\",\\n      \\\"hancement\\\",\\n      \\\"sparsity\\\",\\n      \\\"ture\\\",\\n      \\\"parameter\\\",\\n      \\\"transaction\\\",\\n      \\\"data\\\",\\n      \\\"update\\\",\\n      \\\"FN\\\",\\n      \\\"Sciences\\\",\\n      \\\"pool-\\\",\\n      \\\"relu\\\",\\n      \\\"Ye\\\",\\n      \\\"d(a\\\",\\n      \\\"bility\\\",\\n      \\\"B\\\",\\n      \\\"challenge\\\",\\n      \\\"AUC\\\",\\n      \\\"presence\\\",\\n      \\\"Fig\\\",\\n      \\\"work\\\",\\n      \\\"Chowdhury\\\",\\n      \\\"XAI\\\",\\n      \\\"Size\\\",\\n      \\\"MATLAB(.mat)format\\\",\\n      \\\"width\\\",\\n      \\\"jyijlog(pij\\\",\\n      \\\"OptimizerAdam\\\",\\n      \\\"convergence\\\",\\n      \\\"Singh\\\",\\n      \\\"error\\\",\\n      \\\"initialization\\\",\\n      \\\"invariance\\\",\\n      \\\"similarity\\\",\\n      \\\"Dataset\\\",\\n      \\\"probability\\\",\\n      \\\"extraction\\\",\\n      \\\"set\\\",\\n      \\\"category\\\",\\n      \\\"Loss\\\",\\n      \\\"Cheng\\\",\\n      \\\"control\\\",\\n      \\\"quality\\\",\\n      \\\"acceptance\\\",\\n      \\\"recall\\\",\\n      \\\"task\\\",\\n      \\\"Model\\\",\\n      \\\"lesion\\\",\\n      \\\"Netmodelhaving\\\",\\n      \\\"term\\\",\\n      \\\"preprocess\\\",\\n      \\\"number\\\",\\n      \\\"Huang\\\",\\n      \\\"dimension\\\",\\n      \\\"component\\\",\\n      \\\"Index\\\",\\n      \\\"Section4\\\",\\n      \\\"display\\\",\\n      \\\"Plot\\\",\\n      \\\"importance\\\",\\n      \\\"analysis\\\",\\n      \\\"%\\\",\\n      \\\"mance\\\",\\n      \\\"positive\\\",\\n      \\\"bustness\\\",\\n      \\\"loss\\\",\\n      \\\"Retrieval\\\",\\n      \\\"boundary\\\",\\n      \\\"element\\\",\\n      \\\"layer\\\",\\n      \\\"consequence\\\",\\n      \\\"itie\\\",\\n      \\\"Distance\\\",\\n      \\\"study\\\",\\n      \\\"256x256\\\",\\n      \\\"toimprovegeneralizationandpreventoverfitting\\\",\\n      \\\"enhancement\\\",\\n      \\\"partition\\\",\\n      \\\"efficiency\\\",\\n      \\\"Quickshift\\\",\\n      \\\"coefficient\\\",\\n      \\\"source\\\",\\n      \\\"Abstract\\\",\\n      \\\"Velden\\\",\\n      \\\"ascis\\\",\\n      \\\"this\\\",\\n      \\\"operation\\\",\\n      \\\"behavior\\\",\\n      \\\"population\\\",\\n      \\\"both\\\",\\n      \\\"planning\\\",\\n      \\\"test\\\",\\n      \\\"section\\\",\\n      \\\"figure\\\",\\n      \\\"Hamed\\\",\\n      \\\"selection\\\",\\n      \\\"D(A\\\",\\n      \\\"generalization\\\",\\n      \\\"Union\\\",\\n      \\\"supx\\u2208Xd(x\\\",\\n      \\\"xai\\\",\\n      \\\"network\\\",\\n      \\\"Networking\\\",\\n      \\\"cluster\\\",\\n      \\\"Zhou\\\",\\n      \\\"precision\\\",\\n      \\\"Curve\\\",\\n      \\\"paper\\\",\\n      \\\"limit\\\",\\n      \\\"Dropout\\\",\\n      \\\"radiologist\\\",\\n      \\\"pro-\\\",\\n      \\\"transparency\\\",\\n      \\\"AI\\\",\\n      \\\"negative\\\",\\n      \\\"Coefficient\\\",\\n      \\\"count\\\",\\n      \\\"proximity\\\",\\n      \\\"effectiveness\\\",\\n      \\\"Models\\\",\\n      \\\"limitation\\\",\\n      \\\"Classifier\\\",\\n      \\\"ratio\\\",\\n      \\\"ability\\\",\\n      \\\"developer\\\",\\n      \\\"resnet\\\",\\n      \\\"General-\\\",\\n      \\\"feature\\\",\\n      \\\"convolution\\\",\\n      \\\"capability\\\",\\n      \\\"unit\\\",\\n      \\\"Systems\\\",\\n      \\\"other\\\",\\n      \\\"pooling\\\",\\n      \\\"Kuijf\\\",\\n      \\\"glioma\\\",\\n      \\\"geneitie\\\",\\n      \\\"frontier\\\",\\n      \\\"file\\\",\\n      \\\"ResNet32\\\",\\n      \\\"Intersection\\\",\\n      \\\"com-\\\",\\n      \\\"box\\\",\\n      \\\"reliance\\\",\\n      \\\"Awad\\\",\\n      \\\"Selection\\\",\\n      \\\"area\\\",\\n      \\\"imaging\\\",\\n      \\\"finding\\\",\\n      \\\"Matrix\\\",\\n      \\\"xiandg(xi\\\",\\n      \\\"mask\\\",\\n      \\\"variety\\\",\\n      \\\"Area\\\",\\n      \\\"map\\\",\\n      \\\"TP+FN(4\\\",\\n      \\\"incorporation\\\",\\n      \\\"Liu\\\",\\n      \\\"Parameters\\\",\\n      \\\"Results\\\",\\n      \\\"Pathikreet\\\",\\n      \\\"domain\\\",\\n      \\\"eration\\\",\\n      \\\"Adam\\\",\\n      \\\"Acquisition\\\",\\n      \\\"it\\\",\\n      \\\"IJANA\\\",\\n      \\\"Tumor\\\",\\n      \\\"achievement\\\",\\n      \\\"Data\\\",\\n      \\\"Advancements\\\",\\n      \\\"b)is\\\",\\n      \\\"t1-\\\",\\n      \\\"gorithm\\\",\\n      \\\"they\\\",\\n      \\\"threshold\\\",\\n      \\\"visualization\\\",\\n      \\\"ResNet\\\",\\n      \\\"sensitivity\\\",\\n      \\\"datum\\\",\\n      \\\"regularization\\\",\\n      \\\"epoch\\\",\\n      \\\"gradegliomacollection\\\",\\n      \\\"reason\\\",\\n      \\\"worthiness\\\",\\n      \\\"image\\\",\\n      \\\"mechanism\\\",\\n      \\\"Pereira\\\",\\n      \\\"Entropy\\\",\\n      \\\"explanation\\\",\\n      \\\"Dice\\\",\\n      \\\"scaling\\\",\\n      \\\"pass\\\",\\n      \\\"diagnosis\\\",\\n      \\\"abnormality\\\",\\n      \\\"improvement\\\",\\n      \\\"thisdatasetamalgamatesimagesfrommultiplesource\\\",\\n      \\\"which\\\",\\n      \\\"augmentation\\\",\\n      \\\"U\\\",\\n      \\\"Rate\\\",\\n      \\\"planation\\\",\\n      \\\"Tasks\\\",\\n      \\\"generalizability\\\",\\n      \\\"Roy\\\",\\n      \\\"Y),supy\\u2208Yd(X\\\",\\n      \\\"Targ\\\",\\n      \\\"path\\\",\\n      \\\"mean\\\",\\n      \\\"Pinto\\\",\\n      \\\"bibliography\\\",\\n      \\\"output\\\",\\n      \\\"Lyman\\\",\\n      \\\"discussion\\\",\\n      \\\"researcher\\\",\\n      \\\"tp\\\",\\n      \\\"sibilitie\\\",\\n      \\\"reliability\\\",\\n      \\\"features(9\\\",\\n      \\\"problem\\\",\\n      \\\"Net\\\",\\n      \\\"Viergever\\\",\\n      \\\"Value\\\",\\n      \\\"VGG16\\\",\\n      \\\"Sengupta\\\",\\n      \\\"research\\\",\\n      \\\"Length\\\",\\n      \\\"Silva\\\",\\n      \\\"year\\\",\\n      \\\"TCIA\\\",\\n      \\\"strength\\\",\\n      \\\"arxiv:1603.08029\\\",\\n      \\\"nis\\\",\\n      \\\"Analysis\\\",\\n      \\\"Regularization\\\",\\n      \\\"Technology\\\",\\n      \\\"strategy\\\",\\n      \\\"block\\\"\\n    ],\\n    \\\"word_count\\\": 6577,\\n    \\\"sentence_count\\\": 326\\n  },\\n  \\\"analytics\\\": {\\n    \\\"word_count\\\": 6577,\\n    \\\"sentence_count\\\": 326,\\n    \\\"average_sentence_length\\\": 20.17,\\n    \\\"entity_count\\\": 524,\\n    \\\"keyword_count\\\": 523,\\n    \\\"most_common_entities\\\": [\\n      [\\n        \\\"CARDINAL\\\",\\n        217\\n      ],\\n      [\\n        \\\"ORG\\\",\\n        109\\n      ],\\n      [\\n        \\\"PERSON\\\",\\n        88\\n      ],\\n      [\\n        \\\"PERCENT\\\",\\n        24\\n      ],\\n      [\\n        \\\"DATE\\\",\\n        23\\n      ]\\n    ],\\n    \\\"most_common_words\\\": [\\n      [\\n        \\\"model\\\",\\n        84\\n      ],\\n      [\\n        \\\"segmentation\\\",\\n        51\\n      ],\\n      [\\n        \\\"classification\\\",\\n        44\\n      ],\\n      [\\n        \\\"custom\\\",\\n        39\\n      ],\\n      [\\n        \\\"images\\\",\\n        35\\n      ],\\n      [\\n        \\\"net\\\",\\n        31\\n      ],\\n      [\\n        \\\"lime\\\",\\n        31\\n      ],\\n      [\\n        \\\"score\\\",\\n        30\\n      ],\\n      [\\n        \\\"u\\\",\\n        29\\n      ],\\n      [\\n        \\\"brain\\\",\\n        28\\n      ]\\n    ]\\n  }\\n}\", \"xml_output\": \"<?xml version=\\\"1.0\\\" ?>\\n<document>\\n  <structured_data>\\n    <entities>\\n      <item>\\n        <text>Brain Tumors</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net Models</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Pathikreet Chowdhury</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Rajiv Gandhi Institute of Petroleum Technology</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>229304</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>India</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>21cs2026,gsrivastava}@rgipt.ac.in</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>VGG</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>the Explainable AI</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>99.79%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>0.889</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Addition-</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>wereportaLIMEexplanationstabilityscoreof0.8169andasparsity</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.1190</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>98.70%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>97.63%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>97.64%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>F1 - Score</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>97.63%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.923</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.203.These</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1.00</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Brain Tumor</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>AI</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1 IntroductionSegmentation</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>the past few years</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Explainable AI</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>fed</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>AI</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>8</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>9</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>VGG Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>10</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Section 2</text>\\n        <label>LAW</label>\\n      </item>\\n      <item>\\n        <text>Section 3</text>\\n        <label>LAW</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2.1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Enhanced U-Net</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net:Title Suppressed Due to Excessive Length</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Fig</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Neural Network for SegmentationResidual Connections: Residual</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Batch Normalization:</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>Batch</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>SpatialDropoutisincorporatedwithineachconvolutionalblock</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>2.2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Convolutional Neural Network</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Conv Layer 1</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4x4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>Pathikreet and GargiFig</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>Neural Network</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Conv Layer 2</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>64</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4x4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>Conv Layer 3</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>4x4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>Conv Layer 4</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>128</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4x4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>first</text>\\n        <label>ORDINAL</label>\\n      </item>\\n      <item>\\n        <text>second</text>\\n        <label>ORDINAL</label>\\n      </item>\\n      <item>\\n        <text>max</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>3x3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>third</text>\\n        <label>ORDINAL</label>\\n      </item>\\n      <item>\\n        <text>max</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>3x3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fully Connected Layers:</text>\\n        <label>FAC</label>\\n      </item>\\n      <item>\\n        <text>Fully Connected</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>512</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ReLU</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>0.5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>two</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>max</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Proposed ModelsApplication on Classification Models</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Application on Segmentation Models:</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2.4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Data Acquisition</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>256x256</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Adam</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Adam</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Model Evaluation</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>F1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3.1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>7023</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Jun Cheng\\u2019s</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Brain Tumor Dataset</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>11</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>12</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3064</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>233</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>three</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>glioma</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>1426</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>930</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>766</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>four</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>SarTaj Dataset</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>FLAIR</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>The Cancer Imaging Archive</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>TCIA</text>\\n        <label>LAW</label>\\n      </item>\\n      <item>\\n        <text>110</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>FLAIR</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>110</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>dataset8 Pathikreet</text>\\n        <label>FAC</label>\\n      </item>\\n      <item>\\n        <text>Gargi</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>zero</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1s</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0s</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3.2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Evaluation MetricsSegmentation</text>\\n        <label>EVENT</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Dice Coefficient (Dice Similarity Index</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>DSC):The Dice</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2|A\\u2229B|</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Ais</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Bis</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>IoU</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Jaccard Index):IoU</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>0</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Predictive Value</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>TP+FP(3</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>TP</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>FP</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>True Positive Rate</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Recall =TPTP+FN(4</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>FN</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>F1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>a single metric</text>\\n        <label>QUANTITY</label>\\n      </item>\\n      <item>\\n        <text>F1 Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>2\\u00d7Precision</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>d(a</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>d(a</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Euclidean</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Accuracy</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ROC</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>AUC</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>F1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1\\u22121</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>8)</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>xiandE(x\\u2032</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Pathikreet and Gargi</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1\\u2212Number</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fidelity</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1\\u22121</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3.3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Segmentation Model (Enhanced U-Net</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Batch Size</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>16</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>one</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>50</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>TheAdamoptimizerischosenforitsadaptivelearningrate</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>11</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Binary Cross Entropy</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>NNX</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>jyijlog(pij</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>11</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Nis</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Classification Model (Custom</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Batch Size</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>100</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Adam</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Adam</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>between 0 and 1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Cross Entropy</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>NNX</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Nis</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Softmax</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>1000</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>L1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Pathikreet</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Gargi</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Max</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>200</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>two</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Custom CNN</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>98.70%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>96.35%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>96.2%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>97.63%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>95.24%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>95.78%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>97.64%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>96.12%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>95.12%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>F1-Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>97.47%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>96.15%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>95.76%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>Custom</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.923</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.846</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.687</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.208</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.196</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>0.225</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.556</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.418</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.310</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ResNet32</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>F1-Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Table 2</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>Custom U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>99.8%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>the U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>99.11%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>The Custom U-Net</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>0.0132</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>the U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.0425</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.889</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.847</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>The Custom U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIMETitle Suppressed Due</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>13</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Custom CNN</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>Model ResNet32</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>98.70%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>96.35%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>96.2%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>97.63</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>95.78</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>97.64</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>95.12</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>F1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>97.47</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>96.15</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>95.76</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.923</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.687</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.208</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.225</text>\\n        <label>PRODUCT</label>\\n      </item>\\n      <item>\\n        <text>Fidelity Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.310 0.556 0.418</text>\\n        <label>MONEY</label>\\n      </item>\\n      <item>\\n        <text>0.8169</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.7873</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.1190</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.1221</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>LOC</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.6036</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>the Custom U-Net</text>\\n        <label>FAC</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>the Confusion MatrixTable 2</text>\\n        <label>LAW</label>\\n      </item>\\n      <item>\\n        <text>Custom U-net</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>ResUnetEvaluation Metrics Custom</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Model U-net</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>ResNet</text>\\n        <label>NORP</label>\\n      </item>\\n      <item>\\n        <text>99.8%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>99.11%</text>\\n        <label>PERCENT</label>\\n      </item>\\n      <item>\\n        <text>0.0132 0.0425</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Union Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>0.889</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>0.847</text>\\n        <label>MONEY</label>\\n      </item>\\n      <item>\\n        <text>0.8169 0.7873</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fidelity Score</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Plot</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>three</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>7</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>IoU Score Negative</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>IoU</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>1.14</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Pathikreet and GargiFig</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>3.Confusion Matrix</text>\\n        <label>QUANTITY</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Fig</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>4.Plot</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fig</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>LIME</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Conclusion and Future Scope</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>15</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fig</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>three</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Fig</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>1</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>U-Net</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>CNN</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Adriano Pinto</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Victor Alves</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Carlos A Silva</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>35(5):1240\\u20131251, 2016</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>2</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Mahmoud Khaled Abd-Ellah</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Ali Ismail Awad</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Ashraf AM Khalaf</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Hesham FA Hamed</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>61:300\\u2013318, 2019</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>3</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Esraa Galal Mahmoud</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Nadder Hamdy</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>2010</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>Computer Engineering &amp;amp; Systems</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>368\\u2013373</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2010</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>4</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Applied Sciences</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>10(6):1999, 2020</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>5</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Gargi Srivastava</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Vibhav</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>International Journal of Ad-</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Networking and Applications - IJANA</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>Advancements in Smart Computing and Information Security</text>\\n        <label>WORK_OF_ART</label>\\n      </item>\\n      <item>\\n        <text>2022</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>Rajkot</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>India</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>November 24-26, 2022</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>11\\u201316</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2022</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>6</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Barredo Arrieta</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Natalia D\\u00edaz-Rodr\\u00edguez</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>Javier Del Ser</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>AdrienBennetot</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Siham Tabik</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Alberto Barbado</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Salvador Garc\\u00eda</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Sergio</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Daniel Molina</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Richard Benjamins</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Concepts</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>58:82\\u2013115, 2020</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>Velden</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Kenneth GA</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Gilhuijs</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Max A</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Viergever</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Medical Image Analysis</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>79:102470</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2022</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>8</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Sabrina Kirrane</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>2020</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>9</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Diogo Almeida</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Kevin Lyman</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>General-</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>2016</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>10</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Yuting Ye</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Robert Wang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Chiao Liu</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Kaushik</text>\\n        <label>GPE</label>\\n      </item>\\n      <item>\\n        <text>Roy</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Frontiers</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>13:95, 2019</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>11</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Jun Cheng</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Wei Huang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Shuangliang Cao</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Ru Yang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Wei Yang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>ZhaoqiangYun</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Zhijian Wang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Qianjin Feng</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>10(10):e0140381,</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>2015</text>\\n        <label>DATE</label>\\n      </item>\\n      <item>\\n        <text>12</text>\\n        <label>CARDINAL</label>\\n      </item>\\n      <item>\\n        <text>Jun Cheng</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Wei Yang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Meiyan Huang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Wei Huang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Jun Jiang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Yujia Zhou</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Ru Yang</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Jie Zhao</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Yanqiu Feng</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>Qianjin Feng</text>\\n        <label>PERSON</label>\\n      </item>\\n      <item>\\n        <text>al. Retrieval</text>\\n        <label>ORG</label>\\n      </item>\\n      <item>\\n        <text>11(6):e0157112, 2016</text>\\n        <label>DATE</label>\\n      </item>\\n    </entities>\\n    <sentences>\\n      <item>Enhanced Classification and Segmentation ofBrain Tumors in MRI Images Using CustomCNN and U-Net Models with XAIPathikreet Chowdhury and Gargi Srivastava[0000\\u22120001\\u22126770\\u2212561X]Rajiv Gandhi Institute of Petroleum Technology, Jais Uttar Pradesh 229304, India{21cs2026,gsrivastava}@rgipt.ac.inhttp://www.rgipt.ac.inAbstract.</item>\\n      <item>This study aims to classify brain tumors in MRI images intofour categories: glioma, meningioma, absent, and pituitary tumors, aswell as segment low-grade gliomas.</item>\\n      <item>We evaluate our proposed models onfour publicly available datasets to ensure robustness and generalizability.</item>\\n      <item>For classification tasks, we compare the performance of our custom CNNmodel against established models like ResNet and VGG.</item>\\n      <item>For segmenta-tion tasks, we compare our custom U-Net model with the original U-Netand ResNet-based encoders.</item>\\n      <item>To validate the effectiveness of our models,we employ the Explainable AI (XAI) method LIME, providing insightsinto why our custom architectures outperform others.</item>\\n      <item>Our custom U-Netmodel achieves a validation accuracy of 99.79% and an Intersection overUnion (IoU) score of 0.889 for low-grade glioma segmentation.</item>\\n      <item>Addition-ally,wereportaLIMEexplanationstabilityscoreof0.8169andasparsityscore of 0.1190.</item>\\n      <item>The proposed custom CNN model achieves a validationaccuracy of 98.70% , weighted avg precision of 97.63% , recall of 97.64%and weighted F1 - Score of 97.63%.</item>\\n      <item>The model achieves a LIME stabilityscore of 0.923 and a sparsity score of 0.203.These results highlight thepotential of our custom models to enhance accuracy and interpretabilityin brain tumor classification and segmentation tasks, offering significantimprovements over existing methodologies.</item>\\n      <item>The custom U-net model isalso an excellent negative classifier achieving a perfect 1.00 IoU score forclassifying MRI scans which do not have any tumor.</item>\\n      <item>Keywords: Brain Tumor \\u00b7Explainable AI \\u00b7LIME.</item>\\n      <item>1 IntroductionSegmentation and classification of brain tumors in MRI images have been exten-sively researched over the past few years.</item>\\n      <item>Researchers have been leveraging deepneural networks to achieve significant improvements in this area</item>\\n      <item>[1, 2, 3, 4, 5].</item>\\n      <item>However, the advent of Explainable AI (XAI)</item>\\n      <item>[6] has introduced new pos-sibilities, allowing us to scrutinize network structures and understand the un-derlying mechanisms behind their final results.</item>\\n      <item>This perspective is crucial fordiscerning why certain models perform better than others and how existing neu-ral network architectures can be modified for improved efficiency and accuracy.2 Pathikreet and GargiConsequently, this can lead to smaller networks that save training time and pro-vide faster results when deployed.</item>\\n      <item>Previously, deep learning models operated largely as black boxes.</item>\\n      <item>Input imageswere fed into the network, and based on the output, additional images that ledto misclassifications were incorporated into the training set to enhance learning.</item>\\n      <item>Developers focused on layer adjustment, feedback incorporation, loss prevention,and hyperparameter tuning, often relying on trial and error to achieve better re-sults.</item>\\n      <item>With XAI, the opaque nature of neural networks has transformed intoa transparent process, providing clear insights into the inner workings of thenetwork.</item>\\n      <item>This transparency empowers developers to exert more control over thenetwork, significantly reducing development time and reliance on trial and error</item>\\n      <item>[7].</item>\\n      <item>Moreover, traditional deep learning models were constrained to predicting pre-defined classes without the ability to indicate uncertainty.</item>\\n      <item>XAI now enablesus to identify when the network reaches an &quot;I don\\u2019t know&quot; stage, allowing forthe development of custom models tailored to specific needs rather than merelyadapting existing network architectures for different domains through transferlearning.</item>\\n      <item>Explainable AI also enhances the accountability, fairness, and transparency ofdeep learning models.</item>\\n      <item>This is particularly important in medical diagnostics,where the consequences of misclassification and segmentation can be severe.</item>\\n      <item>In this paper, we develop a custom segmentation and classification model, pro-viding interpretation with the LIME model</item>\\n      <item>[8].</item>\\n      <item>We compare our results withbenchmark neural network models such as ResNet [9] and VGG Net [10].</item>\\n      <item>The paper is organized as follows: we detail our methodology in Section 2, fol-lowed by the experimentation details in Section 3 and presentation of our resultsand their discussion in Section4.</item>\\n      <item>The conclusions and implications our findingsare presented in the Section 5.2 Methodology2.1 Proposed Neural Network for Segmentation</item>\\n      <item>The proposed segmentation network is a custom U-Net, specifically designed toenhance the segmentation of brain MRI images.</item>\\n      <item>The architecture retains thecanonical encoder-decoder structure of the original U-Net, with several enhance-ments aimed at capturing more complex features and improving segmentationperformance.</item>\\n      <item>Figure 1 displays the network architecture.</item>\\n      <item>Proposed Enhanced U-Net Our proposed Enhanced U-Net model intro-duces several critical modifications to the original U-Net architecture, aimedat augmenting its performance for brain tumor segmentation tasks.</item>\\n      <item>These en-hancements are strategically designed to improve feature extraction, model ro-bustness, and overall segmentation accuracy.</item>\\n      <item>Here are the key differences andenhancements compared to the original U-Net:Title Suppressed Due to Excessive Length 3Fig.</item>\\n      <item>1.Proposed Neural Network for SegmentationResidual Connections: Residual connections are integrated within each convo-lutional block.</item>\\n      <item>This strategy helps alleviate the vanishing gradient problem andfacilitates the training of deeper networks by allowing gradients to flow moreeffectively through the network.</item>\\n      <item>The residual path is implemented by addingthe input to the output of the convolutional layers within the block.</item>\\n      <item>Batch Normalization:</item>\\n      <item>Batch normalization layers are employed after each con-volutional layer.</item>\\n      <item>This technique stabilizes and accelerates the training process bynormalizing the activations, thus reducing internal covariate shift.</item>\\n      <item>It also allowsfor higher learning rates and reduces sensitivity to initialization.</item>\\n      <item>Spatial Dropout:</item>\\n      <item>SpatialDropoutisincorporatedwithineachconvolutionalblocktoimprovegeneralizationandpreventoverfitting.</item>\\n      <item>Thisformofdropoutrandomlydrops entire feature maps rather than individual elements, which is particularlyeffective for spatial data, ensuring that the model does not become overly relianton specific feature maps.</item>\\n      <item>Byincorporatingtheseenhancements,ourproposedEnhancedU-Netmodelaimsto significantly improve segmentation performance in terms of accuracy, robust-ness, and generalization capabilities.</item>\\n      <item>2.2 Proposed Neural Network for ClassificationThe proposed classification network is a custom Convolutional Neural Network(CNN) specifically designed for classifying brain MRI images into benign andmalignant tumors.</item>\\n      <item>It incorporates advanced convolutional layers with batch nor-malization, ReLU activations, and strategic pooling operations to enhance fea-tureextractionandclassificationaccuracy.</item>\\n      <item>Figure2displaystheProposedNeuralNetwork for Classification.</item>\\n      <item>Convolutional Layers:</item>\\n      <item>The network comprises four convolutional layers, eachcontributing to hierarchical feature learning:\\u2013Conv Layer 1: Applies 32 filters of size 4x4 with a stride of 1, utilizing batchnormalization and ReLU activation.4 Pathikreet and GargiFig.</item>\\n      <item>2.Proposed Neural Network for Classification\\u2013Conv Layer 2: Employs 64 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.</item>\\n      <item>\\u2013Conv Layer 3: Utilizes 128 filters of size 4x4 with a stride of 1, integratedwith batch normalization and ReLU activation.</item>\\n      <item>\\u2013Conv Layer 4: Applies 128 filters of size 4x4 with a stride of 1, followed bybatch normalization and ReLU activation.</item>\\n      <item>Pooling Layers:</item>\\n      <item>Pooling operations are strategically placed to reduce spatialdimensions and enhance translational invariance:\\u2013Pooling Layer 1: After the first and second convolutional layers, a max pool-ing operation with a kernel size of 3x3 and a stride of 3 is applied.</item>\\n      <item>\\u2013Pooling Layer 2: Following the third convolutional layer, a max pooling op-eration with a kernel size of 3x3 and a stride of 2 is employed.</item>\\n      <item>Fully Connected Layers: After feature extraction through convolution and pool-ing, the network incorporates fully connected layers for final classification:\\u2013Fully Connected (FC) Layer 1: Composed of 512 units with ReLU activation,facilitating complex feature integration.\\u2013Dropout Regularization: Implemented with a dropout rate of 0.5 before thefinal classification layer to prevent overfitting by randomly deactivating neu-rons during training.</item>\\n      <item>\\u2013Final Classification FC Layer 2: The last layer outputs predictions corre-spondingtothenumberofclasses,leveragingintegratedfeaturesforaccurateclassification.</item>\\n      <item>Enhanced Performance: The strategic use of two different strides in the maxpooling layers significantly enhances test accuracy, allowing the network to ef-fectively capture and integrate hierarchical features from varying spatial scales.</item>\\n      <item>This design choice optimizes the network\\u2019s ability to discern between benign andmalignant brain tumor images with increased precision and reliability in medicalimaging applications.</item>\\n      <item>Title Suppressed Due to Excessive Length 52.3 Applying LIME on Proposed ModelsApplication on Classification Models: For the classification task, LIME isapplied to understand the reasons behind the classification of brain MRI imagesas benign or malignant or no tumor.</item>\\n      <item>The steps include:1.</item>\\n      <item>Selecting Instances: A subset of correctly classified and misclassified imagesfrom the test set is selected for explanation.</item>\\n      <item>2.</item>\\n      <item>Generating Explanations: LIME generates explanations for each selected in-stance by highlighting the regions of the image that most influenced themodel\\u2019s decision.</item>\\n      <item>3.</item>\\n      <item>Visualizing Explanations: The explanations are visualized as heatmaps over-laid on the original images, showing which parts of the image contributedmost to the classification.</item>\\n      <item>Application on Segmentation Models: For the segmentation task, LIMEhelps to explain why certain regions of the MRI images were segmented as tumorareas.</item>\\n      <item>The steps include:1.</item>\\n      <item>Selecting Instances: A set of segmented images, including both successfuland failed segmentations, is chosen for explanation.</item>\\n      <item>2. Generating Perturbations: Perturbations are applied to the images, and themodel\\u2019s segmentation predictions for these perturbed samples are obtained.</item>\\n      <item>3.</item>\\n      <item>Fitting an Interpretable Model: A simpler model is fitted to approximate thesegmentation decisions of the original model.</item>\\n      <item>4. Visualizing Explanations: The important features (image segments) that in-fluenced the segmentation decision are visualized, helping to understand themodel\\u2019s behavior.</item>\\n      <item>2.4 Algorithm for the Proposed WorkThe following steps outline the algorithm we employed in our research for pre-processing data, training custom models for classification and segmentation, ap-plying LIME for explainability, and evaluating the models using specific metrics.</item>\\n      <item>Step 1: Data Preparation:1.</item>\\n      <item>Data Acquisition:</item>\\n      <item>We collected a publicly available dataset of brain MRIimages.</item>\\n      <item>The dataset includes labels for classification (glioma , pituitary ,meningioma ,no tumor) and ground truth masks for segmentation.</item>\\n      <item>2.</item>\\n      <item>Data Preprocessing:We normalized the images to a standard scale (e.g., [0,1]).</item>\\n      <item>The images were resized to a fixed size (e.g., 256x256 pixels) to ensureuniform input dimensions.</item>\\n      <item>Used a custom data loading function to load ,preprocess MRI Images and load them in a Dataframe.</item>\\n      <item>We augmented thedataset using techniques such as cropping out the brain sections only toenhance model generalization and accuracy and also adding data augmenta-tions such as horizontal flips and rotations.6</item>\\n      <item>Pathikreet and GargiStep 2: Model Training:1.</item>\\n      <item>Classification Model: We defined a custom CNN architecture for classifica-tion.</item>\\n      <item>The model parameters were initialized, and the model was compiledwith an appropriate optimizer (Adam) and loss function (cross-entropy loss).</item>\\n      <item>The dataset was split into training, validation, and test sets.</item>\\n      <item>We trained themodel on the training set and validated it on the validation set.</item>\\n      <item>The trainingprocesswasmonitored,andearlystoppingwasappliedtopreventoverfitting.</item>\\n      <item>2. Segmentation Model: We defined a custom U-Net architecture for segmen-tation.</item>\\n      <item>The model parameters were initialized, and the model was com-piled with an appropriate optimizer (Adam) and loss function (binary cross-entropy with Dice coefficient).</item>\\n      <item>The model was trained on the training setand validated on the validation set.</item>\\n      <item>The training process was monitored,and early stopping was applied to prevent overfitting.</item>\\n      <item>Step 3: Model Evaluation1.</item>\\n      <item>Classification Evaluation:</item>\\n      <item>We evaluated the trained classification model onthe test set using metrics such as accuracy, precision, recall, and F1 score.</item>\\n      <item>2. Segmentation Evaluation:</item>\\n      <item>We evaluated the trained segmentation model onthe test set using metrics such as Dice coefficient, Intersection over Union(IoU), and pixel-wise accuracy.</item>\\n      <item>Step 4: Applying LIME for Explainability1.</item>\\n      <item>Instance Selection: We selected a subset of correctly classified and misclas-sified images from the test set for classification explainability.</item>\\n      <item>We selected aset of successful and failed segmentations from the test set for segmentationexplainability.</item>\\n      <item>2. Generate Explanations: We applied LIME to generate explanations for theselected instances.</item>\\n      <item>For classification, we created perturbed samples and fitan interpretable model to approximate the classifier\\u2019s behavior locally.</item>\\n      <item>Forsegmentation, we created perturbed samples and fit an interpretable modelto approximate the segmenter\\u2019s behavior locally.</item>\\n      <item>3.</item>\\n      <item>VisualizationandInterpretation:Wevisualizedtheexplanationsasheatmapsto highlight the regions of the images that contributed most to the model\\u2019spredictions.</item>\\n      <item>The visualizations were interpreted to understand the model\\u2019sdecision-making process.</item>\\n      <item>Step 5: Quantitative Evaluation of Explanations1.</item>\\n      <item>Explanation Stability: We measured the consistency of the explanationswhen the input image was slightly perturbed.</item>\\n      <item>2. Explanation Sparsity:</item>\\n      <item>We evaluated the proportion of the image highlightedin the explanation to assess conciseness.</item>\\n      <item>3. Explanation Fidelity: We assessed how well the interpretable model approx-imated the original model\\u2019s predictions.</item>\\n      <item>Title Suppressed Due to Excessive Length 73 Experimental Details3.1 Dataset UsedFor Classification Tasks:</item>\\n      <item>For the classification tasks, we employed a com-bined dataset comprising 7023 images of human brain MRI images.</item>\\n      <item>These im-ages are categorized into four distinct classes: glioma, meningioma, no tumor,andpituitary.</item>\\n      <item>Thisdatasetamalgamatesimagesfrommultiplesources,providinga diverse and comprehensive collection for robust classification model trainingand evaluation.</item>\\n      <item>Sources of Data:1.</item>\\n      <item>Jun Cheng\\u2019s Brain Tumor Dataset [11, 12]: This dataset contains 3064 T1-weighted contrast-enhanced images from 233 patients, distributed acrossthree tumor types: meningioma (708 slices), glioma (1426 slices), and pitu-itary tumor (930 slices).</item>\\n      <item>The dataset is split into four subsets, each archivedin a .zip file containing approximately 766 slices.</item>\\n      <item>The data is organized inMATLAB(.mat)format,witheachfilestoringtheimagedataandassociatedannotations.</item>\\n      <item>2. Br35H Dataset: The Br35H dataset contains a diverse collection of brainMRI images used for various brain tumor classification tasks.</item>\\n      <item>The datasetincludes images categorized into the four classes mentioned above, furtherenhancing the diversity and robustness of our classification model.</item>\\n      <item>3. SarTaj Dataset: This dataset includes additional brain MRI images used tocomplement the classification model\\u2019s training dataset, ensuring a robustlearning process.</item>\\n      <item>The combined dataset from these sources provided a rich variety of images,enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumors.</item>\\n      <item>The images in the combined dataset were preprocessed to ensure uniformity inresolution and intensity normalization, followed by augmentation techniques tofurther increase the dataset size and variability, such as rotation, flipping, andcontrast adjustment.</item>\\n      <item>For Segmentation Tasks:Brain MRI Segmentation Dataset: This dataset includes brain MR images alongwith manual FLAIR abnormality segmentation masks.</item>\\n      <item>The images were sourcedfrom The Cancer Imaging Archive (TCIA) and correspond to 110 patients in-cludedinTheCancerGenomeAtlas(TCGA)lower-gradegliomacollection,eachhavingatleastonefluid-attenuatedinversionrecovery(FLAIR)sequenceandas-sociated genomic clusterdata.</item>\\n      <item>FLAIR sequences, known for their high sensitivityto lesions and abnormalities within the brain tissue.</item>\\n      <item>Manual segmentation maskswere created by expert radiologists, delineating the abnormal regions with highprecision.</item>\\n      <item>Covers 110 patients, providing a comprehensive and diverse dataset8 Pathikreet and Gargifor training and evaluating segmentation models.</item>\\n      <item>Each image has been standardized to a uniform resolution, ensuring consistencyacross the dataset.</item>\\n      <item>Images were preprocessed to correct for intensity inhomo-geneities and were normalized to have zero mean and unit variance.</item>\\n      <item>Data aug-mentation techniques, including random rotations, scaling, and elastic deforma-tions, were applied to increase the effective size of the training set and to improvethe robustness of the segmentation model.</item>\\n      <item>The segmentation masks provide abinary representation of the tumor regions, with 1s indicating the presence of atumor and 0s representing healthy tissue.</item>\\n      <item>These masks are crucial for trainingsupervised learning models for segmentation tasks.3.2 Evaluation MetricsSegmentation : For evaluating the performance of our segmentation models,we employed several standard metrics to ensure a comprehensive assessment:1.</item>\\n      <item>Dice Coefficient (Dice Similarity Index, DSC):The Dice coefficient measuresthe overlap between the predicted segmentation and the ground truth.</item>\\n      <item>Itranges from 0 to 1, with 1 indicating perfect overlap.</item>\\n      <item>D(A, B) =2|A\\u2229B||A|+|B|(1)where Ais the set of pixels in the predicted segmentation and Bis the setof pixels in the ground truth segmentation.</item>\\n      <item>2. Intersection over Union (IoU, Jaccard Index):IoU measures the ratio of theintersection to the union of the predicted and ground truth segmentations.</item>\\n      <item>It ranges from 0 to 1, with 1 indicating perfect segmentation.</item>\\n      <item>IoU=Area of overlapArea of union=A\\u2229BA\\u222aB(2)3.</item>\\n      <item>Precision (Positive Predictive Value):</item>\\n      <item>Precision indicates the proportion oftrue positive pixels among all pixels that were predicted as positive.</item>\\n      <item>Precision =TPTP+FP(3)where TP is the number of true positive pixels and FP is the number of falsepositive pixels.</item>\\n      <item>4.</item>\\n      <item>Recall (Sensitivity, True Positive Rate):</item>\\n      <item>Recall measures the proportion oftrue positive pixels among all actual positive pixels.</item>\\n      <item>Recall =TPTP+FN(4)where FN is the number of false negative pixels.</item>\\n      <item>Title Suppressed Due to Excessive Length 95.</item>\\n      <item>F1 Score: The F1 score is the harmonic mean of precision and recall, pro-viding a single metric that balances both.</item>\\n      <item>F1 Score =2\\u00d7Precision \\u00d7RecallPrecision +Recall(5)</item>\\n      <item>6.</item>\\n      <item>Hausdorff Distance:</item>\\n      <item>This metric measures the maximum distance betweenthe predicted segmentation boundary and the ground truth boundary, pro-viding insight into the spatial accuracy of the segmentation.</item>\\n      <item>dH(X, Y) =maxsupx\\u2208Xd(x, Y),supy\\u2208Yd(X, y)(6)where sup represents the supremum operator and d(a, B) =infb\\u2208Bd(a, b).</item>\\n      <item>infis the infimum operator d(a, B)quantifies the distance from a point a\\u2208Xto the subset B\\u2286X.d(a, b)is the Euclidean distance between points aandb.Classification : For the classification tasks, the following metrics were utilizedto evaluate the model performance:1.</item>\\n      <item>Accuracy: Accuracy is the ratio of correctly predicted instances to the totalinstances.</item>\\n      <item>It provides a straightforward measure of overall performance.</item>\\n      <item>2. Confusion Matrix:</item>\\n      <item>The confusion matrix provides a detailed breakdown ofthe classification performance, displaying the counts of true positives, truenegatives, false positives, and false negatives for each class.</item>\\n      <item>3.</item>\\n      <item>Receiver Operating Characteristic (ROC) Curve and Area Under the Curve(AUC): The ROC curve plots the true positive rate against the false pos-itive rate at various threshold settings.</item>\\n      <item>The AUC provides a single scalarvalue summarizing the model\\u2019s performance across all thresholds.</item>\\n      <item>An AUCof 1 indicates perfect classification, while an AUC of 0.5 suggests no betterperformance than random guessing.</item>\\n      <item>Accuracy =TP+TNTP+TN+FP+FN(7)Precision, Recall and F1-scores are also used to evaluate the results.</item>\\n      <item>LIME (Local Interpretable Model-Agnostic Explanations) :</item>\\n      <item>For the in-terpretability of our models, particularly in understanding their decision-makingprocesses, we employed LIME to generate explanations.</item>\\n      <item>The following metricswere used to evaluate the quality of the explanations provided by LIME:</item>\\n      <item>1. Explanation Stability: Explanation stability measures the consistency of ex-planations when slight perturbations are made to the input data.</item>\\n      <item>High sta-bility indicates that small changes in the input do not significantly alter theexplanation.</item>\\n      <item>Stability = 1\\u22121nXi=1n|E(xi)\\u2212E(x\\u2032i)| (8)where E(xi)is the explanation for instance xiandE(x\\u2032i)is the explanationfor the perturbed instance and nis the number of samples.10 Pathikreet and Gargi2. Explanation Sparsity: Explanation sparsity evaluates the proportion of fea-tures used in the explanation compared to the total number of features.</item>\\n      <item>Sparse explanations are preferred as they are easier to interpret.</item>\\n      <item>Sparsity = 1\\u2212Number of features in explanationTotal number of features(9)3.</item>\\n      <item>ExplanationFidelity:Explanationfidelitymeasureshowwelltheexplanationapproximates the original model\\u2019s behavior.</item>\\n      <item>High fidelity indicates that thesurrogate model used for generating explanations closely mimics the originalmodel.</item>\\n      <item>Fidelity = 1\\u22121nnXi=1(f(xi)\\u2212g(xi))2(10)where f(xi)is the prediction of the original model for instance xiandg(xi)is the prediction of the surrogate model.</item>\\n      <item>3.3 Setting Up Parameter ValuesFor both the segmentation and classification tasks, careful selection and tuningof hyperparameters were crucial to optimizing the performance of our neuralnetwork models.</item>\\n      <item>The parameter values for various components of our proposedwork are detailed below:Segmentation Model (Enhanced U-Net)1.</item>\\n      <item>Learning Rate: 1e-4.</item>\\n      <item>The learning rate determines the step size at each itera-tion while moving toward a minimum of the loss function.</item>\\n      <item>A smaller learningrate ensures a stable convergence but might require more epochs.</item>\\n      <item>2. Batch Size: 16.</item>\\n      <item>The batch size indicates the number of training samplesused in one forward/backward pass.</item>\\n      <item>A moderate batch size balances memoryefficiency and gradient stability.</item>\\n      <item>3.</item>\\n      <item>Epochs: 50</item>\\n      <item>The number of epochs defines how many times the learning al-gorithm will work through the entire training dataset.</item>\\n      <item>More epochs can leadto better convergence but may also increase the risk of overfitting.</item>\\n      <item>4.</item>\\n      <item>Dropout Rate:\\u2013Initial layers: 0.1\\u2013Middle layers: 0.2\\u2013Final layer: 0.3Dropoutisusedtopreventoverfittingbyrandomlysettingafractionofinputunits to 0 at each update during training time.</item>\\n      <item>Different rates are used fordifferent layers to balance regularization and learning.</item>\\n      <item>5.</item>\\n      <item>OptimizerAdam.</item>\\n      <item>TheAdamoptimizerischosenforitsadaptivelearningratecapabilities and efficient handling of sparse gradients, making it suitable fortraining deep neural networks.</item>\\n      <item>Title Suppressed Due to Excessive Length 116.</item>\\n      <item>Loss Function:</item>\\n      <item>Binary cross entropy is used for measuring the performanceof a classification model whose output is a probability value between 0 and1.</item>\\n      <item>It is well-suited for segmentation tasks where the output is a binary mask.</item>\\n      <item>Binary Cross Entropy =\\u22121NNXiMXjyijlog(pij) (11)where Nis the number of samples and Mis the number of classes.</item>\\n      <item>Classification Model (Custom CNN)1.</item>\\n      <item>Learning Rate: 1e-3.</item>\\n      <item>A higher learning rate compared to the segmentationmodel to ensure faster convergence while maintaining stability.</item>\\n      <item>2. Batch Size: 32.</item>\\n      <item>A larger batch size to improve gradient estimation accuracyand training speed.</item>\\n      <item>3.</item>\\n      <item>Epochs: 100.</item>\\n      <item>A higher number of epochs to ensure sufficient training forconvergence.</item>\\n      <item>4.</item>\\n      <item>Dropout Rate: 0.5.</item>\\n      <item>A higher dropout rate to strongly regularize the modeland prevent overfitting, given the smaller dataset size.</item>\\n      <item>5.</item>\\n      <item>Optimizer Adam.</item>\\n      <item>Adam optimizer is chosen for its efficient gradient compu-tation and adaptive learning rates, facilitating robust training.</item>\\n      <item>6.</item>\\n      <item>Loss Function: Cross Entropy Loss.</item>\\n      <item>Cross entropy loss is used for multi-classclassification tasks, where it evaluates the performance of a classificationmodel whose output is a probability value between 0 and 1 for each class.</item>\\n      <item>Cross Entropy =\\u22121NNXj=1[tjlog(pj)</item>\\n      <item>+ (1 \\u2212tj)log(1\\u2212pj)](12)</item>\\n      <item>Nis the number of data points, tjis the truth value and pjis the Softmaxprobability for taking the truth value.</item>\\n      <item>LIME Parameters1. Number of Samples: 1000.</item>\\n      <item>The number of perturbed samples generated toexplain each prediction.</item>\\n      <item>More samples can improve explanation fidelity butincrease computational cost.</item>\\n      <item>2. KernelWidth:0.25.Thewidthofthekernelusedforweightingtheperturbedsamples.</item>\\n      <item>A smaller width focuses the explanation on samples closer to theoriginal instance.</item>\\n      <item>3.</item>\\n      <item>Feature Selection Method: Forward Selection.</item>\\n      <item>The method used for selectingthe most important features in the perturbed samples.</item>\\n      <item>Forward selectioniteratively adds features to improve explanation quality.</item>\\n      <item>4.</item>\\n      <item>Regularization: L1 Regularization.</item>\\n      <item>L1 regularization encourages sparsity inthe explanation, making it more interpretable by focusing on the most in-fluential features.12 Pathikreet and Gargi5.</item>\\n      <item>Segmenter:</item>\\n      <item>Quickshift\\u2013Kernel Size: 4.</item>\\n      <item>Controls the spatial scale of the segmentation.</item>\\n      <item>A largerkernel size results in larger segments.\\u2013Max Distance: 200.</item>\\n      <item>Limits the distance in the color space between twopixels to be merged.</item>\\n      <item>\\u2013Ratio: 0.5.</item>\\n      <item>Balances the color proximity and spatial proximity.</item>\\n      <item>A higherratio gives more importance to color similarity.</item>\\n      <item>Quickshift is a mode-seeking segmentation algorithm that clusters pixelsbased on color similarity and spatial proximity.</item>\\n      <item>It is used to generate super-pixels, which are smaller segments of the image that preserve local informa-tion.</item>\\n      <item>4 Results and DiscussionTable 1 presents the evaluation results of our proposed Custom CNN modelcompared to ResNet32 and VGG16 models.</item>\\n      <item>The Custom CNN model achieved the highest accuracy (98.70%) comparedto ResNet32 (96.35%) and VGG16 (96.2%), indicating superior overall perfor-mance in classifying brain tumors.</item>\\n      <item>The Custom CNN model also demonstratedthe highest precision (97.63%), compared to ResNet32 (95.24%) and VGG16(95.78%), indicating a lower rate of false positives.</item>\\n      <item>Recall:</item>\\n      <item>The Custom CNNmodel achieved a recall of 97.64%, slightly higher than ResNet32 (96.12%) andVGG16 (95.12%), indicating a higher rate of true positives.</item>\\n      <item>The Custom CNNmodel showed the highest F1-Score (97.47%), compared to ResNet32 (96.15%)and VGG16 (95.76%), balancing precision and recall effectively.</item>\\n      <item>The CustomCNN model had the highest LIME Explanation Stability Score (0.923), com-pared to ResNet32 (0.846) and VGG16 (0.687), indicating more stable explana-tions across similar inputs.</item>\\n      <item>The Custom CNN model had a LIME ExplanationSparsity Score of 0.208, compared to ResNet32 (0.196) and VGG16 (0.225), withlower sparsity indicating more concise explanations.</item>\\n      <item>ResNet32 had the highestLIME Explanation Fidelity Score (0.556), followed by VGG16 (0.418), and theCustom CNN model (0.310).</item>\\n      <item>This measures how well the explanation model ap-proximates the original model.</item>\\n      <item>These results demonstrate that the Custom CNN model generally outperformsboth ResNet32 and VGG16 in terms of classification accuracy, precision, recall,and F1-Score, while providing highly stable and reasonably concise explanationsas measured by LIME.</item>\\n      <item>Table 2 presents the evaluation results of our proposed Custom U-Net modelcompared with a U-Net model that uses ResNet as its encoder.</item>\\n      <item>The Custom U-Net model achieved a higher validation accuracy (99.8%) compared to the U-Netwith ResNet as its encoder (99.11%).</item>\\n      <item>The Custom U-Net model demonstrated alower validation loss (0.0132) than the U-Net with ResNet (0.0425), indicatingbetter performance in minimizing error.</item>\\n      <item>The IoU score, which measures the over-lapbetweenthepredictedandgroundtruthsegments,washigherfortheCustomU-Net model (0.889) compared to the ResNet-based U-Net (0.847), suggest-ing more accurate segmentation.</item>\\n      <item>The Custom U-Net model had a higher LIMETitle Suppressed Due to Excessive Length 13Table 1.</item>\\n      <item>Evaluation Results of proposed Custom CNN model compared to ResNetand VGG16Evaluation Metrics Custom CNN Model ResNet32 VGG16Accuracy 98.70% 96.35% 96.2%Precision 97.63 95.24 95.78Recall 97.64 96.12 95.12F1-Score 97.47 96.15 95.76LIME Explaination Stability Score 0.923 0.846 0.687LIME Explaination Sparsity Score 0.208 0.196 0.225LIME Explaination Fidelity Score 0.310 0.556 0.418Explanation Stability Score (0.8169) versus the ResNet-based U-Net (0.7873),indicating more stable explanations across similar inputs.</item>\\n      <item>Both models showedsimilarLIMEExplanationSparsityScores,withtheCustomU-Netmodelhavinga slightly lower score (0.1190) compared to the ResNet-based U-Net (0.1221).</item>\\n      <item>Lower sparsity indicates more concise explanations.</item>\\n      <item>The ResNet-based U-Netmodel exhibited a higher LIME Explanation Fidelity Score (0.6036) comparedto the Custom U-Net model (0.5447), which measures how well the explanationmodel approximates the original model.</item>\\n      <item>Figure 3 displays the Confusion MatrixTable 2.</item>\\n      <item>Evaluation Results for the proposed Custom U-net model compared withResUnetEvaluation Metrics Custom U-net Model U-net with ResNet as EncoderValidation Accuracy 99.8% 99.11%Validation Loss 0.0132 0.0425Intersection over Union Score (IoU) 0.889 0.847LIME Explaination Stability Score 0.8169 0.7873LIME Explaination Sparsity Score 0.1190 0.1221LIME Explaination Fidelity Score 0.5447 0.6036of custom CNN.</item>\\n      <item>Figure 4 displays the Plot of ground truth scans , masks andpredictedmasksandlabelsalongwithIoUScore.</item>\\n      <item>Figure5displaystheMRIScanon which LIME visualizations are generated.</item>\\n      <item>Figure 6 displays the LIME Visual-izations of Custom CNN with feature heatmap , positive and negative influencesand top three feature visualization.</item>\\n      <item>Figure 7 displays the plot of ground truthscans , masks and predicted masks and labels along with IoU Score NegativeClassifier acheiving perfect IoU of 1.14 Pathikreet and GargiFig.</item>\\n      <item>3.Confusion Matrix of custom CNNFig.</item>\\n      <item>4.Plot of ground truth scans , masks and predicted masks and labels along withIoU ScoreFig.</item>\\n      <item>5.MRI Scan on which LIME visualizations are generated5 Conclusion and Future Scope</item>\\n      <item>In this study, we developed and evaluated advanced neural network architecturesforbraintumorclassificationandsegmentationusingMRIimages.</item>\\n      <item>OurenhancedTitle Suppressed Due to Excessive Length 15Fig.</item>\\n      <item>6.LIME Visualizations of Custom CNN with feature heatmap , positive andnegative influences and top three feature visualizationFig.</item>\\n      <item>7.Plot of ground truth scans , masks and predicted masks and labels along withIoU Score Negative Classifier acheiving perfect IoU of 1U-Net model demonstrated improved segmentation performance, while our cus-tom CNN showed significant accuracy in classification tasks.</item>\\n      <item>The integration ofExplainable AI (XAI) techniques, particularly LIME, provided valuable insightsinto the model\\u2019s decision-making processes, enhancing interpretability and trust-worthiness.</item>\\n      <item>Combining multiple neural network architectures, such as integrating attentionmechanisms or transformers, could improve both classification and segmenta-tion performance.</item>\\n      <item>Leveraging larger and more diverse datasets can improve thegeneralizability of our models across different populations and imaging modal-ities.</item>\\n      <item>Developing models capable of real-time processing can facilitate clinicalapplications, enabling quicker diagnosis and treatment planning.</item>\\n      <item>Incorporatingmore advanced XAI techniques can provide deeper insights into model behavior,improving interpretability and clinical acceptance.</item>\\n      <item>Utilizing pre-trained modelson broader datasets and fine-tuning them on specific medical imaging tasks canenhance model performance and reduce training time.</item>\\n      <item>Implementing these mod-els in clinical workflows and electronic health records (EHR) systems can aid inautomated diagnosis and decision support.</item>\\n      <item>Bibliography</item>\\n      <item>[1] S\\u00e9rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva.</item>\\n      <item>Braintumor segmentation using convolutional neural networks in mri images.</item>\\n      <item>IEEE transactions on medical imaging , 35(5):1240\\u20131251, 2016.</item>\\n      <item>[2] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, andHesham FA Hamed.</item>\\n      <item>A review on brain tumor diagnosis from mri images:Practical implications, key achievements, and lessons learned.</item>\\n      <item>Magneticresonance imaging , 61:300\\u2013318, 2019.</item>\\n      <item>[3] Ehab F Badran, Esraa Galal Mahmoud, and Nadder Hamdy.</item>\\n      <item>An algorithmfor detecting brain tumors in mri images.</item>\\n      <item>In The 2010 International Con-ference on Computer Engineering &amp;amp; Systems , pages 368\\u2013373.</item>\\n      <item>IEEE, 2010.</item>\\n      <item>[4] Milica M Bad\\u017ea and Marko \\u010c Barjaktarovi\\u0107.</item>\\n      <item>Classification of brain tumorsfrom mri images using a convolutional neural network.</item>\\n      <item>Applied Sciences ,10(6):1999, 2020.</item>\\n      <item>[5] Prashant, Gargi Srivastava, and Vibhav Prakash Singh.</item>\\n      <item>Ensemble of deeplearning approaches for detection of brain.</item>\\n      <item>In International Journal of Ad-vanced Networking and Applications - IJANA:</item>\\n      <item>1st International Conferenceon Advancements in Smart Computing and Information Security, ASCIS2022, Rajkot, India, November 24-26, 2022 , pages 11\\u201316.</item>\\n      <item>Eswar Publica-tions, 2022.</item>\\n      <item>[6] Alejandro Barredo Arrieta, Natalia D\\u00edaz-Rodr\\u00edguez, Javier Del Ser, AdrienBennetot, Siham Tabik, Alberto Barbado, Salvador Garc\\u00eda, Sergio Gil-L\\u00f3pez, Daniel Molina, Richard Benjamins, et al.</item>\\n      <item>Explainable artificial in-telligence (xai): Concepts, taxonomies, opportunities and challenges towardresponsible ai.</item>\\n      <item>Information fusion , 58:82\\u2013115, 2020.</item>\\n      <item>[7] Bas HM Van der Velden, Hugo J Kuijf, Kenneth GA Gilhuijs, and Max AViergever.</item>\\n      <item>Explainable artificial intelligence (xai) in deep learning-basedmedical image analysis.</item>\\n      <item>Medical Image Analysis , 79:102470, 2022.</item>\\n      <item>[8] J\\u00fcrgen Dieber and Sabrina Kirrane.</item>\\n      <item>Why model why?</item>\\n      <item>assessing thestrengths and limitations of lime.</item>\\n      <item>arXiv preprint arXiv:2012.00093 , 2020.</item>\\n      <item>[9] Sasha Targ, Diogo Almeida, and Kevin Lyman.</item>\\n      <item>Resnet in resnet: General-izing residual architectures.</item>\\n      <item>arXiv preprint arXiv:1603.08029 , 2016.</item>\\n      <item>[10] Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.Going deeper in spiking neural networks: Vgg and residual architectures.</item>\\n      <item>Frontiers in neuroscience , 13:95, 2019.</item>\\n      <item>[11] Jun Cheng, Wei Huang, Shuangliang Cao, Ru Yang, Wei Yang, ZhaoqiangYun, Zhijian Wang, and Qianjin Feng.</item>\\n      <item>Enhanced performance of braintumor classification via tumor region augmentation and partition.</item>\\n      <item>PloSone, 10(10):e0140381, 2015.</item>\\n      <item>[12] Jun Cheng, Wei Yang, Meiyan Huang, Wei Huang, Jun Jiang, Yujia Zhou,Ru Yang, Jie Zhao, Yanqiu Feng, Qianjin Feng, et al. Retrieval of braintumors by adaptive spatial pooling and fisher vector representation.</item>\\n      <item>PloSone, 11(6):e0157112, 2016.</item>\\n    </sentences>\\n    <keywords>\\n      <item>shift</item>\\n      <item>accountability</item>\\n      <item>Conclusion</item>\\n      <item>classification</item>\\n      <item>Evaluation</item>\\n      <item>balance</item>\\n      <item>methodology</item>\\n      <item>ment</item>\\n      <item>approach</item>\\n      <item>accuracy</item>\\n      <item>ization</item>\\n      <item>nature</item>\\n      <item>presentation</item>\\n      <item>prediction</item>\\n      <item>learning</item>\\n      <item>net</item>\\n      <item>Precision</item>\\n      <item>Conference</item>\\n      <item>Score</item>\\n      <item>proportion</item>\\n      <item>space</item>\\n      <item>Institute</item>\\n      <item>intro-</item>\\n      <item>Connections</item>\\n      <item>Segmentation</item>\\n      <item>Visualizations</item>\\n      <item>advent</item>\\n      <item>region</item>\\n      <item>Mahmoud</item>\\n      <item>point</item>\\n      <item>age</item>\\n      <item>Journal</item>\\n      <item>fairness</item>\\n      <item>VGG</item>\\n      <item>Yang</item>\\n      <item>Performance</item>\\n      <item>Almeida</item>\\n      <item>absent</item>\\n      <item>Normalization</item>\\n      <item>evaluation</item>\\n      <item>cludedinTheCancerGenomeAtlas(TCGA)lower</item>\\n      <item>1</item>\\n      <item>Accuracy</item>\\n      <item>method</item>\\n      <item>predictedmasksandlabelsalongwithIoUScore</item>\\n      <item>Computing</item>\\n      <item>pixel</item>\\n      <item>keyword</item>\\n      <item>terpretability</item>\\n      <item>Netmodelaims</item>\\n      <item>Methodology</item>\\n      <item>choice</item>\\n      <item>ron</item>\\n      <item>Details</item>\\n      <item>patient</item>\\n      <item>FP</item>\\n      <item>robustness</item>\\n      <item>function</item>\\n      <item>operator</item>\\n      <item>architecture</item>\\n      <item>processing</item>\\n      <item>stability</item>\\n      <item>Sensitivity</item>\\n      <item>label</item>\\n      <item>arXiv</item>\\n      <item>guessing</item>\\n      <item>interpretability</item>\\n      <item>b</item>\\n      <item>Gargi</item>\\n      <item>variability</item>\\n      <item>influence</item>\\n      <item>Function</item>\\n      <item>annotation</item>\\n      <item>result</item>\\n      <item>forbraintumorclassificationandsegmentationusingmriimage</item>\\n      <item>Images</item>\\n      <item>trial</item>\\n      <item>uniformity</item>\\n      <item>sequence</item>\\n      <item>auc</item>\\n      <item>explainability</item>\\n      <item>process</item>\\n      <item>Network</item>\\n      <item>normalization</item>\\n      <item>Hamdy</item>\\n      <item>meningioma</item>\\n      <item>tation</item>\\n      <item>technique</item>\\n      <item>sup</item>\\n      <item>interpretation</item>\\n      <item>conciseness</item>\\n      <item>time</item>\\n      <item>dataset</item>\\n      <item>Enhanced</item>\\n      <item>Jiang</item>\\n      <item>iou</item>\\n      <item>Srivastava</item>\\n      <item>difference</item>\\n      <item>byincorporatingtheseenhancement</item>\\n      <item>Zhao</item>\\n      <item>change</item>\\n      <item>Alves</item>\\n      <item>application</item>\\n      <item>matrix</item>\\n      <item>wevisualizedtheexplanationsasheatmap</item>\\n      <item>Barjaktarovi\\u0107</item>\\n      <item>perturbation</item>\\n      <item>brain</item>\\n      <item>Explanations</item>\\n      <item>+</item>\\n      <item>Yun</item>\\n      <item>Method</item>\\n      <item>collection</item>\\n      <item>Preparation</item>\\n      <item>Prashant</item>\\n      <item>Figure5displaystheMRIScan</item>\\n      <item>cost</item>\\n      <item>union</item>\\n      <item>that</item>\\n      <item>risk</item>\\n      <item>Mis</item>\\n      <item>segmentation</item>\\n      <item>consistency</item>\\n      <item>subset</item>\\n      <item>implication</item>\\n      <item>distance</item>\\n      <item>training</item>\\n      <item>Classification</item>\\n      <item>entropy</item>\\n      <item>Layer</item>\\n      <item>truth</item>\\n      <item>activation</item>\\n      <item>Scope</item>\\n      <item>metric</item>\\n      <item>step</item>\\n      <item>Ellah</item>\\n      <item>use</item>\\n      <item>Engineering</item>\\n      <item>Dataframe</item>\\n      <item>misclassification</item>\\n      <item>Scan</item>\\n      <item>Gilhuijs</item>\\n      <item>development</item>\\n      <item>performance</item>\\n      <item>sult</item>\\n      <item>ing</item>\\n      <item>classifier</item>\\n      <item>explanationfidelity</item>\\n      <item>integration</item>\\n      <item>flip</item>\\n      <item>11\\u201316</item>\\n      <item>-</item>\\n      <item>taxonomy</item>\\n      <item>Tumors</item>\\n      <item>pitu-</item>\\n      <item>curve</item>\\n      <item>stride</item>\\n      <item>table</item>\\n      <item>Recall</item>\\n      <item>tion</item>\\n      <item>we</item>\\n      <item>Section</item>\\n      <item>7.plot</item>\\n      <item>decision</item>\\n      <item>overfitting</item>\\n      <item>Wang</item>\\n      <item>workflow</item>\\n      <item>arxiv:2012.00093</item>\\n      <item>tuning</item>\\n      <item>i)|</item>\\n      <item>Feng</item>\\n      <item>Khalaf</item>\\n      <item>speed</item>\\n      <item>index):iou</item>\\n      <item>diversity</item>\\n      <item>lapbetweenthepredictedandgroundtruthsegment</item>\\n      <item>size</item>\\n      <item>sample</item>\\n      <item>system</item>\\n      <item>custom</item>\\n      <item>setting</item>\\n      <item>prevention</item>\\n      <item>stance</item>\\n      <item>overlap</item>\\n      <item>Work</item>\\n      <item>support</item>\\n      <item>telligence</item>\\n      <item>Badran</item>\\n      <item>slice</item>\\n      <item>instance</item>\\n      <item>assessment</item>\\n      <item>potential</item>\\n      <item>diagnostic</item>\\n      <item>score</item>\\n      <item>ai</item>\\n      <item>perspective</item>\\n      <item>Table</item>\\n      <item>encoder</item>\\n      <item>Cao</item>\\n      <item>segment</item>\\n      <item>handling</item>\\n      <item>Ad-</item>\\n      <item>IoU</item>\\n      <item>class</item>\\n      <item>conclusion</item>\\n      <item>lime</item>\\n      <item>Security</item>\\n      <item>optimizer</item>\\n      <item>ally</item>\\n      <item>fusion</item>\\n      <item>representation</item>\\n      <item>variance</item>\\n      <item>spondingtothenumberofclasse</item>\\n      <item>|a|+|b|(1</item>\\n      <item>neuroscience</item>\\n      <item>minimum</item>\\n      <item>insight</item>\\n      <item>Algorithm</item>\\n      <item>opportunity</item>\\n      <item>tureextractionandclassificationaccuracy</item>\\n      <item>fidelity</item>\\n      <item>Fidelity</item>\\n      <item>filter</item>\\n      <item>ap-</item>\\n      <item>need</item>\\n      <item>el</item>\\n      <item>al</item>\\n      <item>IEEE</item>\\n      <item>LIME</item>\\n      <item>y</item>\\n      <item>=</item>\\n      <item>CNN</item>\\n      <item>type</item>\\n      <item>intelligence</item>\\n      <item>explanationfidelitymeasureshowwelltheexplanation</item>\\n      <item>November</item>\\n      <item>Dieber</item>\\n      <item>segmenter</item>\\n      <item>modification</item>\\n      <item>resolution</item>\\n      <item>malization</item>\\n      <item>detection</item>\\n      <item>each</item>\\n      <item>Kirrane</item>\\n      <item>input</item>\\n      <item>detail</item>\\n      <item>uncertainty</item>\\n      <item>model</item>\\n      <item>Sparsity</item>\\n      <item>enhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumor</item>\\n      <item>visualizationandinterpretation</item>\\n      <item>working</item>\\n      <item>rotation</item>\\n      <item>value</item>\\n      <item>Application</item>\\n      <item>measure</item>\\n      <item>intersection</item>\\n      <item>part</item>\\n      <item>breakdown</item>\\n      <item>connection</item>\\n      <item>LIMETitle</item>\\n      <item>clusterdata</item>\\n      <item>algorithm</item>\\n      <item>rate</item>\\n      <item>review</item>\\n      <item>adjustment</item>\\n      <item>scan</item>\\n      <item>Archive</item>\\n      <item>tissue</item>\\n      <item>hyperparameter</item>\\n      <item>similarLIMEExplanationSparsityScores</item>\\n      <item>lesson</item>\\n      <item>validation</item>\\n      <item>stage</item>\\n      <item>gradient</item>\\n      <item>tumor</item>\\n      <item>Explainability</item>\\n      <item>Stability</item>\\n      <item>flipping</item>\\n      <item>structure</item>\\n      <item>scale</item>\\n      <item>I</item>\\n      <item>transformer</item>\\n      <item>plot</item>\\n      <item>TP+FP(3</item>\\n      <item>ference</item>\\n      <item>hancement</item>\\n      <item>sparsity</item>\\n      <item>ture</item>\\n      <item>parameter</item>\\n      <item>transaction</item>\\n      <item>data</item>\\n      <item>update</item>\\n      <item>FN</item>\\n      <item>Sciences</item>\\n      <item>pool-</item>\\n      <item>relu</item>\\n      <item>Ye</item>\\n      <item>d(a</item>\\n      <item>bility</item>\\n      <item>B</item>\\n      <item>challenge</item>\\n      <item>AUC</item>\\n      <item>presence</item>\\n      <item>Fig</item>\\n      <item>work</item>\\n      <item>Chowdhury</item>\\n      <item>XAI</item>\\n      <item>Size</item>\\n      <item>MATLAB(.mat)format</item>\\n      <item>width</item>\\n      <item>jyijlog(pij</item>\\n      <item>OptimizerAdam</item>\\n      <item>convergence</item>\\n      <item>Singh</item>\\n      <item>error</item>\\n      <item>initialization</item>\\n      <item>invariance</item>\\n      <item>similarity</item>\\n      <item>Dataset</item>\\n      <item>probability</item>\\n      <item>extraction</item>\\n      <item>set</item>\\n      <item>category</item>\\n      <item>Loss</item>\\n      <item>Cheng</item>\\n      <item>control</item>\\n      <item>quality</item>\\n      <item>acceptance</item>\\n      <item>recall</item>\\n      <item>task</item>\\n      <item>Model</item>\\n      <item>lesion</item>\\n      <item>Netmodelhaving</item>\\n      <item>term</item>\\n      <item>preprocess</item>\\n      <item>number</item>\\n      <item>Huang</item>\\n      <item>dimension</item>\\n      <item>component</item>\\n      <item>Index</item>\\n      <item>Section4</item>\\n      <item>display</item>\\n      <item>Plot</item>\\n      <item>importance</item>\\n      <item>analysis</item>\\n      <item>%</item>\\n      <item>mance</item>\\n      <item>positive</item>\\n      <item>bustness</item>\\n      <item>loss</item>\\n      <item>Retrieval</item>\\n      <item>boundary</item>\\n      <item>element</item>\\n      <item>layer</item>\\n      <item>consequence</item>\\n      <item>itie</item>\\n      <item>Distance</item>\\n      <item>study</item>\\n      <item>256x256</item>\\n      <item>toimprovegeneralizationandpreventoverfitting</item>\\n      <item>enhancement</item>\\n      <item>partition</item>\\n      <item>efficiency</item>\\n      <item>Quickshift</item>\\n      <item>coefficient</item>\\n      <item>source</item>\\n      <item>Abstract</item>\\n      <item>Velden</item>\\n      <item>ascis</item>\\n      <item>this</item>\\n      <item>operation</item>\\n      <item>behavior</item>\\n      <item>population</item>\\n      <item>both</item>\\n      <item>planning</item>\\n      <item>test</item>\\n      <item>section</item>\\n      <item>figure</item>\\n      <item>Hamed</item>\\n      <item>selection</item>\\n      <item>D(A</item>\\n      <item>generalization</item>\\n      <item>Union</item>\\n      <item>supx\\u2208Xd(x</item>\\n      <item>xai</item>\\n      <item>network</item>\\n      <item>Networking</item>\\n      <item>cluster</item>\\n      <item>Zhou</item>\\n      <item>precision</item>\\n      <item>Curve</item>\\n      <item>paper</item>\\n      <item>limit</item>\\n      <item>Dropout</item>\\n      <item>radiologist</item>\\n      <item>pro-</item>\\n      <item>transparency</item>\\n      <item>AI</item>\\n      <item>negative</item>\\n      <item>Coefficient</item>\\n      <item>count</item>\\n      <item>proximity</item>\\n      <item>effectiveness</item>\\n      <item>Models</item>\\n      <item>limitation</item>\\n      <item>Classifier</item>\\n      <item>ratio</item>\\n      <item>ability</item>\\n      <item>developer</item>\\n      <item>resnet</item>\\n      <item>General-</item>\\n      <item>feature</item>\\n      <item>convolution</item>\\n      <item>capability</item>\\n      <item>unit</item>\\n      <item>Systems</item>\\n      <item>other</item>\\n      <item>pooling</item>\\n      <item>Kuijf</item>\\n      <item>glioma</item>\\n      <item>geneitie</item>\\n      <item>frontier</item>\\n      <item>file</item>\\n      <item>ResNet32</item>\\n      <item>Intersection</item>\\n      <item>com-</item>\\n      <item>box</item>\\n      <item>reliance</item>\\n      <item>Awad</item>\\n      <item>Selection</item>\\n      <item>area</item>\\n      <item>imaging</item>\\n      <item>finding</item>\\n      <item>Matrix</item>\\n      <item>xiandg(xi</item>\\n      <item>mask</item>\\n      <item>variety</item>\\n      <item>Area</item>\\n      <item>map</item>\\n      <item>TP+FN(4</item>\\n      <item>incorporation</item>\\n      <item>Liu</item>\\n      <item>Parameters</item>\\n      <item>Results</item>\\n      <item>Pathikreet</item>\\n      <item>domain</item>\\n      <item>eration</item>\\n      <item>Adam</item>\\n      <item>Acquisition</item>\\n      <item>it</item>\\n      <item>IJANA</item>\\n      <item>Tumor</item>\\n      <item>achievement</item>\\n      <item>Data</item>\\n      <item>Advancements</item>\\n      <item>b)is</item>\\n      <item>t1-</item>\\n      <item>gorithm</item>\\n      <item>they</item>\\n      <item>threshold</item>\\n      <item>visualization</item>\\n      <item>ResNet</item>\\n      <item>sensitivity</item>\\n      <item>datum</item>\\n      <item>regularization</item>\\n      <item>epoch</item>\\n      <item>gradegliomacollection</item>\\n      <item>reason</item>\\n      <item>worthiness</item>\\n      <item>image</item>\\n      <item>mechanism</item>\\n      <item>Pereira</item>\\n      <item>Entropy</item>\\n      <item>explanation</item>\\n      <item>Dice</item>\\n      <item>scaling</item>\\n      <item>pass</item>\\n      <item>diagnosis</item>\\n      <item>abnormality</item>\\n      <item>improvement</item>\\n      <item>thisdatasetamalgamatesimagesfrommultiplesource</item>\\n      <item>which</item>\\n      <item>augmentation</item>\\n      <item>U</item>\\n      <item>Rate</item>\\n      <item>planation</item>\\n      <item>Tasks</item>\\n      <item>generalizability</item>\\n      <item>Roy</item>\\n      <item>Y),supy\\u2208Yd(X</item>\\n      <item>Targ</item>\\n      <item>path</item>\\n      <item>mean</item>\\n      <item>Pinto</item>\\n      <item>bibliography</item>\\n      <item>output</item>\\n      <item>Lyman</item>\\n      <item>discussion</item>\\n      <item>researcher</item>\\n      <item>tp</item>\\n      <item>sibilitie</item>\\n      <item>reliability</item>\\n      <item>features(9</item>\\n      <item>problem</item>\\n      <item>Net</item>\\n      <item>Viergever</item>\\n      <item>Value</item>\\n      <item>VGG16</item>\\n      <item>Sengupta</item>\\n      <item>research</item>\\n      <item>Length</item>\\n      <item>Silva</item>\\n      <item>year</item>\\n      <item>TCIA</item>\\n      <item>strength</item>\\n      <item>arxiv:1603.08029</item>\\n      <item>nis</item>\\n      <item>Analysis</item>\\n      <item>Regularization</item>\\n      <item>Technology</item>\\n      <item>strategy</item>\\n      <item>block</item>\\n    </keywords>\\n    <word_count>6577</word_count>\\n    <sentence_count>326</sentence_count>\\n  </structured_data>\\n  <analytics>\\n    <word_count>6577</word_count>\\n    <sentence_count>326</sentence_count>\\n    <average_sentence_length>20.17</average_sentence_length>\\n    <entity_count>524</entity_count>\\n    <keyword_count>523</keyword_count>\\n    <most_common_entities>\\n      <item>\\n        <name>CARDINAL</name>\\n        <value>217</value>\\n      </item>\\n      <item>\\n        <name>ORG</name>\\n        <value>109</value>\\n      </item>\\n      <item>\\n        <name>PERSON</name>\\n        <value>88</value>\\n      </item>\\n      <item>\\n        <name>PERCENT</name>\\n        <value>24</value>\\n      </item>\\n      <item>\\n        <name>DATE</name>\\n        <value>23</value>\\n      </item>\\n    </most_common_entities>\\n    <most_common_words>\\n      <item>\\n        <name>model</name>\\n        <value>84</value>\\n      </item>\\n      <item>\\n        <name>segmentation</name>\\n        <value>51</value>\\n      </item>\\n      <item>\\n        <name>classification</name>\\n        <value>44</value>\\n      </item>\\n      <item>\\n        <name>custom</name>\\n        <value>39</value>\\n      </item>\\n      <item>\\n        <name>images</name>\\n        <value>35</value>\\n      </item>\\n      <item>\\n        <name>net</name>\\n        <value>31</value>\\n      </item>\\n      <item>\\n        <name>lime</name>\\n        <value>31</value>\\n      </item>\\n      <item>\\n        <name>score</name>\\n        <value>30</value>\\n      </item>\\n      <item>\\n        <name>u</name>\\n        <value>29</value>\\n      </item>\\n      <item>\\n        <name>brain</name>\\n        <value>28</value>\\n      </item>\\n    </most_common_words>\\n  </analytics>\\n</document>\\n\", \"extracted_text\": \"Enhanced Classification and Segmentation of\\nBrain Tumors in MRI Images Using Custom\\nCNN and U-Net Models with XAI\\nPathikreet Chowdhury and Gargi Srivastava[0000\\u22120001\\u22126770\\u2212561X]\\nRajiv Gandhi Institute of Petroleum Technology, Jais Uttar Pradesh 229304, India\\n{21cs2026,gsrivastava}@rgipt.ac.in\\nhttp://www.rgipt.ac.in\\nAbstract. This study aims to classify brain tumors in MRI images into\\nfour categories: glioma, meningioma, absent, and pituitary tumors, as\\nwell as segment low-grade gliomas. We evaluate our proposed models on\\nfour publicly available datasets to ensure robustness and generalizability.\\nFor classification tasks, we compare the performance of our custom CNN\\nmodel against established models like ResNet and VGG. For segmenta-\\ntion tasks, we compare our custom U-Net model with the original U-Net\\nand ResNet-based encoders. To validate the effectiveness of our models,\\nwe employ the Explainable AI (XAI) method LIME, providing insights\\ninto why our custom architectures outperform others. Our custom U-Net\\nmodel achieves a validation accuracy of 99.79% and an Intersection over\\nUnion (IoU) score of 0.889 for low-grade glioma segmentation. Addition-\\nally,wereportaLIMEexplanationstabilityscoreof0.8169andasparsity\\nscore of 0.1190. The proposed custom CNN model achieves a validation\\naccuracy of 98.70% , weighted avg precision of 97.63% , recall of 97.64%\\nand weighted F1 - Score of 97.63%. The model achieves a LIME stability\\nscore of 0.923 and a sparsity score of 0.203.These results highlight the\\npotential of our custom models to enhance accuracy and interpretability\\nin brain tumor classification and segmentation tasks, offering significant\\nimprovements over existing methodologies. The custom U-net model is\\nalso an excellent negative classifier achieving a perfect 1.00 IoU score for\\nclassifying MRI scans which do not have any tumor.\\nKeywords: Brain Tumor \\u00b7Explainable AI \\u00b7LIME.\\n1 Introduction\\nSegmentation and classification of brain tumors in MRI images have been exten-\\nsively researched over the past few years. Researchers have been leveraging deep\\nneural networks to achieve significant improvements in this area [1, 2, 3, 4, 5].\\nHowever, the advent of Explainable AI (XAI) [6] has introduced new pos-\\nsibilities, allowing us to scrutinize network structures and understand the un-\\nderlying mechanisms behind their final results. This perspective is crucial for\\ndiscerning why certain models perform better than others and how existing neu-\\nral network architectures can be modified for improved efficiency and accuracy.2 Pathikreet and Gargi\\nConsequently, this can lead to smaller networks that save training time and pro-\\nvide faster results when deployed.\\nPreviously, deep learning models operated largely as black boxes. Input images\\nwere fed into the network, and based on the output, additional images that led\\nto misclassifications were incorporated into the training set to enhance learning.\\nDevelopers focused on layer adjustment, feedback incorporation, loss prevention,\\nand hyperparameter tuning, often relying on trial and error to achieve better re-\\nsults. With XAI, the opaque nature of neural networks has transformed into\\na transparent process, providing clear insights into the inner workings of the\\nnetwork. This transparency empowers developers to exert more control over the\\nnetwork, significantly reducing development time and reliance on trial and error\\n[7].\\nMoreover, traditional deep learning models were constrained to predicting pre-\\ndefined classes without the ability to indicate uncertainty. XAI now enables\\nus to identify when the network reaches an \\\"I don\\u2019t know\\\" stage, allowing for\\nthe development of custom models tailored to specific needs rather than merely\\nadapting existing network architectures for different domains through transfer\\nlearning.\\nExplainable AI also enhances the accountability, fairness, and transparency of\\ndeep learning models. This is particularly important in medical diagnostics,\\nwhere the consequences of misclassification and segmentation can be severe.\\nIn this paper, we develop a custom segmentation and classification model, pro-\\nviding interpretation with the LIME model [8]. We compare our results with\\nbenchmark neural network models such as ResNet [9] and VGG Net [10].\\nThe paper is organized as follows: we detail our methodology in Section 2, fol-\\nlowed by the experimentation details in Section 3 and presentation of our results\\nand their discussion in Section4. The conclusions and implications our findings\\nare presented in the Section 5.\\n2 Methodology\\n2.1 Proposed Neural Network for Segmentation\\nThe proposed segmentation network is a custom U-Net, specifically designed to\\nenhance the segmentation of brain MRI images. The architecture retains the\\ncanonical encoder-decoder structure of the original U-Net, with several enhance-\\nments aimed at capturing more complex features and improving segmentation\\nperformance. Figure 1 displays the network architecture.\\nProposed Enhanced U-Net Our proposed Enhanced U-Net model intro-\\nduces several critical modifications to the original U-Net architecture, aimed\\nat augmenting its performance for brain tumor segmentation tasks. These en-\\nhancements are strategically designed to improve feature extraction, model ro-\\nbustness, and overall segmentation accuracy. Here are the key differences and\\nenhancements compared to the original U-Net:Title Suppressed Due to Excessive Length 3\\nFig. 1.Proposed Neural Network for Segmentation\\nResidual Connections: Residual connections are integrated within each convo-\\nlutional block. This strategy helps alleviate the vanishing gradient problem and\\nfacilitates the training of deeper networks by allowing gradients to flow more\\neffectively through the network. The residual path is implemented by adding\\nthe input to the output of the convolutional layers within the block.\\nBatch Normalization: Batch normalization layers are employed after each con-\\nvolutional layer. This technique stabilizes and accelerates the training process by\\nnormalizing the activations, thus reducing internal covariate shift. It also allows\\nfor higher learning rates and reduces sensitivity to initialization.\\nSpatial Dropout: SpatialDropoutisincorporatedwithineachconvolutionalblock\\ntoimprovegeneralizationandpreventoverfitting.Thisformofdropoutrandomly\\ndrops entire feature maps rather than individual elements, which is particularly\\neffective for spatial data, ensuring that the model does not become overly reliant\\non specific feature maps.\\nByincorporatingtheseenhancements,ourproposedEnhancedU-Netmodelaims\\nto significantly improve segmentation performance in terms of accuracy, robust-\\nness, and generalization capabilities.\\n2.2 Proposed Neural Network for Classification\\nThe proposed classification network is a custom Convolutional Neural Network\\n(CNN) specifically designed for classifying brain MRI images into benign and\\nmalignant tumors. It incorporates advanced convolutional layers with batch nor-\\nmalization, ReLU activations, and strategic pooling operations to enhance fea-\\ntureextractionandclassificationaccuracy.Figure2displaystheProposedNeural\\nNetwork for Classification.\\nConvolutional Layers: The network comprises four convolutional layers, each\\ncontributing to hierarchical feature learning:\\n\\u2013Conv Layer 1: Applies 32 filters of size 4x4 with a stride of 1, utilizing batch\\nnormalization and ReLU activation.4 Pathikreet and Gargi\\nFig. 2.Proposed Neural Network for Classification\\n\\u2013Conv Layer 2: Employs 64 filters of size 4x4 with a stride of 1, followed by\\nbatch normalization and ReLU activation.\\n\\u2013Conv Layer 3: Utilizes 128 filters of size 4x4 with a stride of 1, integrated\\nwith batch normalization and ReLU activation.\\n\\u2013Conv Layer 4: Applies 128 filters of size 4x4 with a stride of 1, followed by\\nbatch normalization and ReLU activation.\\nPooling Layers: Pooling operations are strategically placed to reduce spatial\\ndimensions and enhance translational invariance:\\n\\u2013Pooling Layer 1: After the first and second convolutional layers, a max pool-\\ning operation with a kernel size of 3x3 and a stride of 3 is applied.\\n\\u2013Pooling Layer 2: Following the third convolutional layer, a max pooling op-\\neration with a kernel size of 3x3 and a stride of 2 is employed.\\nFully Connected Layers: After feature extraction through convolution and pool-\\ning, the network incorporates fully connected layers for final classification:\\n\\u2013Fully Connected (FC) Layer 1: Composed of 512 units with ReLU activation,\\nfacilitating complex feature integration.\\n\\u2013Dropout Regularization: Implemented with a dropout rate of 0.5 before the\\nfinal classification layer to prevent overfitting by randomly deactivating neu-\\nrons during training.\\n\\u2013Final Classification FC Layer 2: The last layer outputs predictions corre-\\nspondingtothenumberofclasses,leveragingintegratedfeaturesforaccurate\\nclassification.\\nEnhanced Performance: The strategic use of two different strides in the max\\npooling layers significantly enhances test accuracy, allowing the network to ef-\\nfectively capture and integrate hierarchical features from varying spatial scales.\\nThis design choice optimizes the network\\u2019s ability to discern between benign and\\nmalignant brain tumor images with increased precision and reliability in medical\\nimaging applications.Title Suppressed Due to Excessive Length 5\\n2.3 Applying LIME on Proposed Models\\nApplication on Classification Models: For the classification task, LIME is\\napplied to understand the reasons behind the classification of brain MRI images\\nas benign or malignant or no tumor. The steps include:\\n1. Selecting Instances: A subset of correctly classified and misclassified images\\nfrom the test set is selected for explanation.\\n2. Generating Explanations: LIME generates explanations for each selected in-\\nstance by highlighting the regions of the image that most influenced the\\nmodel\\u2019s decision.\\n3. Visualizing Explanations: The explanations are visualized as heatmaps over-\\nlaid on the original images, showing which parts of the image contributed\\nmost to the classification.\\nApplication on Segmentation Models: For the segmentation task, LIME\\nhelps to explain why certain regions of the MRI images were segmented as tumor\\nareas. The steps include:\\n1. Selecting Instances: A set of segmented images, including both successful\\nand failed segmentations, is chosen for explanation.\\n2. Generating Perturbations: Perturbations are applied to the images, and the\\nmodel\\u2019s segmentation predictions for these perturbed samples are obtained.\\n3. Fitting an Interpretable Model: A simpler model is fitted to approximate the\\nsegmentation decisions of the original model.\\n4. Visualizing Explanations: The important features (image segments) that in-\\nfluenced the segmentation decision are visualized, helping to understand the\\nmodel\\u2019s behavior.\\n2.4 Algorithm for the Proposed Work\\nThe following steps outline the algorithm we employed in our research for pre-\\nprocessing data, training custom models for classification and segmentation, ap-\\nplying LIME for explainability, and evaluating the models using specific metrics.\\nStep 1: Data Preparation:\\n1. Data Acquisition: We collected a publicly available dataset of brain MRI\\nimages. The dataset includes labels for classification (glioma , pituitary ,\\nmeningioma ,no tumor) and ground truth masks for segmentation.\\n2. Data Preprocessing:We normalized the images to a standard scale (e.g., [0,\\n1]). The images were resized to a fixed size (e.g., 256x256 pixels) to ensure\\nuniform input dimensions. Used a custom data loading function to load ,\\npreprocess MRI Images and load them in a Dataframe. We augmented the\\ndataset using techniques such as cropping out the brain sections only to\\nenhance model generalization and accuracy and also adding data augmenta-\\ntions such as horizontal flips and rotations.6 Pathikreet and Gargi\\nStep 2: Model Training:\\n1. Classification Model: We defined a custom CNN architecture for classifica-\\ntion. The model parameters were initialized, and the model was compiled\\nwith an appropriate optimizer (Adam) and loss function (cross-entropy loss).\\nThe dataset was split into training, validation, and test sets. We trained the\\nmodel on the training set and validated it on the validation set. The training\\nprocesswasmonitored,andearlystoppingwasappliedtopreventoverfitting.\\n2. Segmentation Model: We defined a custom U-Net architecture for segmen-\\ntation. The model parameters were initialized, and the model was com-\\npiled with an appropriate optimizer (Adam) and loss function (binary cross-\\nentropy with Dice coefficient). The model was trained on the training set\\nand validated on the validation set. The training process was monitored,\\nand early stopping was applied to prevent overfitting.\\nStep 3: Model Evaluation\\n1. Classification Evaluation: We evaluated the trained classification model on\\nthe test set using metrics such as accuracy, precision, recall, and F1 score.\\n2. Segmentation Evaluation: We evaluated the trained segmentation model on\\nthe test set using metrics such as Dice coefficient, Intersection over Union\\n(IoU), and pixel-wise accuracy.\\nStep 4: Applying LIME for Explainability\\n1. Instance Selection: We selected a subset of correctly classified and misclas-\\nsified images from the test set for classification explainability. We selected a\\nset of successful and failed segmentations from the test set for segmentation\\nexplainability.\\n2. Generate Explanations: We applied LIME to generate explanations for the\\nselected instances. For classification, we created perturbed samples and fit\\nan interpretable model to approximate the classifier\\u2019s behavior locally. For\\nsegmentation, we created perturbed samples and fit an interpretable model\\nto approximate the segmenter\\u2019s behavior locally.\\n3. VisualizationandInterpretation:Wevisualizedtheexplanationsasheatmaps\\nto highlight the regions of the images that contributed most to the model\\u2019s\\npredictions. The visualizations were interpreted to understand the model\\u2019s\\ndecision-making process.\\nStep 5: Quantitative Evaluation of Explanations\\n1. Explanation Stability: We measured the consistency of the explanations\\nwhen the input image was slightly perturbed.\\n2. Explanation Sparsity: We evaluated the proportion of the image highlighted\\nin the explanation to assess conciseness.\\n3. Explanation Fidelity: We assessed how well the interpretable model approx-\\nimated the original model\\u2019s predictions.Title Suppressed Due to Excessive Length 7\\n3 Experimental Details\\n3.1 Dataset Used\\nFor Classification Tasks: For the classification tasks, we employed a com-\\nbined dataset comprising 7023 images of human brain MRI images. These im-\\nages are categorized into four distinct classes: glioma, meningioma, no tumor,\\nandpituitary.Thisdatasetamalgamatesimagesfrommultiplesources,providing\\na diverse and comprehensive collection for robust classification model training\\nand evaluation.\\nSources of Data:\\n1. Jun Cheng\\u2019s Brain Tumor Dataset [11, 12]: This dataset contains 3064 T1-\\nweighted contrast-enhanced images from 233 patients, distributed across\\nthree tumor types: meningioma (708 slices), glioma (1426 slices), and pitu-\\nitary tumor (930 slices). The dataset is split into four subsets, each archived\\nin a .zip file containing approximately 766 slices. The data is organized in\\nMATLAB(.mat)format,witheachfilestoringtheimagedataandassociated\\nannotations.\\n2. Br35H Dataset: The Br35H dataset contains a diverse collection of brain\\nMRI images used for various brain tumor classification tasks. The dataset\\nincludes images categorized into the four classes mentioned above, further\\nenhancing the diversity and robustness of our classification model.\\n3. SarTaj Dataset: This dataset includes additional brain MRI images used to\\ncomplement the classification model\\u2019s training dataset, ensuring a robust\\nlearning process.\\nThe combined dataset from these sources provided a rich variety of images,\\nenhancingthemodel\\u2019sabilitytogeneralizeacrossdifferenttypesofbraintumors.\\nThe images in the combined dataset were preprocessed to ensure uniformity in\\nresolution and intensity normalization, followed by augmentation techniques to\\nfurther increase the dataset size and variability, such as rotation, flipping, and\\ncontrast adjustment.\\nFor Segmentation Tasks:\\nBrain MRI Segmentation Dataset: This dataset includes brain MR images along\\nwith manual FLAIR abnormality segmentation masks. The images were sourced\\nfrom The Cancer Imaging Archive (TCIA) and correspond to 110 patients in-\\ncludedinTheCancerGenomeAtlas(TCGA)lower-gradegliomacollection,each\\nhavingatleastonefluid-attenuatedinversionrecovery(FLAIR)sequenceandas-\\nsociated genomic clusterdata. FLAIR sequences, known for their high sensitivity\\nto lesions and abnormalities within the brain tissue. Manual segmentation masks\\nwere created by expert radiologists, delineating the abnormal regions with high\\nprecision. Covers 110 patients, providing a comprehensive and diverse dataset8 Pathikreet and Gargi\\nfor training and evaluating segmentation models.\\nEach image has been standardized to a uniform resolution, ensuring consistency\\nacross the dataset. Images were preprocessed to correct for intensity inhomo-\\ngeneities and were normalized to have zero mean and unit variance. Data aug-\\nmentation techniques, including random rotations, scaling, and elastic deforma-\\ntions, were applied to increase the effective size of the training set and to improve\\nthe robustness of the segmentation model. The segmentation masks provide a\\nbinary representation of the tumor regions, with 1s indicating the presence of a\\ntumor and 0s representing healthy tissue. These masks are crucial for training\\nsupervised learning models for segmentation tasks.\\n3.2 Evaluation Metrics\\nSegmentation : For evaluating the performance of our segmentation models,\\nwe employed several standard metrics to ensure a comprehensive assessment:\\n1. Dice Coefficient (Dice Similarity Index, DSC):The Dice coefficient measures\\nthe overlap between the predicted segmentation and the ground truth. It\\nranges from 0 to 1, with 1 indicating perfect overlap.\\nD(A, B) =2|A\\u2229B|\\n|A|+|B|(1)\\nwhere Ais the set of pixels in the predicted segmentation and Bis the set\\nof pixels in the ground truth segmentation.\\n2. Intersection over Union (IoU, Jaccard Index):IoU measures the ratio of the\\nintersection to the union of the predicted and ground truth segmentations.\\nIt ranges from 0 to 1, with 1 indicating perfect segmentation.\\nIoU=Area of overlap\\nArea of union=A\\u2229B\\nA\\u222aB(2)\\n3. Precision (Positive Predictive Value): Precision indicates the proportion of\\ntrue positive pixels among all pixels that were predicted as positive.\\nPrecision =TP\\nTP+FP(3)\\nwhere TP is the number of true positive pixels and FP is the number of false\\npositive pixels.\\n4. Recall (Sensitivity, True Positive Rate): Recall measures the proportion of\\ntrue positive pixels among all actual positive pixels.\\nRecall =TP\\nTP+FN(4)\\nwhere FN is the number of false negative pixels.Title Suppressed Due to Excessive Length 9\\n5. F1 Score: The F1 score is the harmonic mean of precision and recall, pro-\\nviding a single metric that balances both.\\nF1 Score =2\\u00d7Precision \\u00d7Recall\\nPrecision +Recall(5)\\n6. Hausdorff Distance: This metric measures the maximum distance between\\nthe predicted segmentation boundary and the ground truth boundary, pro-\\nviding insight into the spatial accuracy of the segmentation.\\ndH(X, Y) =max\\b\\nsupx\\u2208Xd(x, Y),supy\\u2208Yd(X, y)\\t\\n(6)\\nwhere sup represents the supremum operator and d(a, B) =infb\\u2208Bd(a, b). inf\\nis the infimum operator d(a, B)quantifies the distance from a point a\\u2208X\\nto the subset B\\u2286X.d(a, b)is the Euclidean distance between points aand\\nb.\\nClassification : For the classification tasks, the following metrics were utilized\\nto evaluate the model performance:\\n1. Accuracy: Accuracy is the ratio of correctly predicted instances to the total\\ninstances. It provides a straightforward measure of overall performance.\\n2. Confusion Matrix: The confusion matrix provides a detailed breakdown of\\nthe classification performance, displaying the counts of true positives, true\\nnegatives, false positives, and false negatives for each class.\\n3. Receiver Operating Characteristic (ROC) Curve and Area Under the Curve\\n(AUC): The ROC curve plots the true positive rate against the false pos-\\nitive rate at various threshold settings. The AUC provides a single scalar\\nvalue summarizing the model\\u2019s performance across all thresholds. An AUC\\nof 1 indicates perfect classification, while an AUC of 0.5 suggests no better\\nperformance than random guessing.\\nAccuracy =TP+TN\\nTP+TN+FP+FN(7)\\nPrecision, Recall and F1-scores are also used to evaluate the results.\\nLIME (Local Interpretable Model-Agnostic Explanations) : For the in-\\nterpretability of our models, particularly in understanding their decision-making\\nprocesses, we employed LIME to generate explanations. The following metrics\\nwere used to evaluate the quality of the explanations provided by LIME:\\n1. Explanation Stability: Explanation stability measures the consistency of ex-\\nplanations when slight perturbations are made to the input data. High sta-\\nbility indicates that small changes in the input do not significantly alter the\\nexplanation.\\nStability = 1\\u22121\\nnX\\ni=1n|E(xi)\\u2212E(x\\u2032\\ni)| (8)\\nwhere E(xi)is the explanation for instance xiandE(x\\u2032\\ni)is the explanation\\nfor the perturbed instance and nis the number of samples.10 Pathikreet and Gargi\\n2. Explanation Sparsity: Explanation sparsity evaluates the proportion of fea-\\ntures used in the explanation compared to the total number of features.\\nSparse explanations are preferred as they are easier to interpret.\\nSparsity = 1\\u2212Number of features in explanation\\nTotal number of features(9)\\n3. ExplanationFidelity:Explanationfidelitymeasureshowwelltheexplanation\\napproximates the original model\\u2019s behavior. High fidelity indicates that the\\nsurrogate model used for generating explanations closely mimics the original\\nmodel.\\nFidelity = 1\\u22121\\nnnX\\ni=1(f(xi)\\u2212g(xi))2(10)\\nwhere f(xi)is the prediction of the original model for instance xiandg(xi)\\nis the prediction of the surrogate model.\\n3.3 Setting Up Parameter Values\\nFor both the segmentation and classification tasks, careful selection and tuning\\nof hyperparameters were crucial to optimizing the performance of our neural\\nnetwork models. The parameter values for various components of our proposed\\nwork are detailed below:\\nSegmentation Model (Enhanced U-Net)\\n1. Learning Rate: 1e-4. The learning rate determines the step size at each itera-\\ntion while moving toward a minimum of the loss function. A smaller learning\\nrate ensures a stable convergence but might require more epochs.\\n2. Batch Size: 16. The batch size indicates the number of training samples\\nused in one forward/backward pass. A moderate batch size balances memory\\nefficiency and gradient stability.\\n3. Epochs: 50 The number of epochs defines how many times the learning al-\\ngorithm will work through the entire training dataset. More epochs can lead\\nto better convergence but may also increase the risk of overfitting.\\n4. Dropout Rate:\\n\\u2013Initial layers: 0.1\\n\\u2013Middle layers: 0.2\\n\\u2013Final layer: 0.3\\nDropoutisusedtopreventoverfittingbyrandomlysettingafractionofinput\\nunits to 0 at each update during training time. Different rates are used for\\ndifferent layers to balance regularization and learning.\\n5. OptimizerAdam.TheAdamoptimizerischosenforitsadaptivelearningrate\\ncapabilities and efficient handling of sparse gradients, making it suitable for\\ntraining deep neural networks.Title Suppressed Due to Excessive Length 11\\n6. Loss Function: Binary cross entropy is used for measuring the performance\\nof a classification model whose output is a probability value between 0 and\\n1. It is well-suited for segmentation tasks where the output is a binary mask.\\nBinary Cross Entropy =\\u22121\\nNNX\\niMX\\njyijlog(pij) (11)\\nwhere Nis the number of samples and Mis the number of classes.\\nClassification Model (Custom CNN)\\n1. Learning Rate: 1e-3. A higher learning rate compared to the segmentation\\nmodel to ensure faster convergence while maintaining stability.\\n2. Batch Size: 32. A larger batch size to improve gradient estimation accuracy\\nand training speed.\\n3. Epochs: 100. A higher number of epochs to ensure sufficient training for\\nconvergence.\\n4. Dropout Rate: 0.5. A higher dropout rate to strongly regularize the model\\nand prevent overfitting, given the smaller dataset size.\\n5. Optimizer Adam. Adam optimizer is chosen for its efficient gradient compu-\\ntation and adaptive learning rates, facilitating robust training.\\n6. Loss Function: Cross Entropy Loss. Cross entropy loss is used for multi-class\\nclassification tasks, where it evaluates the performance of a classification\\nmodel whose output is a probability value between 0 and 1 for each class.\\nCross Entropy =\\u22121\\nNNX\\nj=1[tjlog(pj) + (1 \\u2212tj)log(1\\u2212pj)](12)\\nNis the number of data points, tjis the truth value and pjis the Softmax\\nprobability for taking the truth value.\\nLIME Parameters\\n1. Number of Samples: 1000. The number of perturbed samples generated to\\nexplain each prediction. More samples can improve explanation fidelity but\\nincrease computational cost.\\n2. KernelWidth:0.25.Thewidthofthekernelusedforweightingtheperturbed\\nsamples. A smaller width focuses the explanation on samples closer to the\\noriginal instance.\\n3. Feature Selection Method: Forward Selection. The method used for selecting\\nthe most important features in the perturbed samples. Forward selection\\niteratively adds features to improve explanation quality.\\n4. Regularization: L1 Regularization. L1 regularization encourages sparsity in\\nthe explanation, making it more interpretable by focusing on the most in-\\nfluential features.12 Pathikreet and Gargi\\n5. Segmenter: Quickshift\\n\\u2013Kernel Size: 4. Controls the spatial scale of the segmentation. A larger\\nkernel size results in larger segments.\\n\\u2013Max Distance: 200. Limits the distance in the color space between two\\npixels to be merged.\\n\\u2013Ratio: 0.5. Balances the color proximity and spatial proximity. A higher\\nratio gives more importance to color similarity.\\nQuickshift is a mode-seeking segmentation algorithm that clusters pixels\\nbased on color similarity and spatial proximity. It is used to generate super-\\npixels, which are smaller segments of the image that preserve local informa-\\ntion.\\n4 Results and Discussion\\nTable 1 presents the evaluation results of our proposed Custom CNN model\\ncompared to ResNet32 and VGG16 models.\\nThe Custom CNN model achieved the highest accuracy (98.70%) compared\\nto ResNet32 (96.35%) and VGG16 (96.2%), indicating superior overall perfor-\\nmance in classifying brain tumors. The Custom CNN model also demonstrated\\nthe highest precision (97.63%), compared to ResNet32 (95.24%) and VGG16\\n(95.78%), indicating a lower rate of false positives. Recall: The Custom CNN\\nmodel achieved a recall of 97.64%, slightly higher than ResNet32 (96.12%) and\\nVGG16 (95.12%), indicating a higher rate of true positives. The Custom CNN\\nmodel showed the highest F1-Score (97.47%), compared to ResNet32 (96.15%)\\nand VGG16 (95.76%), balancing precision and recall effectively. The Custom\\nCNN model had the highest LIME Explanation Stability Score (0.923), com-\\npared to ResNet32 (0.846) and VGG16 (0.687), indicating more stable explana-\\ntions across similar inputs. The Custom CNN model had a LIME Explanation\\nSparsity Score of 0.208, compared to ResNet32 (0.196) and VGG16 (0.225), with\\nlower sparsity indicating more concise explanations. ResNet32 had the highest\\nLIME Explanation Fidelity Score (0.556), followed by VGG16 (0.418), and the\\nCustom CNN model (0.310). This measures how well the explanation model ap-\\nproximates the original model.\\nThese results demonstrate that the Custom CNN model generally outperforms\\nboth ResNet32 and VGG16 in terms of classification accuracy, precision, recall,\\nand F1-Score, while providing highly stable and reasonably concise explanations\\nas measured by LIME.\\nTable 2 presents the evaluation results of our proposed Custom U-Net model\\ncompared with a U-Net model that uses ResNet as its encoder. The Custom U-\\nNet model achieved a higher validation accuracy (99.8%) compared to the U-Net\\nwith ResNet as its encoder (99.11%). The Custom U-Net model demonstrated a\\nlower validation loss (0.0132) than the U-Net with ResNet (0.0425), indicating\\nbetter performance in minimizing error. The IoU score, which measures the over-\\nlapbetweenthepredictedandgroundtruthsegments,washigherfortheCustom\\nU-Net model (0.889) compared to the ResNet-based U-Net (0.847), suggest-\\ning more accurate segmentation. The Custom U-Net model had a higher LIMETitle Suppressed Due to Excessive Length 13\\nTable 1. Evaluation Results of proposed Custom CNN model compared to ResNet\\nand VGG16\\nEvaluation Metrics Custom CNN Model ResNet32 VGG16\\nAccuracy 98.70% 96.35% 96.2%\\nPrecision 97.63 95.24 95.78\\nRecall 97.64 96.12 95.12\\nF1-Score 97.47 96.15 95.76\\nLIME Explaination Stability Score 0.923 0.846 0.687\\nLIME Explaination Sparsity Score 0.208 0.196 0.225\\nLIME Explaination Fidelity Score 0.310 0.556 0.418\\nExplanation Stability Score (0.8169) versus the ResNet-based U-Net (0.7873),\\nindicating more stable explanations across similar inputs. Both models showed\\nsimilarLIMEExplanationSparsityScores,withtheCustomU-Netmodelhaving\\na slightly lower score (0.1190) compared to the ResNet-based U-Net (0.1221).\\nLower sparsity indicates more concise explanations. The ResNet-based U-Net\\nmodel exhibited a higher LIME Explanation Fidelity Score (0.6036) compared\\nto the Custom U-Net model (0.5447), which measures how well the explanation\\nmodel approximates the original model. Figure 3 displays the Confusion Matrix\\nTable 2. Evaluation Results for the proposed Custom U-net model compared with\\nResUnet\\nEvaluation Metrics Custom U-net Model U-net with ResNet as Encoder\\nValidation Accuracy 99.8% 99.11%\\nValidation Loss 0.0132 0.0425\\nIntersection over Union Score (IoU) 0.889 0.847\\nLIME Explaination Stability Score 0.8169 0.7873\\nLIME Explaination Sparsity Score 0.1190 0.1221\\nLIME Explaination Fidelity Score 0.5447 0.6036\\nof custom CNN. Figure 4 displays the Plot of ground truth scans , masks and\\npredictedmasksandlabelsalongwithIoUScore.Figure5displaystheMRIScan\\non which LIME visualizations are generated. Figure 6 displays the LIME Visual-\\nizations of Custom CNN with feature heatmap , positive and negative influences\\nand top three feature visualization. Figure 7 displays the plot of ground truth\\nscans , masks and predicted masks and labels along with IoU Score Negative\\nClassifier acheiving perfect IoU of 1.14 Pathikreet and Gargi\\nFig. 3.Confusion Matrix of custom CNN\\nFig. 4.Plot of ground truth scans , masks and predicted masks and labels along with\\nIoU Score\\nFig. 5.MRI Scan on which LIME visualizations are generated\\n5 Conclusion and Future Scope\\nIn this study, we developed and evaluated advanced neural network architectures\\nforbraintumorclassificationandsegmentationusingMRIimages.OurenhancedTitle Suppressed Due to Excessive Length 15\\nFig. 6.LIME Visualizations of Custom CNN with feature heatmap , positive and\\nnegative influences and top three feature visualization\\nFig. 7.Plot of ground truth scans , masks and predicted masks and labels along with\\nIoU Score Negative Classifier acheiving perfect IoU of 1\\nU-Net model demonstrated improved segmentation performance, while our cus-\\ntom CNN showed significant accuracy in classification tasks. The integration of\\nExplainable AI (XAI) techniques, particularly LIME, provided valuable insights\\ninto the model\\u2019s decision-making processes, enhancing interpretability and trust-\\nworthiness.\\nCombining multiple neural network architectures, such as integrating attention\\nmechanisms or transformers, could improve both classification and segmenta-\\ntion performance.Leveraging larger and more diverse datasets can improve the\\ngeneralizability of our models across different populations and imaging modal-\\nities. Developing models capable of real-time processing can facilitate clinical\\napplications, enabling quicker diagnosis and treatment planning. Incorporating\\nmore advanced XAI techniques can provide deeper insights into model behavior,\\nimproving interpretability and clinical acceptance. Utilizing pre-trained models\\non broader datasets and fine-tuning them on specific medical imaging tasks can\\nenhance model performance and reduce training time. Implementing these mod-\\nels in clinical workflows and electronic health records (EHR) systems can aid in\\nautomated diagnosis and decision support.Bibliography\\n[1] S\\u00e9rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva. Brain\\ntumor segmentation using convolutional neural networks in mri images.\\nIEEE transactions on medical imaging , 35(5):1240\\u20131251, 2016.\\n[2] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, and\\nHesham FA Hamed. A review on brain tumor diagnosis from mri images:\\nPractical implications, key achievements, and lessons learned. Magnetic\\nresonance imaging , 61:300\\u2013318, 2019.\\n[3] Ehab F Badran, Esraa Galal Mahmoud, and Nadder Hamdy. An algorithm\\nfor detecting brain tumors in mri images. In The 2010 International Con-\\nference on Computer Engineering & Systems , pages 368\\u2013373. IEEE, 2010.\\n[4] Milica M Bad\\u017ea and Marko \\u010c Barjaktarovi\\u0107. Classification of brain tumors\\nfrom mri images using a convolutional neural network. Applied Sciences ,\\n10(6):1999, 2020.\\n[5] Prashant, Gargi Srivastava, and Vibhav Prakash Singh. Ensemble of deep\\nlearning approaches for detection of brain. In International Journal of Ad-\\nvanced Networking and Applications - IJANA: 1st International Conference\\non Advancements in Smart Computing and Information Security, ASCIS\\n2022, Rajkot, India, November 24-26, 2022 , pages 11\\u201316. Eswar Publica-\\ntions, 2022.\\n[6] Alejandro Barredo Arrieta, Natalia D\\u00edaz-Rodr\\u00edguez, Javier Del Ser, Adrien\\nBennetot, Siham Tabik, Alberto Barbado, Salvador Garc\\u00eda, Sergio Gil-\\nL\\u00f3pez, Daniel Molina, Richard Benjamins, et al. Explainable artificial in-\\ntelligence (xai): Concepts, taxonomies, opportunities and challenges toward\\nresponsible ai. Information fusion , 58:82\\u2013115, 2020.\\n[7] Bas HM Van der Velden, Hugo J Kuijf, Kenneth GA Gilhuijs, and Max A\\nViergever. Explainable artificial intelligence (xai) in deep learning-based\\nmedical image analysis. Medical Image Analysis , 79:102470, 2022.\\n[8] J\\u00fcrgen Dieber and Sabrina Kirrane. Why model why? assessing the\\nstrengths and limitations of lime. arXiv preprint arXiv:2012.00093 , 2020.\\n[9] Sasha Targ, Diogo Almeida, and Kevin Lyman. Resnet in resnet: General-\\nizing residual architectures. arXiv preprint arXiv:1603.08029 , 2016.\\n[10] Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.\\nGoing deeper in spiking neural networks: Vgg and residual architectures.\\nFrontiers in neuroscience , 13:95, 2019.\\n[11] Jun Cheng, Wei Huang, Shuangliang Cao, Ru Yang, Wei Yang, Zhaoqiang\\nYun, Zhijian Wang, and Qianjin Feng. Enhanced performance of brain\\ntumor classification via tumor region augmentation and partition. PloS\\none, 10(10):e0140381, 2015.\\n[12] Jun Cheng, Wei Yang, Meiyan Huang, Wei Huang, Jun Jiang, Yujia Zhou,\\nRu Yang, Jie Zhao, Yanqiu Feng, Qianjin Feng, et al. Retrieval of brain\\ntumors by adaptive spatial pooling and fisher vector representation. PloS\\none, 11(6):e0157112, 2016.\"}"
}